{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2b0c2ba-8834-48aa-b390-66c462e7488b",
   "metadata": {},
   "source": [
    "## Chapter 10 : Modern RNN\n",
    "- **LSTM**\n",
    "  - The structure is :\n",
    "  - <img alt=\"LSTM Arch\" src=\"https://zh.d2l.ai/_images/lstm-3.svg\" style=\"background-color: white; display: inline-block;\"> LSTM Arch\n",
    "- **GRU**\n",
    "  - <img alt=\"GRU Arch\" src=\"https://d2l.ai/_images/gru-3.svg\" style=\"background-color: white; display: inline-block;\"> GRU Arch\n",
    "  - Reset gates help capture short-term dependencies in sequences.\n",
    "  - Update gates help capture long-term dependencies in sequences.\n",
    "- **Deep RNN**\n",
    "  - <img alt=\"Deep RNN\" src=\"https://d2l.ai/_images/deep-rnn.svg\" style=\"background-color: white; display: inline-block;\"> Deep RNN\n",
    "  - In deep rnn, the output is the last layer of the hidden state with every timestep, and state is the last time step hidden state with all layer of rnn.\n",
    "- **Bidirection RNN**, it is slow and gradient chain is long\n",
    "  - $P(x_1,\\ldots,x_T,h_1,\\ldots,h_T)=\\prod_{t=1}^TP(h_t\\mid h_{t-1})P(x_t\\mid h_t),\\mathrm{~where~}P(h_1\\mid h_0)=P(h_1)$, it is a hidden markov model. We can use dynamic programming method compute is from start to end, also from end to start. Just how B-RNN is capable of.\n",
    "  - <img alt=\"Bidirection RNN\" src=\"https://zh.d2l.ai/_images/birnn.svg\" style=\"background-color: white; display: inline-block;\"> B-RNN\n",
    "  - And we just need to concatenate these two H.\n",
    "- **Machine translation**\n",
    "  - non-breaking space, some space should not split to new line, like Mr. Smith.\n",
    "  - Teacher Forcing : all the input will be pad with \\<pad\\>, source token no special treat, decoder input (target seq use as input) will start with \\<bos\\>, and label is shift by 1 (no \\<bos\\> at the begining).\n",
    "  - **Important**: when use teacher forcing, the truth target is feed to the decoder. This will make the traning faster and stable, but it will make training and predicting different (because when predicting we do not have truth target label, we have to repeatedly predict). We can make them the same, but the tranning will be harder.\n",
    "- **Sequence to Sequence**\n",
    "  - We use this Encoder - Decoder Arch to get varied length input and varied length output.\n",
    "  - We do not use one-hot, instead we use nn.Embed layer, which will take token i, and return ith row of the matrix of this embeding layer.\n",
    "  - From the encoder, we get the hidden states, and use a funcion $c = q(h_1, \\cdots, h_T)$, for example, just use the $h_T$. And in the decoder, we concatenate this with the target embed output, and feed to rnn.\n",
    "  - When calculating the loss, we should not take \\<pad\\> into acount. So we need to musk the loss with the tokens.\n",
    "  - <img alt=\"Encoder Decoder\" src=\"https://d2l.ai/_images/seq2seq-details.svg\" style=\"background-color: white; display: inline-block;\"> Encoder Decoder\n",
    "  - Bilingual Evaluation Understudy, BLEU evaluates whether this n-gram in the predicted sequence appears in the target sequence. For example, target sequence ABCDEF, predict sequence ABBCD, $p_1 = 4/5$, we have ABCD in the target sequence, $p_2 = 3 / 4$, we have AB, BC, CD. So we get BLEU as $\\exp\\left(\\min\\left(0,1-\\frac{\\mathrm{len}_{\\mathrm{label}}}{\\mathrm{len}_{\\mathrm{pred}}}\\right)\\right)\\prod_{n=1}^kp_n^{1/2^n}$, higher n will have higher weight, small length of predict length takes lower.\n",
    "- **Beam Search**\n",
    "  - Before this section, we use greedy search to get prediction, use argmax on the prediction vector : $y_{t^{\\prime}}=\\underset{y\\in\\mathcal{Y}}{\\operatorname*{\\operatorname*{argmax}}}P(y\\mid y_1,\\ldots,y_{t^{\\prime}-1},\\mathbf{c})$, where $\\mathcal Y$ is the vacab. Once our model outputs “<eos>” (or we reach the maximum length $T'$) the output sequence is completed.\n",
    "  - However, use the most likely tokens is not the same with the most likely sequence : $\\prod_{t^{\\prime}=1}^{T^{\\prime}}P(y_{t^{\\prime}}\\mid y_1,\\ldots,y_{t^{\\prime}-1},\\mathbf{c})$. For example, in this figure below, ACB will have this probability of 0.5 * 0.3 * 0.6 = 0.09. On the other hand, greedy search choose ABC which is 0.5 * 0.4 * 0.4 = 0.08, it is lower, not optimal!\n",
    "  - <img alt=\"Max sequence\" src=\"https://d2l.ai/_images/s2s-prob2.svg\" style=\"background-color: white; display: inline-block;\">Max sequence <img alt=\"Max token\" src=\"https://d2l.ai/_images/s2s-prob1.svg\" style=\"background-color: white; display: inline-block;\"> Max token.\n",
    "  - If we want the optimal one, we need to do exhaustive search, search all possible sequence, it is not possible!\n",
    "  - The most straightforward type of beam search is keep k candidates. In time step 2, we get $P ( A, y_{2} \\mid\\mathbf{c} )=P ( A \\mid\\mathbf{c} ) P ( y_{2} \\mid A, \\mathbf{c} )$ for the top, and $P ( C, y_{2} \\mid\\mathbf{c} )=P ( C \\mid\\mathbf{c} ) P ( y_{2} \\mid C, \\mathbf{c} ) $ for the bottom, then choose most 2 from them. And then choose sequence that maximize $\\frac{1} {L^{\\alpha}} \\mathrm{l o g} \\, P ( y_{1}, \\ldots, y_{L} \\mid\\mathbf{c} )=\\frac{1} {L^{\\alpha}} \\sum_{t^{\\prime}=1}^{L} \\mathrm{l o g} \\, P ( y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, \\mathbf{c} ) ; $. Note tha we have **6** candidates (A, C ..).\n",
    "  - <img alt=\"Max sequence\" src=\"https://d2l.ai/_images/beam-search.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\"> -->\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f270645-8482-489c-88fe-cf3918bf9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e85a8-7167-41d1-bfc4-10418f18cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a LSTM model here\n",
    "\"\"\"\n",
    "\n",
    "class LSTMScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        if H_C is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                          device=inputs.device)\n",
    "            C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                          device=inputs.device)\n",
    "        else:\n",
    "            H, C = H_C\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
    "                            torch.matmul(H, self.W_hi) + self.b_i)\n",
    "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
    "                            torch.matmul(H, self.W_hf) + self.b_f)\n",
    "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
    "                            torch.matmul(H, self.W_ho) + self.b_o)\n",
    "            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
    "                               torch.matmul(H, self.W_hc) + self.b_c)\n",
    "            C = F * C + I * C_tilde # This is important, use F to forget, I to input\n",
    "            H = O * torch.tanh(C)\n",
    "            outputs.append(H)\n",
    "        return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1694a56-03ca-4586-91d3-b3449b89c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a GRU model here\n",
    "\"\"\"\n",
    "\n",
    "class GRUScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
    "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        if H is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                          device=inputs.device)\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            Z = torch.sigmoid(torch.matmul(X, self.W_xz) +\n",
    "                            torch.matmul(H, self.W_hz) + self.b_z)\n",
    "            R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
    "                            torch.matmul(H, self.W_hr) + self.b_r)\n",
    "            H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                               torch.matmul(R * H, self.W_hh) + self.b_h)\n",
    "            H = Z * H + (1 - Z) * H_tilde\n",
    "            outputs.append(H)\n",
    "        return outputs, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa492c44-7b2f-4ce6-91c7-c7de57562138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LSTM                                     [10, 16, 32]              7,936\n",
       "==========================================================================================\n",
       "Total params: 7,936\n",
       "Trainable params: 7,936\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 1.27\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.03\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = nn.LSTM(28, 32) # 28 is one-hot size, 32 is hidden size\n",
    "summary(lstm_model, input_size = (10, 16, 28)) # 10 is squence length (time step), 16 is batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06cc21bd-0c36-4107-89dd-95e5fc01e5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNN                                      [10, 16, 32]              1,984\n",
       "==========================================================================================\n",
       "Total params: 1,984\n",
       "Trainable params: 1,984\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.32\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = nn.RNN(28, 32) # param size is exaclty 1 / 4 of lstm\n",
    "summary(rnn_model, input_size = (10, 16, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f46ae3-bdd6-4354-837d-e1f7eb090487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GRU                                      [10, 16, 32]              5,952\n",
       "==========================================================================================\n",
       "Total params: 5,952\n",
       "Trainable params: 5,952\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.95\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 0.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model = nn.GRU(28, 32) # 3 times of RNN\n",
    "summary(gru_model, input_size = (10, 16, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93586f50-0e85-4390-9de6-bf9476d2072c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNN                                      [10, 16, 32]              4,096\n",
       "==========================================================================================\n",
       "Total params: 4,096\n",
       "Trainable params: 4,096\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.66\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 0.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_deep_model = nn.RNN(28, 32, 2) # a little bigger than 2 times of noraml RNN\n",
    "summary(rnn_deep_model, input_size = (10, 16, 28), depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0504d69f-a6f5-4035-8bf4-8156d7ff781e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNN                                      [10, 16, 64]              3,968\n",
       "==========================================================================================\n",
       "Total params: 3,968\n",
       "Trainable params: 3,968\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.63\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 0.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_bidirect_model = nn.RNN(28, 32, bidirectional = True) # 2 time of normal RNN\n",
    "summary(rnn_bidirect_model, input_size = (10, 16, 28), depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37891281-c5c2-4a32-b55f-0660f6401b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "        \n",
    "class MyData:\n",
    "    def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "            self._read_data())\n",
    "        \n",
    "    def _read_data(self, path = \"../../Data/fra-eng/fra.txt\"):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def _preprocess(self, text):\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        # Insert space between words and punctuation marks\n",
    "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "               for i, char in enumerate(text.lower())]\n",
    "        return ''.join(out)\n",
    "    \n",
    "    def _tokenize(self, text, max_examples=None):\n",
    "        src, tgt = [], []\n",
    "        for i, line in enumerate(text.split('\\n')):\n",
    "            if max_examples and i > max_examples: break\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                # Skip empty tokens\n",
    "                src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "                tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "        return src, tgt\n",
    "\n",
    "    def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "        def _build_array(sentences, vocab, is_tgt=False):\n",
    "            pad_or_trim = lambda seq, t: (\n",
    "                seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "            if is_tgt:\n",
    "                sentences = [['<bos>'] + s for s in sentences] # tgt length will be longer\n",
    "            if vocab is None:\n",
    "                vocab = Vocab(sentences, min_freq=2)\n",
    "            array = torch.tensor([vocab[s] for s in sentences])\n",
    "            valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1) # this is needed\n",
    "            return array, vocab, valid_len\n",
    "        src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                                  self.num_train + self.num_val)\n",
    "        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "        return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "                src_vocab, tgt_vocab)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        \"\"\"Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                           shuffle=train)\n",
    "        \n",
    "    def get_dataloader(self, train):\n",
    "        idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader(self.arrays, train, idx)\n",
    "\n",
    "    def build(self, src_sentences, tgt_sentences):\n",
    "        raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "            src_sentences, tgt_sentences)])\n",
    "        arrays, _, _ = self._build_arrays(\n",
    "            raw_text, self.src_vocab, self.tgt_vocab)\n",
    "        return arrays\n",
    "\n",
    "def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n",
    "    \"\"\"Plot the histogram for list length pairs.\"\"\"\n",
    "    _, _, patches = plt.hist(\n",
    "        [[len(l) for l in xlist], [len(l) for l in ylist]])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    for patch in patches[1].patches:\n",
    "        patch.set_hatch('/')\n",
    "    plt.legend(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02de4719-3545-432b-9af3-46807e9b13f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça al\n"
     ]
    }
   ],
   "source": [
    "data = MyData()\n",
    "raw_text = data.read_data(\"../../Data/fra-eng/fra.txt\")\n",
    "text = data._preprocess(raw_text)\n",
    "src, tgt = data._tokenize(text)\n",
    "print(text[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97027b85-64cd-4b50-bf77-64347ff0fa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_list_len_pair_hist(['source', 'target'], '# tokens per sequence',\n",
    "                        'count', src, tgt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfd08b7-5391-4b85-ba82-4dc2de0ada7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: tensor([[188,   6,   3,   4,   4,   4,   4,   4,   4],\n",
      "        [ 86,  43,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [  5,   0,   3,   4,   4,   4,   4,   4,   4]], dtype=torch.int32)\n",
      "decoder input: tensor([[  3, 161,   7,   4,   5,   5,   5,   5,   5],\n",
      "        [  3, 108, 183,  98,   2,   4,   5,   5,   5],\n",
      "        [  3, 211,   6,   0,   4,   5,   5,   5,   5]], dtype=torch.int32)\n",
      "source len excluding pad: tensor([3, 4, 3], dtype=torch.int32)\n",
      "label: tensor([[161,   7,   4,   5,   5,   5,   5,   5,   5],\n",
      "        [108, 183,  98,   2,   4,   5,   5,   5,   5],\n",
      "        [211,   6,   0,   4,   5,   5,   5,   5,   5]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data = MyData(batch_size=3)\n",
    "src, tgt, src_valid_len, label = next(iter(data.train_dataloader()))\n",
    "print('source:', src.type(torch.int32))\n",
    "print('decoder input:', tgt.type(torch.int32))\n",
    "print('source len excluding pad:', src_valid_len.type(torch.int32))\n",
    "print('label:', label.type(torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483ee40c-7daf-4ec5-a260-958d5d76c320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<bos>', '<pad>', '<eos>')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tgt_vocab.idx_to_token[3], data.tgt_vocab.idx_to_token[5], data.src_vocab.idx_to_token[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79a15f8e-0c3f-4a89-8bae-1acc98e7de89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: ['hi', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "target: ['<bos>', 'salut', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "src, tgt, _,  _ = data.build(['hi .'], ['salut .'])\n",
    "print('source:', data.src_vocab.to_tokens(src[0].type(torch.int32)))\n",
    "print('target:', data.tgt_vocab.to_tokens(tgt[0].type(torch.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e90059-7776-463d-9244-1fb61bd8b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder\n",
    "\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):  #@save\n",
    "    \"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding)\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056e4ff3-c29c-4885-9b21-d2f2b657f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder\n",
    "\"\"\"\n",
    "\n",
    "class Decoder(nn.Module):  #@save\n",
    "    \"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding)\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654c69cf-b10b-4ce7-8a3d-6da494c01532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encode - Decoder Arch\n",
    "\"\"\"\n",
    "\n",
    "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "reduce_mean = lambda x, *args, **kwargs: x.mean(*args, **kwargs)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"The base class of classification models.\n",
    "\n",
    "    Defined in :numref:`sec_classification`\"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
    "\n",
    "    def accuracy(self, Y_hat, Y, averaged=True):\n",
    "        \"\"\"Compute the number of correct predictions.\n",
    "    \n",
    "        Defined in :numref:`sec_classification`\"\"\"\n",
    "        Y_hat = torch.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
    "        preds = astype(argmax(Y_hat, axis=1), Y.dtype)\n",
    "        compare = astype(preds == torch.reshape(Y, -1), torch.float32)\n",
    "        return reduce_mean(compare) if averaged else compare\n",
    "\n",
    "    def loss(self, Y_hat, Y, averaged=True):\n",
    "        \"\"\"Defined in :numref:`sec_softmax_concise`\"\"\"\n",
    "        Y_hat = torch.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
    "        Y = torch.reshape(Y, (-1,))\n",
    "        return F.cross_entropy(\n",
    "            Y_hat, Y, reduction='mean' if averaged else 'none')\n",
    "\n",
    "    def layer_summary(self, X_shape):\n",
    "        \"\"\"Defined in :numref:`sec_lenet`\"\"\"\n",
    "        X = torch.randn(*X_shape)\n",
    "        for layer in self.net:\n",
    "            X = layer(X)\n",
    "            print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "\n",
    "class EncoderDecoder(Classifier):  #@save\n",
    "    \"\"\"The base class for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only\n",
    "        return self.decoder(dec_X, dec_state)[0]\n",
    "\n",
    "    def predict_step(self, batch, device, num_steps,\n",
    "                 save_attention_weights=False):\n",
    "        batch = [a.to(device) for a in batch]\n",
    "        src, tgt, src_valid_len, _ = batch\n",
    "        enc_all_outputs = self.encoder(src, src_valid_len)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n",
    "        outputs, attention_weights = [tgt[:, (0)].unsqueeze(1), ], []\n",
    "        for _ in range(num_steps):\n",
    "            Y, dec_state = self.decoder(outputs[-1], dec_state) # output is a list!\n",
    "            outputs.append(Y.argmax(2)) # Y is (batch, 1(step), vocab), so ouput is (batch, 1)\n",
    "            # Save attention weights (to be covered later)\n",
    "            if save_attention_weights:\n",
    "                attention_weights.append(self.decoder.attention_weights)\n",
    "        return torch.cat(outputs[1:], 1), attention_weights # do not return <pad>, and return (batch, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdbbd1b8-ab1d-427b-8c43-8ebcee244673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seq2seq(module):  #@save\n",
    "    \"\"\"Initialize weights for sequence-to-sequence learning.\"\"\"\n",
    "    if type(module) == nn.Linear:\n",
    "         nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.GRU:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])\n",
    "\n",
    "class Seq2SeqEncoder(Encoder):  #@save\n",
    "    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout = dropout)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        embs = self.embedding(X.t().type(torch.int64))\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        outputs, state = self.rnn(embs)\n",
    "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, state\n",
    "\n",
    "class Seq2SeqDecoder(Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size+num_hiddens, num_hiddens,\n",
    "                           num_layers, dropout = dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        return enc_all_outputs\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        embs = self.embedding(X.t().type(torch.int32))\n",
    "        enc_output, hidden_state = state\n",
    "        # context shape: (batch_size, num_hiddens)\n",
    "        context = enc_output[-1]\n",
    "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
    "        context = context.repeat(embs.shape[0], 1, 1)\n",
    "        # Concat at the feature dimension\n",
    "        embs_and_context = torch.cat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = self.dense(outputs).swapaxes(0, 1)\n",
    "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
    "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, [enc_output, hidden_state]\n",
    "\n",
    "class Seq2Seq(EncoderDecoder):  #@save\n",
    "    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tgt_pad = tgt_pad\n",
    "        self.lr = lr\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam optimizer is used here\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        l = super().loss(Y_hat, Y, averaged=False)\n",
    "        mask = (Y.reshape(-1) != self.tgt_pad).type(torch.float32)\n",
    "        return (l * mask).sum() / mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff18cca-dc5d-4fc5-9e23-eb2e3fba8498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 4, 16]) torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 9\n",
    "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "X = torch.zeros((batch_size, num_steps))\n",
    "enc_outputs, enc_state = encoder(X)\n",
    "print(enc_outputs.shape, enc_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64201895-41ca-4ccf-bbce-912b94544882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9, 10]) torch.Size([9, 4, 16]) torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "state = decoder.init_state(encoder(X))\n",
    "dec_outputs, state = decoder(X, state)\n",
    "print(dec_outputs.shape, state[0].shape, state[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3d06ff4-ab20-4584-94cc-bdca02a5ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(encoder, decoder, 5, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dab4c1c-dcde-4d4d-b64c-1bce52d84b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.1774e-03, -1.3611e-01,  6.0320e-02, -2.3777e-01,  1.2387e-01,\n",
      "          1.7110e-02,  4.3592e-02, -2.2250e-01, -1.2387e-01,  4.6138e-02],\n",
      "        [-5.8463e-02, -1.6122e-02,  1.6028e-02, -4.3072e-02,  1.2748e-01,\n",
      "         -8.2334e-02,  1.7483e-01, -2.9459e-01, -2.0162e-01,  6.0619e-02],\n",
      "        [-9.1908e-02,  4.3872e-02, -1.4242e-04,  4.0666e-02,  1.3917e-01,\n",
      "         -1.3944e-01,  2.5276e-01, -3.1762e-01, -2.5067e-01,  8.6200e-02],\n",
      "        [-1.1042e-01,  7.2683e-02, -3.7957e-03,  7.8237e-02,  1.4776e-01,\n",
      "         -1.7278e-01,  2.9364e-01, -3.2229e-01, -2.8004e-01,  1.1303e-01],\n",
      "        [-1.2128e-01,  8.6662e-02, -3.1796e-03,  9.6976e-02,  1.5228e-01,\n",
      "         -1.9283e-01,  3.1392e-01, -3.2116e-01, -2.9649e-01,  1.3621e-01],\n",
      "        [-1.2786e-01,  9.3677e-02, -1.6188e-03,  1.0680e-01,  1.5400e-01,\n",
      "         -2.0560e-01,  3.2374e-01, -3.1839e-01, -3.0519e-01,  1.5406e-01],\n",
      "        [-1.3198e-01,  9.7356e-02, -1.1450e-04,  1.1190e-01,  1.5404e-01,\n",
      "         -2.1419e-01,  3.2832e-01, -3.1541e-01, -3.0953e-01,  1.6674e-01],\n",
      "        [-1.3467e-01,  9.9363e-02,  1.1365e-03,  1.1444e-01,  1.5314e-01,\n",
      "         -2.2017e-01,  3.3023e-01, -3.1274e-01, -3.1153e-01,  1.7524e-01],\n",
      "        [-1.3653e-01,  1.0047e-01,  2.1437e-03,  1.1557e-01,  1.5180e-01,\n",
      "         -2.2440e-01,  3.3077e-01, -3.1053e-01, -3.1232e-01,  1.8071e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([36, 10])\n",
      "tensor([ 0.0052, -0.1361,  0.0603, -0.2378,  0.1239,  0.0171,  0.0436, -0.2225,\n",
      "        -0.1239,  0.0461], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(dec_outputs[0])\n",
    "dec_outputs_loss = torch.reshape(dec_outputs, (-1, dec_outputs.shape[-1]))\n",
    "print(dec_outputs_loss.shape)\n",
    "print(dec_outputs_loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52320595-aa30-4ed8-8613-1f207c40cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[3],\n",
      "        [3],\n",
      "        [3]])]\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([3, 9])\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print([tgt[:, (index)].unsqueeze(1), ])\n",
    "print(tgt[:, (index)].shape)\n",
    "print(tgt[:, (index)].unsqueeze(1).shape)\n",
    "print((tgt[:, (index)].unsqueeze(1))[-1].shape)\n",
    "print(tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ae94c40-5747-465d-9bdf-15f48a1d1086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "        [4, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "        [4, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "        [4, 6, 6, 6, 6, 6, 6, 6, 6]])\n",
      "torch.Size([4, 9])\n"
     ]
    }
   ],
   "source": [
    "print(dec_outputs.argmax(2))\n",
    "print(dec_outputs.argmax(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c54a54-7953-487e-a458-2b6d9955dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BLEU \n",
    "\"\"\"\n",
    "def bleu(pred_seq, label_seq, k):  #@save\n",
    "    \"\"\"Compute the BLEU.\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, min(k, len_pred) + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
