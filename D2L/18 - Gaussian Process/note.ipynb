{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0ea3fa-aefc-452c-852e-6a7f71554fa1",
   "metadata": {},
   "source": [
    "## Chapter 18 : Gaussian Processes\n",
    "- Definition: any subset of a GP is multivariate Gaussian distribution. It is defined solely on its mean and covariance function.\n",
    "  - For a given mean function $m(x)$, a covariance function $k(x,x^{'})$, noise $w \\sim \\mathcal N(0, \\sigma^2)$, conditioned on Data $(X,y)$, for new data point $x^*$, we have:\n",
    "    $$ \n",
    "    \\begin{array}{lll}\n",
    "    \\mu(x^*) &=& m(x^*) + k(x^*, X) \\left [ k(X,X) + \\sigma^2 I\\right]^{-1}(y - m(x^*)) \\\\\n",
    "    v(x^*)   &=& k(x^*,x^*) - k(x^*, X)\\left [ k(X,X) + \\sigma^2 I\\right]^{-1}k(X, x^*)\n",
    "    \\end{array}\n",
    "    $$\n",
    "- Priors:\n",
    "  - For any function with linear parameters, which takes this form $f(x) = \\sum_i w_i \\phi_i(x)$, is a GP, where $w \\sim \\mathcal N(u, S)$. And the mean function is $m(x) = u^{\\top}\\phi(x)$, covariance function is $k(x, x^{'}) = \\phi(x)^{\\top}S\\phi(x)$. Any GP can take this form, but $\\phi(x)$ maybe infinite dimension, for example, RBF kernel.\n",
    "    $$\n",
    "    \\begin{array}{lll}\n",
    "    k(x, x^{'}) &=& E\\left[(f(x)-E[f(x)])(f(x^{'})-E[f(x^{'})])\\right] \\\\\n",
    "                &&\\rightarrow  E[f(x)] = E[W^T\\phi(x)] = U^T\\phi(x), W=[w_0, w_1, \\ldots], U = [u_0, u_1, \\ldots] \\\\\n",
    "                &=& E\\left[ (W^T-U^T) \\phi(x) (W^T-U^T) \\phi(x^{'}) \\right] \\\\\n",
    "                &&\\rightarrow  \\hat W = W - U \\\\\n",
    "                &=& E\\left[ \\hat W^T \\phi(x) \\hat W^T \\phi(x^{'}) \\right] \\\\\n",
    "                &=& E\\left[ \\phi(x)^T \\hat W \\hat W^T \\phi(x^{'}) \\right] \\\\\n",
    "                &=& \\phi(x)^T E[\\hat W \\hat W^T] \\phi(x^{'}) \\\\\n",
    "                &=& \\phi(x)^T S \\phi(x^{'})\n",
    "    \\end{array}\n",
    "    $$\n",
    "    The above prove use the fact that $w_i$ is independent (iid).\n",
    "  - *RBF kernel* (stationary kernel): can be derived with the above form\n",
    "    $$f(x) = \\sum_{i=1}^Jw_i\\phi_i(x)ï¼Œw_i \\sim \\mathcal N\\left (0, \\frac{\\sigma^2}{J} \\right), \\phi_i(x) = \\exp \\left(-\\frac{(x-c_i)^2}{2l^2} \\right)$$\n",
    "    push the dimension to infinity, we get:\n",
    "    $$k(x,x^{'}) = \\sigma^2 \\exp \\left( -\\frac{(x-x^{'})^2}{2l^2} \\right)$$\n",
    "  - *Neural Network Kernel* is non-stationary kernel:\n",
    "    $$f(x) = b + \\sum_{i=1}^Jv_ih(x;u_i), b\\sim\\mathcal N(0,\\sigma_b^2), v\\sim\\mathcal N(0,\\sigma_v^2/J), u\\sim \\mathcal N(0,\\Sigma).$$\n",
    "    We get:\n",
    "    $$k(x, x^{'}) = \\frac{2}{\\pi} \\sin\\left( \\frac{2\\bar x^{\\top} \\Sigma \\bar x^{'}}{\\sqrt{(1+2\\bar x^{\\top} \\Sigma \\bar x)(1+2\\bar x^{'\\top} \\Sigma \\bar x^{'})}} \\right)$$\n",
    "- Inference:\n",
    "  - With the prior, we get $y|f,x \\sim \\mathcal N \\left(0, k(X,X)+\\sigma^2I \\right)$ for $y=f(x)+\\epsilon$, $f(x) \\sim \\mathcal{GP}, \\epsilon \\sim \\mathcal N(0, \\sigma^2)$. So:\n",
    "    $$p(y|f,x) = \\frac{1}{(2\\pi)^{n/2}|k(X,X)+\\sigma^2I|^{1/2}} \\exp \\left(-\\frac{1}{2}y^T(k(X,X)+\\sigma^2I)^{-1}y \\right)$$\n",
    "    Take log on it, we get:\n",
    "    $$\\log p(y|f,x) = -\\frac{1}{2}(k(X,X)+\\sigma^2I)^{-1}y - \\frac{1}{2}|k(X,X)+\\sigma^2I| - \\frac{n}{2}\\log2\\pi$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237bb9b-0644-4271-b2a0-37b1fed797d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
