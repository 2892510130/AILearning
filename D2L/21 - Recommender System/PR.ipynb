{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f209a-87cf-41e9-8fc0-06f4546014e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for PR Loss and NCFPR\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b945bc3-c9bf-4296-b608-ca8dab250f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(pos: torch.Tensor, neg: torch.Tensor)-> torch.Tensor:\n",
    "    \"\"\"Bayesian Personalized Ranking Loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pos : torch.Tensor\n",
    "        Ranking logit (0..1)\n",
    "    neg : torch.Tensor\n",
    "        Ranking logit (0..1)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    loss scalar\n",
    "    \"\"\"\n",
    "    diff = pos - neg\n",
    "    return -F.logsigmoid(diff).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d5eff-5969-402a-815d-47f1258119b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(pos: torch.Tensor, neg: torch.Tensor)-> torch.Tensor:\n",
    "    diff = pos - neg\n",
    "    return torch.maximum(margin-diff, 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143d92d-1c7a-41ea-bc67-8be25486d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML100kData(Dataset):\n",
    "    def __init__(self, data_dir:str=\"../Data/ml-100k\", normalize_rating:bool=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.normalize_rating = normalize_rating\n",
    "        self.df, self.num_users, self.num_items = read_data_ml100k(data_dir)\n",
    "        self.user_id = self.df.user_id.values - 1\n",
    "        self.item_id = self.df.item_id.values - 1\n",
    "        self.rating = self.df.rating.values.astype(np.float32)\n",
    "        \n",
    "    def split(self, train_ratio=0.8):\n",
    "        train_len = int(train_ratio * len(self))\n",
    "        test_len = len(self) - train_len\n",
    "        return random_split(self, [train_len, test_len])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        return self.user_id[idx], self.item_id[idx], self.rating[idx]\n",
    "\n",
    "class ML100KPairWise(ML100kData):\n",
    "    def __init__(self, data_dir=\"../Data/ml-100k\",\n",
    "                 test_leave_out=1,\n",
    "                 test_sample_size: int = None):\n",
    "        \"\"\"Pair Wise loader to train NeuMF model.\n",
    "        Samples are slightly different based on train/test mode.\n",
    "\n",
    "        In training mode:\n",
    "        - user_id: int\n",
    "        - item_id: int\n",
    "            Item id that user has interacted with\n",
    "        - neg_item: int\n",
    "            Item id that user hasn't interacted with while training\n",
    "\n",
    "        In testing mode:\n",
    "        - user_id: int\n",
    "        - item_id: int\n",
    "            Random item_id to be ranked by the model\n",
    "        - is_pos: bool\n",
    "            If True, this item is a positive item \n",
    "            that user has interacted with in groundtruth data.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_dir : str, optional\n",
    "            Path to dataset directory, by default \"./ml-100k\"\n",
    "        test_leave_out : int, optional\n",
    "            Leave out how many items per user for testing\n",
    "            By default 1\n",
    "        test_sample_size : int, optional\n",
    "            It is time-consuming to rank all items for every user during\n",
    "            evaluation, we can randomly choose a subset of items to rank\n",
    "            If None, rank all items.\n",
    "            By default None\n",
    "        \"\"\"\n",
    "        super().__init__(data_dir)\n",
    "        # Use 0-based indexing consistently\n",
    "        self.set_all_item_ids = set(range(self.num_items))  # 0 to num_items-1\n",
    "        self.test_leave_out = test_leave_out\n",
    "        self.test_sample_size = test_sample_size\n",
    "        # general\n",
    "        self.train = None\n",
    "        self.has_setup = False\n",
    "        # Split Dataframe\n",
    "        self.split_dataframe()\n",
    "        self.build_candidates()\n",
    "\n",
    "    def split_dataframe(self):\n",
    "        \"\"\"Split ML100K dataframe with the strategy leave-n-out\n",
    "        with timestamp order.\n",
    "        \"\"\"\n",
    "        user_group = self.df.groupby(\"user_id\", sort=False)\n",
    "        train_df = []\n",
    "        test_df = []\n",
    "        for user_id, user_df in user_group:\n",
    "            user_df = user_df.sort_values(\"timestamp\")\n",
    "            train_df.append(user_df[:-self.test_leave_out])\n",
    "            test_df.append(user_df[-self.test_leave_out:])\n",
    "        self.train_df = pd.concat(train_df)\n",
    "        self.test_df = pd.concat(test_df)\n",
    "\n",
    "    def build_candidates(self):\n",
    "        # Train - Make both user_id and item_id 0-based\n",
    "        self.observed_items_per_user_in_train = {\n",
    "            int(user_id) - 1: user_df.item_id.values - 1\n",
    "            for user_id, user_df in self.train_df.groupby(\"user_id\", sort=False)\n",
    "        }\n",
    "        self.unobserved_items_per_user_in_train = {\n",
    "            user_id: np.array(\n",
    "                list(self.set_all_item_ids - set(observed_items)))\n",
    "            for user_id, observed_items in self.observed_items_per_user_in_train.items()\n",
    "        }\n",
    "        # Test - Make both user_id and item_id 0-based\n",
    "        self.gt_pos_items_per_user_in_test = {\n",
    "            int(user_id) - 1: user_df[-self.test_leave_out:].item_id.values - 1\n",
    "            for user_id, user_df in self.test_df.groupby(\"user_id\", sort=False)\n",
    "        }\n",
    "\n",
    "    def split(self, *args, **kwargs):\n",
    "        # Train split\n",
    "        train_split = deepcopy(self)\n",
    "        train_split.user_id = self.train_df.user_id.values - 1\n",
    "        train_split.item_id = self.train_df.item_id.values - 1\n",
    "        train_split.train = True\n",
    "        train_split.has_setup = True\n",
    "        \n",
    "        # Test split\n",
    "        test_split = deepcopy(self)\n",
    "        test_split.user_id = []\n",
    "        test_split.item_id = []\n",
    "        for user_id, items in self.unobserved_items_per_user_in_train.items():\n",
    "            if self.test_sample_size is None:\n",
    "                sample_items = items\n",
    "            elif isinstance(self.test_sample_size, int):\n",
    "                sample_items = np.random.choice(items, self.test_sample_size)\n",
    "            else:\n",
    "                raise TypeError(\"self.test_sample_size should be int\")\n",
    "            sample_items = np.concatenate(\n",
    "                [test_split.gt_pos_items_per_user_in_test[user_id],\n",
    "                 sample_items])\n",
    "            sample_items = np.unique(sample_items)\n",
    "            test_split.user_id += [user_id]*len(sample_items)\n",
    "            test_split.item_id.append(sample_items)\n",
    "        test_split.user_id = np.array(test_split.user_id)\n",
    "        test_split.item_id = np.concatenate(test_split.item_id)\n",
    "        test_split.train = False\n",
    "        test_split.has_setup = True\n",
    "        return train_split, test_split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_id)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert self.has_setup, \"Must run self.setup()\"\n",
    "        if self.train:\n",
    "            user_id = self.user_id[idx]\n",
    "            pos_item = self.item_id[idx]\n",
    "            neg_item = np.random.choice(\n",
    "                self.unobserved_items_per_user_in_train[int(user_id)])\n",
    "            return user_id, pos_item, neg_item\n",
    "        else:\n",
    "            user_id = self.user_id[idx]\n",
    "            item_id = self.item_id[idx]\n",
    "            is_pos = item_id in self.gt_pos_items_per_user_in_test[user_id]\n",
    "            return user_id, item_id, is_pos\n",
    "\n",
    "class LitData(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset:Dataset, \n",
    "        train_ratio:float=0.8, \n",
    "        batch_size:int=32, \n",
    "        num_workers:int=4\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.train_ratio = train_ratio\n",
    "        self.dataloader_kwargs = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_workers\": num_workers,\n",
    "            \"persistent_workers\": True if num_workers > 0 else False\n",
    "        }\n",
    "        self._log_hyperparams = True\n",
    "        self.allow_zero_length_dataloader_with_multiple_devices = False\n",
    "\n",
    "        self.num_users = getattr(self.dataset, \"num_users\", None)\n",
    "        self.num_items = getattr(self.dataset, \"num_items\", None)\n",
    "\n",
    "    def setup(self, stage:str):\n",
    "        self.train_split, self.test_split = self.dataset.split(\n",
    "            self.train_ratio)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_split, **self.dataloader_kwargs, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_split, **self.dataloader_kwargs, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_split, **self.dataloader_kwargs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605fd0b4-4361-4e83-b42a-02153fef64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, embedding_dims: int, num_users: int, num_items: int, hidden_dims: List, **kwargs):\n",
    "        super().__init__()\n",
    "        self.P = nn.Embedding(num_users, embedding_dims)\n",
    "        self.Q = nn.Embedding(num_items, embedding_dims)\n",
    "        self.U = nn.Embedding(num_users, embedding_dims)\n",
    "        self.V = nn.Embedding(num_items, embedding_dims)\n",
    "        mlp = [nn.Linear(embedding_dims*2, hidden_dims[0]),\n",
    "               nn.ReLU()]\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            mlp += [nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
    "                    nn.ReLU()]\n",
    "        self.mlp = nn.Sequential(*mlp)\n",
    "        self.output_layer = nn.Linear(\n",
    "            hidden_dims[-1] + embedding_dims, 1, bias=False)\n",
    "\n",
    "    def forward(self, user_id, item_id) -> torch.Tensor:\n",
    "        p_mf = self.P(user_id)\n",
    "        q_mf = self.Q(item_id)\n",
    "        gmf = p_mf * q_mf\n",
    "\n",
    "        p_mlp = self.U(user_id)\n",
    "        q_mlp = self.V(item_id)\n",
    "        mlp = self.mlp(torch.cat([p_mlp, q_mlp], axis=-1))\n",
    "        logit = self.output_layer(\n",
    "            torch.cat([gmf, mlp], axis=-1))\n",
    "        return logit\n",
    "\n",
    "class LitNeuMF(L.LightningModule):\n",
    "    def __init__(self, lr=0.002, hitrate_cutout=10, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = NeuMF(**kwargs)\n",
    "        self.lr = lr\n",
    "        self.hitrate = RetrievalHitRate(top_k=hitrate_cutout)\n",
    "        self.training_step_outputs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), self.lr, weight_decay=1e-5)\n",
    "\n",
    "    def forward(self, user_id, item_id):\n",
    "        return self.model(user_id, item_id)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_id, pos_item, neg_item = batch\n",
    "        pos_score = self(user_id, pos_item)\n",
    "        neg_score = self(user_id, neg_item)\n",
    "        loss = bpr_loss(pos_score, neg_score)\n",
    "        self.training_step_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        user_id, item_id, is_pos = batch\n",
    "        logit = self(user_id, item_id)\n",
    "        score = torch.sigmoid(logit).reshape(-1,)\n",
    "        self.hitrate.update(score, is_pos, user_id.to(torch.int64))\n",
    "        return \n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.training_step_outputs).mean()\n",
    "        self.logger.experiment.add_scalar(\n",
    "            \"train/loss\", epoch_average, self.current_epoch)\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.logger.experiment.add_scalar(\n",
    "            f\"val/hit_rate@{self.hitrate.top_k}\", self.hitrate.compute(), self.current_epoch)\n",
    "        self.hitrate.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663776b0-2b79-4096-960b-f3a0e2719856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_mf():\n",
    "    embedding_dims, max_epochs, batch_size = 30, 40, 256\n",
    "    data = LitData(\n",
    "        ML100KPairWise(test_sample_size=100),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    model = LitNeuMF(\n",
    "        num_users=data.num_users, num_items=data.num_items,\n",
    "        embedding_dims=embedding_dims,\n",
    "        hidden_dims=[10, 10, 10]\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(\"log\", name=f\"NeuMF\")\n",
    "    trainer = L.Trainer(max_epochs=max_epochs, accelerator=\"auto\", logger=logger)\n",
    "\n",
    "    trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
