{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce9b4e0-8b18-4a6e-b40f-a4b8fb2c358f",
   "metadata": {},
   "source": [
    "## Chapter 17 : Reinforcement Learning\n",
    "- Markov Decision Process\n",
    "  - $\\mathcal S, \\mathcal A$ are states and actions a robot can take, when taking an action, states after may not\n",
    "be deterministic, it has a probability. We use a transition function $T: \\mathcal S \\times \\mathcal A \\times \\mathcal S \\rightarrow [0, 1]$ to denote this, $T(s, a, s') = P(s' | s,a)$ is the probability of reaching s' given $s$ and $a$. For $\\forall s \\in \\mathcal S$ and $\\forall a \\in \\mathcal A$, $\\sum_{s^{'}\\in S}T(s, a, s^{'}) = 1$. Reward $r:\\mathcal S \\times \\mathcal A \\rightarrow \\mathbb R$, $r(s,a)$.\n",
    "  - This can build a MDP, $(\\mathcal S, \\mathcal A, \\mathcal T, r)$, and a trajectory $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, a_2, r_2, \\ldots)$. The *return* of a trajectory is the total reword $R(\\tau) = \\sum_t r_t$, the goal of reinforcement learning is to find a trajectory that has the largest return. The trajectory might be infinite, so in order for a meaningful formular of its return, we introduce a discount factor $\\gamma < 1$, $R(\\tau) = \\sum_{t=0}^{\\infty}\\gamma^tr_t$. For large $\\gamma$, the robot is encouraged to explore, for small one to take a short trajetory to goal.\n",
    "  - Markov system only depend on current state and action, not the history one (but we can always augment the system).\n",
    "    \n",
    "- Value Iteration\n",
    "  - *Value function* is the value of a state, from that state, the expected sum reward (return). *Action Value function* is the value of an action at state s, from that state, take that action, the expected sum reward (return). And $V^{\\pi}(s) = \\sum_{a \\in \\mathcal A}\\pi(a\\mid s) Q^{\\pi}(s,a)$.\n",
    "  - Policy $\\pi(s) = \\pi(a\\mid s)$, and $\\sum_a \\pi(a \\mid s) = 1$. Given a policy, we may get a lot of trajectories, the average return of those are $$V^{\\pi}(s_0) = E_{a_t \\sim \\pi(s_t)}[R(\\tau)] = E_{a_t \\sim \\pi(s_t)}\\left[ \\sum_{t=0}^{\\infty}\\gamma^tr(s_t, a_t) \\right]$$ this is the *value function* of policy $\\pi$. Given two probability (one for choose the action, one for take the action), we can write the probability of a certain trajectory as $$P(\\tau) = \\pi(a_0\\mid s_0) \\cdot P(s_1 \\mid s_0, a_0) \\cdot \\pi(a_1\\mid s_1) \\cdot P(s_2 \\mid s_1, a_1) \\cdots$$ So if we divide the trajectory into 2 parts, $s_0$ and $\\tau^{'}$, we get $$R(\\tau) = r(s_0, a_0) + \\gamma \\sum_{t=1}^{\\infty}\\gamma^{t-1}r(s_t, a_t) = r(s_0, a_0) + \\gamma R(\\tau^{'})$$\n",
    "And the expectaion of this is (the optimal policy is the argmax of this):\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "V^{\\pi}(s_0) &=& E_{a_t \\sim \\pi(s_t)}[r(s_0, a_0) + \\gamma R(\\tau^{'})] \\\\\n",
    "             &=& E_{a_0 \\sim \\pi(s_0)}[r(s_0, a_0)] + E_{a_0 \\sim \\pi(s_0)}\\left[E_{s_1 \\sim P(s_1 \\mid a_0, s_0)}[E_{a_t \\sim \\pi(s_t)}[\\gamma R(\\tau^{'})]]\\right] \\\\ \n",
    "             &=& E_{a_0 \\sim \\pi(s_0)}[r(s_0, a_0)] + E_{a_0 \\sim \\pi(s_0)}\\left[E_{s_1 \\sim P(s_1 \\mid a_0, s_0)}[V^{\\pi}(s_1)]\\right] \\\\\n",
    "             &\\rightarrow & \\\\\n",
    "V^{\\pi}(s)   &=& \\sum_{a \\in \\mathcal A}\\pi(a\\mid s)\\left[r(s,a) + \\gamma \\sum_{s^{'}}P(s^{'} \\mid s, a)V^{\\pi}(s^{'})\\right], \\forall s \\in S \\\\\n",
    "             &\\rightarrow & r(s,a) = \\sum_r p(r \\mid s,a)r = \\sum_r\\sum_{s^{'}}p(r,s^{'} \\mid s,a)r =  \\sum_{s^{'}}p(s^{'} \\mid s,a)r(s^{'})\\\\\n",
    "             &=& \\sum_{a \\in \\mathcal A}\\pi(a\\mid s)\\sum_{s^{'} \\in \\mathcal S}P(s^{'} \\mid s, a)\\left[r(s^{'}) + \\gamma V^{\\pi}(s^{'})\\right]\n",
    "\\end{array}$$\n",
    "Last arrow please refer to *Math of Reinforcement Learning*, if we assume reward only depend on next state.\n",
    "  - *Action Value* (start at $s_0$ but action is fixed to $a_0$):\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "Q^{\\pi}(s_0, a_0) &=& r(s_0, a_0) + E_{a_t \\sim \\pi(s_t)} \\left[ \\sum_{t=1}^{\\infty}\\gamma^{t}r(s_t, a_t) \\right] \\\\\n",
    "                  &\\rightarrow & \\\\\n",
    "Q^{\\pi}(s, a)     &=& r(s,a) + \\gamma \\sum_{s^{'}}P(s^{'} \\mid s, a)\\sum_{a^{'}}\\pi(s^{'} \\mid a^{'})Q^{\\pi}(s^{'}, a^{'}), \\forall s \\in S, a \\in \\mathcal A \\\\\n",
    "                  &=& r(s,a) + \\gamma \\sum_{s^{'}}P(s^{'} \\mid s, a)V^{\\pi}(s^{'}), \\forall s \\in S, a \\in \\mathcal A\n",
    "\\end{array}\n",
    "$$\n",
    "We can also write it into the form of $r(s^{'})$, which is the form Gym takes.\n",
    "  - Value Iteration: initialize the value function to arbitrary values $V_0(s), \\forall s \\in S$. If the policy is deterministic, then at $k^{th}$ iteration (max will return one action, which means the action policy is deterministic): $$V_{k+1}(s) = \\max_{a \\in \\mathcal A} \\left \\{ r(s,a) + \\gamma \\sum_{s^{'}}P(s^{'} \\mid s, a)V_{k}(s^{'}) \\right \\}, \\forall s \\in \\mathcal S$$ And $V^*(s) = \\lim_{k \\rightarrow \\infty} V_k(s)$. The same iteration can be written with action value: $$Q_{k+1}(s,a) = r(s,a) + \\gamma \\max_{a^{'} \\in A}\\sum_{s^{'}}P(s^{'} \\mid s, a)Q_k(s^{'}, a^{'})$$\n",
    "  - Policy Evaluation: if the policy is not deterministic, it is stochastic, we can also get:\n",
    "$$V_{k+1}^{\\pi}(s) = \\sum_{a \\in \\mathcal A}\\pi(a \\mid s) \\left [ r(s,a) + \\gamma \\sum_{s^{'} \\in \\mathcal S}P(s^{'} \\mid s, a)V_{k}^{\\pi}(s^{'}) \\right ], \\forall s \\in \\mathcal S$$\n",
    "\n",
    "- Q-Learning\n",
    "  - Using the robot's policy $\\pi_e(a \\mid s)$, we can approximate the value iteration:\n",
    "    $$\\hat Q = \\min_Q \\frac{1}{nT}\\sum_{i=1}^n\\sum_{t=0}^T\\left(Q(s_t^i, a_t^i)-r(s_t^i, a_t^i)-\\gamma \\max_{a^{'}}Q(s_{t+1}^i, a^{'})\\right)^2 = \\min_Q \\mathscr l(Q)$$\n",
    "    Using gradient descent:\n",
    "    $$\n",
    "    \\begin{array}{lll}\n",
    "    Q(s_t^i, a_t^i) &\\leftarrow& Q(s_t^i, a_t^i) + \\alpha \\nabla_{Q(s_t^i, a_t^i)} \\mathscr l (Q) \\\\\n",
    "                    &=         & (1-\\alpha)Q(s_t^i, a_t^i) +\\alpha\\left(r(s_t^i, a_t^i) + \\gamma \\max_{a^{'}}Q(s_{t+1}^i, a^{'})\\right)\n",
    "    \\end{array}\n",
    "    $$\n",
    "    In order to stop at terminal state, we update the formular:\n",
    "    $$Q(s_t^i, a_t^i) = (1-\\alpha)Q(s_t^i, a_t^i) +\\alpha\\left(r(s_t^i, a_t^i) + \\gamma (1 - \\mathbb 1_{s_{t+1}^i \\text{is terminal}}) \\max_{a^{'}}Q(s_{t+1}^i, a^{'})\\right)$$\n",
    "    Then we get $\\hat \\pi(s) = \\arg \\max_a \\hat Q(s,a)$.\n",
    "  - In order to explore well, with current estimate of Q, set the policy to (*$\\epsilon$-greedy exploration policy*):\n",
    "    $$ \\pi_e(a \\mid s) = \\left \\{ \n",
    "    \\begin{array}{lll}\n",
    "    \\arg \\max_{a^{'}} \\hat Q(s, a^{'}) & \\text{with prob.} ~1 - \\epsilon\\\\\n",
    "    \\text{uniform}(\\mathcal A)         & \\text{with prob.} ~\\epsilon\n",
    "    \\end{array}\n",
    "    \\right .$$\n",
    "    Other choise is *softmax exploration policy*, $T$ is called *temperature*:\n",
    "    $$\\pi_e(a \\mid s) = \\frac{e^{\\hat Q(s,a)/T}}{\\sum_{a^{'}}e^{\\hat Q(s,a^{'})/T}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e924d5-be96-4b9d-af6b-d8d248fc1188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
