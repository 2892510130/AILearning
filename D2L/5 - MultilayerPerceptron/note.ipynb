{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6c885b-0689-4dd6-80d2-2912fadc2acf",
   "metadata": {},
   "source": [
    "## Chapter 5 : Multilayer Perceptrons\n",
    "- Activation Function: relu, sigmoid, tanh ($\\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}$)\n",
    "- Numerical stability: vanish and explode are common\n",
    "  - Symmetry: linear layer and conv (with no share weight) layer are symmetric so we can not tell apart from different weight and try to explain it (for example 2 hidden unit with same initial value, they will update the same way), so we need to **Bread the Symmetry** (like using a dropout)\n",
    "  - Xavier initilization: get from distrubution of zero mean and variance $\\sigma = \\sqrt{2 / (n_{in} + n_{out})}$\n",
    "  - Dropout, shared param...\n",
    "- (Rolnick et al., 2017) has revealed that in the setting of label noise, neural networks tend to fit cleanly labeled data **first** and only subsequently to interpolate the mislabeled data.\n",
    "  - so we can use early stop once error on val is minimal or the patience hit. usually combined with regularization.\n",
    "- Dropout:\n",
    "  - $h^{'} = \\left \\{ \n",
    "  \\begin{array}{lll}\n",
    "  & 0, p \\\\\n",
    "  & \\frac{h}{1-p}, 1-p\n",
    "  \\end{array} \n",
    "  \\right .$, now $E[h^{'}] = E[h]$\n",
    "  - We do not use dropout in test, except we want to know the uncertainty of the model output (by comparing different dropout)\n",
    "  - Use lower p in lower layer (to get lower feature), higher p in higher layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bd5ce2-2342-422a-8b29-4a1645096d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653b85cc-d99b-4a5c-84e1-b637d1de80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: torch.tensor):\n",
    "    a = torch.zeros_like(x)\n",
    "    return torch.max(a, x)\n",
    "\n",
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    if dropout == 1: return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape) > dropout).float()\n",
    "    return mask * X / (1.0 - dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aaf8da4-c7d3-4287-b1e3-1ca42290e0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1594,  1.5834],\n",
      "        [-0.0884, -0.8145]])\n",
      "tensor([[0.1594, 1.5834],\n",
      "        [0.0000, 0.0000]])\n",
      "tensor([[ 0.0000,  3.1668],\n",
      "        [-0.1767, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((2, 2))\n",
    "a_relu = relu(a)\n",
    "print(a)\n",
    "print(a_relu)\n",
    "a_drop = dropout_layer(a, 0.5)\n",
    "print(a_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499e228-17f6-4cb3-8dff-4bf3590dcefe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
