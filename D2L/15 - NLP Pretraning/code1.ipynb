{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b69e50a-3884-4834-9f87-4508a2188705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "from typing import Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23461069-e73b-4dd0-9691-dabb530a399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                   Type               Data/Info\n",
      "-------------------------------------------------------\n",
      "Optional                   _SpecialForm       typing.Optional\n",
      "RandomGenerator            type               <class '__main__.RandomGenerator'>\n",
      "Union                      _SpecialForm       typing.Union\n",
      "Vocab                      type               <class '__main__.Vocab'>\n",
      "all_centers                list               n=343109\n",
      "all_contexts               list               n=343109\n",
      "all_negatives              list               n=343109\n",
      "batch                      tuple              n=4\n",
      "batchify                   function           <function batchify at 0x0000014964F22840>\n",
      "center                     int                9\n",
      "collections                module             <module 'collections' fro<...>ollections\\\\__init__.py'>\n",
      "compare_counts             function           <function compare_counts at 0x0000014961CD1DA0>\n",
      "context                    list               n=2\n",
      "corpus                     list               n=42069\n",
      "counter                    Counter            Counter({'the': 50770, 'N<...>gin': 10, 'gelbart': 10})\n",
      "data                       Tensor             tensor([[1, 1, 0, 0, 0, 0<...>     [1, 1, 1, 0, 0, 0]])\n",
      "generator                  RandomGenerator    <__main__.RandomGenerator<...>ct at 0x000001496096C5C0>\n",
      "get_centers_and_contexts   function           <function get_centers_and<...>ts at 0x0000014961CD1C60>\n",
      "get_negatives              function           <function get_negatives at 0x0000014957E5E840>\n",
      "math                       module             <module 'math' (built-in)>\n",
      "name                       str                labels\n",
      "names                      list               n=4\n",
      "os                         module             <module 'os' (frozen)>\n",
      "random                     module             <module 'random' from 'C:<...>nvs\\\\ai\\\\Lib\\\\random.py'>\n",
      "read_ptb                   function           <function read_ptb at 0x0000014956871F80>\n",
      "sentences                  list               n=42069\n",
      "subsample                  function           <function subsample at 0x0000014961CD0F40>\n",
      "subsampled                 list               n=42069\n",
      "tiny_dataset               list               n=2\n",
      "torch                      module             <module 'torch' from 'C:\\<...>ges\\\\torch\\\\__init__.py'>\n",
      "vocab                      Vocab              <__main__.Vocab object at 0x000001496100EFF0>\n",
      "x_1                        tuple              n=3\n",
      "x_2                        tuple              n=3\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb679f3e-97d8-476a-809e-c9a7937c9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ptb() -> list[str]:\n",
    "    data_dir = \"../../Data/ptb/\"\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "    return [line.split() for line in raw_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8e23ac-0400-40f7-964f-279fb3ee4ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senteces: 42069\n"
     ]
    }
   ],
   "source": [
    "sentences = read_ptb()\n",
    "print(f\"senteces: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e9efef-792b-43bd-a821-c408f313814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Split sentences to word, create map between index and token.\n",
    "    min_freq: discard token which frequency less than it\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokens: Union[list[str], list[list[str]]] = [], \n",
    "        min_freq: int = 0,\n",
    "        reserved_tokens: list[str] = []\n",
    "    ):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                              reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                         for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4091db7c-279f-4479-905f-5fc119493786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 6719\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(sentences, min_freq=10)\n",
    "print(f\"vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f360d5c1-cb4d-40dc-b736-1d8dacbee798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,\n",
       " ['#', '$', '&'],\n",
       " [('the', 50770), ('<unk>', 45020), ('N', 32481)],\n",
       " [('flat-rolled', 1), ('biscuits', 1), ('isi', 1)],\n",
       " 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.unk, vocab.idx_to_token[:3], vocab.token_freqs[:3], vocab.token_freqs[-3:], vocab.token_to_idx['#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d144007-0c88-4552-962a-e799fe78ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Subsample high-frequency words.\n",
    "    \"\"\"\n",
    "    # Exclude unknown tokens ('<unk>')\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk]\n",
    "                 for line in sentences]\n",
    "    counter = collections.Counter([\n",
    "        token for line in sentences for token in line])\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    # Return True if `token` is kept during subsampling\n",
    "    def keep(token):\n",
    "        return(random.uniform(0, 1) <\n",
    "               math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "\n",
    "    return ([[token for token in line if keep(token)] for line in sentences],\n",
    "            counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4838c4c6-497e-4994-ae89-903fe6b8a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42069, 6718\n"
     ]
    }
   ],
   "source": [
    "subsampled, counter = subsample(sentences, vocab)\n",
    "print(f\"{len(subsampled)}, {len(counter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6ac16a-1a1f-4be5-88f9-b3529081f6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the: before=50770, after=2083'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return (f'{token}: '\n",
    "            f'before={sum([l.count(token) for l in sentences])}, '\n",
    "            f'after={sum([l.count(token) for l in subsampled])}')\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e323401-ea1f-401d-b63f-f585324552af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [4127, 3228], [3922, 1922, 4743]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in subsampled]\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea1bc179-b388-470b-b37b-bd599e86f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    \"\"\"\n",
    "    Return center words and context words in skip-gram.\n",
    "    window size is random\n",
    "    \"\"\"\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        # To form a \"center word--context word\" pair, each sentence needs to\n",
    "        # have at least 2 words\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)):  # Context window centered at `i`\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size),\n",
    "                                 min(len(line), i + 1 + window_size)))\n",
    "            # Exclude the center word from the context words\n",
    "            indices.remove(i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af4705ad-792c-494c-9996-d083ab994790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2]\n",
      "center 2 has contexts [1, 3]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [2, 3, 5, 6]\n",
      "center 5 has contexts [3, 4, 6]\n",
      "center 6 has contexts [5]\n",
      "center 7 has contexts [8, 9]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4f9c179-94e0-45fb-98a1-c67f7e974c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'center-context pairs: 1503713'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
    "f'center-context pairs: {sum([len(contexts) for contexts in all_contexts])}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60814c99-c9e2-4067-8936-45b85f9f19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGenerator:\n",
    "    \"\"\"\n",
    "    Randomly draw among {1, ..., n} according to n sampling weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, sampling_weights):\n",
    "        # Exclude\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            # Cache `k` random sampling results\n",
    "            self.candidates = random.choices(\n",
    "                self.population, self.sampling_weights, k=10000)\n",
    "            self.i = 0\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a354e4a-dfb9-4389-b520-4f6bcc6e5008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2, 1, 2, 2, 3, 2, 2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = RandomGenerator([2, 3, 4]) # P(x=1) = 2 / (2 + 3 + 4) = 2 / 9, p(x=2) = 3 / 9, p(x=3) = 4 / 9\n",
    "[generator.draw() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3009cd20-8a7c-42dc-843e-3960695cc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, vocab, counter, K):\n",
    "    \"\"\"\n",
    "    Return noise words in negative sampling.\n",
    "    \"\"\"\n",
    "    # Sampling weights for words with indices 1, 2, ... (index 0 is the\n",
    "    # excluded unknown token) in the vocabulary\n",
    "    # weight is freq of power 2\n",
    "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n",
    "                        for i in range(1, len(vocab))]\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # Noise words cannot be context words\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d3b0f0b-8a87-4f90-b2a8-39e24c8344a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343109, [29, 2441, 788, 2282, 2131], 343109, [3228])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_negatives), all_negatives[0], len(all_contexts), all_contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "413b13c8-8186-44a2-a739-7631df8fe755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    \"\"\"Return a minibatch of examples for skip-gram with negative sampling.\"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\n",
    "        contexts_negatives), torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0029f9bd-b457-4fa4-9d7d-aa47fa84caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tensor([[1],\n",
      "        [1]])\n",
      "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 3, 3, 0]])\n",
      "masks = tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])\n",
      "labels = tensor([[1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x_1 = (1, [2, 2], [3, 3, 3, 3])\n",
    "x_2 = (1, [2, 2, 2], [3, 3])\n",
    "batch = batchify((x_1, x_2))\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
