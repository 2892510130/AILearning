{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2 : Preliminary Knowledge\n",
    "Main content:\n",
    "- 数据操作\n",
    "  - 广播机制（两个数据分别复制扩充到同样的尺寸）\n",
    "  - 节省内存（使用X[:] = \\<expression\\>或X+=\\<expression\\>来避免重新分配）\n",
    "- 数据预处理\n",
    "- 线性代数 \n",
    "  - 转置.T 范数norm\n",
    "  - 非降维求和 (keepdims=True)，累积和cumsum\n",
    "  - torch.dot只支持向量，矩阵和向量间用mv，矩阵之间用mm\n",
    "- 微积分\n",
    "  - 设T是梯度算符，T(Ax) = A.T, T(x.T·A) = A, T(x.T A x) = (A + A.T)x\n",
    "- 自动微分\n",
    "  - 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "  - 自动微分必须是标量，非标量的话要么转成标量，要么指定输出形状\n",
    "  - 分离操作\n",
    "- 概率论\n",
    "- 查阅文档、API的指导\n",
    "  - dir查看可以调用的函数和类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963708091840\n",
      "1963708098880\n",
      "1963708098880\n"
     ]
    }
   ],
   "source": [
    "# 数据操作\n",
    "## '+' 等操作会导致内存的重新分配\n",
    "X = torch.arange(4).reshape(2, 2)\n",
    "Y = torch.arange(4).reshape(2, 2)\n",
    "print(id(X))\n",
    "X = X + Y\n",
    "print(id(X))\n",
    "X[:] = X + Y\n",
    "print(id(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 3],\n",
      "        [6, 9]])\n",
      "tensor([[ 3],\n",
      "        [15]])\n",
      "tensor([[0.0000, 1.0000],\n",
      "        [0.4000, 0.6000]])\n",
      "tensor([[ 0,  3],\n",
      "        [ 6, 15]])\n",
      "4 torch.Size([1, 2]) torch.Size([4, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11.8322, dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性代数\n",
    "## 非降维求和：sum_X维度不会变化\n",
    "sum_X = X.sum(axis=1, keepdim=True)\n",
    "print(X)\n",
    "print(sum_X)\n",
    "print(X / sum_X)\n",
    "print(X.cumsum(axis=1))\n",
    "vector_x = torch.arange(2)\n",
    "### torch.dot只支持向量，矩阵和向量间用mv，矩阵之间用mm\n",
    "torch.mv(X, vector_x), torch.mm(X, Y)\n",
    "tensor_test = torch.arange(8, dtype=float).reshape(4, 1, 2)\n",
    "print(len(tensor_test), tensor_test.sum(axis=0).shape, tensor_test.shape)\n",
    "torch.linalg.norm(tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grade before the compute: None\n",
      "dot grad tensor([ 0.,  4.,  8., 12.]) <MulBackward0 object at 0x000001C93E68F550>\n",
      "sum grad tensor([1., 1., 1., 1.])\n",
      "mult grad tensor([0., 2., 4., 6.])\n",
      "non-scalar grad tensor([0., 2., 4., 6.])\n",
      "tensor([0., 1., 4., 9.]) , which should be: tensor([0, 3, 12, 27]) without detach\n",
      "Two times of beackward():  tensor([2., 2., 2., 2.]) --梯度会累加\n"
     ]
    }
   ],
   "source": [
    "# 自动微分\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "print(\"grade before the compute:\", x.grad)\n",
    "\n",
    "y = 2 * torch.dot(x, x)\n",
    "y.backward()\n",
    "print(\"dot grad\", x.grad, y.grad_fn)\n",
    "\n",
    "x.grad.zero_() # 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(\"sum grad\", x.grad)\n",
    "\n",
    "## 自动微分必须是标量，非标量的话要么转成标量，要么指定输出形状\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward()\n",
    "print(\"mult grad\", x.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.backward(torch.ones_like(y))\n",
    "# gradients = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(\"non-scalar grad\", x.grad)\n",
    "\n",
    "## 分离操作\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "print(x.grad, \", which should be: tensor([0, 3, 12, 27]) without detach\")\n",
    "\n",
    "## 练习\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "y.backward()\n",
    "print(\"Two times of beackward(): \", x.grad, \"--梯度会累加\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13., 24., 11., 18., 14., 20.])\n",
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull']\n"
     ]
    }
   ],
   "source": [
    "# 概率\n",
    "from torch.distributions import multinomial\n",
    "fair_probs = torch.ones([6]) / 6\n",
    "print(multinomial.Multinomial(100, fair_probs).sample())\n",
    "print(dir(torch.distributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another example of backward on non-scalar output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([3., 4.]),\n",
       " tensor([1., 2.]),\n",
       " tensor([1., 1.]),\n",
       " torch.float32,\n",
       " tensor([1., 1.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor([1.0, 2])\n",
    "x2 = torch.tensor([3, 4.0])\n",
    "x1.requires_grad_(True)\n",
    "x2.requires_grad_(True)\n",
    "\n",
    "y = x1 * x2\n",
    "y.retain_grad()\n",
    "\n",
    "y.backward(torch.ones_like(y))\n",
    "\n",
    "print(\"another example of backward on non-scalar output\")\n",
    "x1.grad, x2.grad, y.grad, y.dtype, torch.ones_like(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of backward in control flows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(5., grad_fn=<DotBackward0>),\n",
       " tensor([ 9., 16.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example of backward in control flows\")\n",
    "\n",
    "def auto_grad_in_control_flows(a):\n",
    "    if a[0] > 2:\n",
    "        return a * a\n",
    "    else:\n",
    "        return a.dot(a)\n",
    "\n",
    "a = torch.tensor([1, 2.0])\n",
    "b = torch.tensor([3.0, 4.0])\n",
    "a.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "fa = auto_grad_in_control_flows(a)\n",
    "fb = auto_grad_in_control_flows(b)\n",
    "fa, fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
