{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851d558d-4c99-417f-9017-d482eb233a28",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "- Chapter 2 : Preliminary Knowledge\n",
    "- Chapter 3 : Linear Neural Network\n",
    "- Chapter 4 : Classification\n",
    "- Chapter 5 : Multilayer Perceptrons\n",
    "- Chapter 6 : Beginner Guide\n",
    "- Chapter 7 : CNN\n",
    "- Chapter 8 : Modern CNN\n",
    "- Chapter 9 : RNN\n",
    "\n",
    "  \n",
    "- 工程和硬件优化 RoadMap\n",
    "- 梯度检查技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c420e-be32-4965-a531-ff998f6cc05b",
   "metadata": {},
   "source": [
    "### Chapter 2 : Preliminary Knowledge\n",
    "- 数据操作\n",
    "  - 广播机制（两个数据分别复制扩充到同样的尺寸）\n",
    "  - 节省内存（使用X[:] = \\<expression\\>或X+=\\<expression\\>来避免重新分配）\n",
    "- 数据预处理\n",
    "- 线性代数 \n",
    "  - 转置.T 范数norm\n",
    "  - 非降维求和 (keepdims=True)，累积和cumsum\n",
    "  - torch.dot只支持向量，矩阵和向量间用mv，矩阵之间用mm\n",
    "- 微积分\n",
    "  - 设T是梯度算符，T(Ax) = A.T, T(x.T·A) = A, T(x.T A x) = (A + A.T)x\n",
    "- 自动微分\n",
    "  - 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "  - 自动微分必须是标量，非标量的话要么转成标量，要么指定输出形状\n",
    "  - 分离操作\n",
    "- 概率论\n",
    "- 查阅文档、API的指导\n",
    "  - dir查看可以调用的函数和类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a80380-84c4-4ad0-84e1-4a76d4de7076",
   "metadata": {},
   "source": [
    "### Chapter 3 : Linear Neural Network\n",
    "- Minibatch stochastic gradient descent (小批量随机梯度下降)\n",
    "- 一般的训练过程\n",
    "  - model.forward() 与 y_hat 做差，然后反向传播，优化器根据导数去更新参数\n",
    "- Machine Learning Concept\n",
    "  - lasso regression: l1 norm; ridge regression: l2 norm;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff340e0-b5fd-437e-8f8b-0698e9faf41f",
   "metadata": {},
   "source": [
    "## Chapter 4 : Classification\n",
    "- softmax:\n",
    "  $y_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}$, often minus max(oj) to get numerical stable\n",
    "- Information theory\n",
    "  - cross-entropy loss：$l(y, \\hat y) = - \\sum y_i * \\log(\\hat y_i)$\n",
    "  - amount of information $\\log{\\frac{1}{P(j)}} = - \\log{P(j)}$ \n",
    "  - entorpy $H[P] = \\sum -P(j) \\log{P(j)}$\n",
    "  - cross-entorpy $H(P, Q) = \\sum -P(j) \\log{Q(j)}, ~ P=Q \\rightarrow H(P, Q) = H(P, P) = H(P)$. In pytorch, F.cross_entropy will do the softmax for you.\n",
    "- Image Classification Rules:\n",
    "  - image stored in (channel, height, weight) manner.\n",
    "- Distrubution shift:\n",
    "  - Covariate Shift (feature shift): $p(x) \\neq q(x), p(y|x) = q(y|x)$\n",
    "    - For example: p(x) and q(x) are features of oral and urban house, y is the price, we assume the feature and label relation is the same\n",
    "    - Method: weighted by $\\beta(x) = p(x) / q(x) \\rightarrow \\int\\int l(f(x), y)p(y|x)p(x)dxdy = \\int\\int l(f(x), y)q(y|x)q(x) \\frac{p(x)}{q(x)}dxdy \\rightarrow \\sum_i \\beta_i l(f(x_i), y_i)$, $\\beta$ can be obtained with logistic regression.\n",
    "  - Label Shift, $p(y) \\neq q(y), p(x|y) = q(x|y)$, the same method $\\beta(y) = p(y) / q(y)$, but now $q(y)$ is hard to get, we need compute a confusion matrix on the val data then use the model to pridcit the distrubution of the $q(y)$\n",
    "  - Concept Shift (the concept of the label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab587f-119f-4a04-bbde-9e7d49953d9d",
   "metadata": {},
   "source": [
    "## Chapter 5 : Multilayer Perceptrons\n",
    "- Activation Function: relu, sigmoid, tanh ($\\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}$)\n",
    "- Numerical stability: vanish and explode are common\n",
    "  - Symmetry: linear layer and conv (with no share weight) layer are symmetric so we can not tell apart from different weight and try to explain it (for example 2 hidden unit with same initial value, they will update the same way), so we need to **Bread the Symmetry** (like using a dropout)\n",
    "  - Xavier initilization: get from distrubution of zero mean and variance $\\sigma = \\sqrt{2 / (n_{in} + n_{out})}$\n",
    "  - Dropout, shared param...\n",
    "- (Rolnick et al., 2017) has revealed that in the setting of label noise, neural networks tend to fit cleanly labeled data **first** and only subsequently to interpolate the mislabeled data.\n",
    "  - so we can use early stop once error on val is minimal or the patience hit. usually combined with regularization.\n",
    "- Dropout:\n",
    "  - $h^{'} = \\left \\{ \n",
    "  \\begin{array}{lll}\n",
    "  & 0, p \\\\\n",
    "  & \\frac{h}{1-p}, 1-p\n",
    "  \\end{array} \n",
    "  \\right .$, now $E[h^{'}] = E[h]$\n",
    "  - We do not use dropout in test, except we want to know the uncertainty of the model output (by comparing different dropout)\n",
    "  - Use lower p in lower layer (to get lower feature), higher p in higher layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8510492-eac8-4c69-bc9f-49cbe5acb237",
   "metadata": {},
   "source": [
    "## Chapter 6: Beginner Guide\n",
    "- Tied layer: gradient will add up along different chain\n",
    "- Custom initialization: `apply` method\n",
    "- I/O\n",
    "  - save tensor: `torch.save(x:Uinon[List[tensor], Dict], name:str)` and load\n",
    "  - save model: the same, just input dict of the net (`net.state_dict()`) then `net.load_state_dict(torch.load(name))`\n",
    "- GPU\n",
    "  - operation between tensors must in the same GPU\n",
    "  - print or transform to numpy will copy to memory, and even worse wait the python **GIL** (`Global Interpreter Lock`, make sure at the same time only one thread can execute the python bytecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72453bd2-625c-49c8-9715-37123159d39e",
   "metadata": {},
   "source": [
    "## Chapter 7 : CNN\n",
    "1. **Invariance**: translation equivariance, locality -> The earliest layers should respond similarly to the same patch and focus on local regions.\n",
    "2. **Convolution**: math is $(f * g)(i, j) = \\sum_a \\sum_b f(a, b)  g(i - a, j - b)$, remind that **cross-correlation** is $(f * g)(i, j) = \\sum_a \\sum_b f(a, b)  g(i + a, j + b)$\n",
    "   - The difference is not important as we will learn the kernel, `k_conv_learned = k_corr_learned.T`, or `conv(X, k_conv_learned) = corr(X, k_corr_learned)`\n",
    "3. **Receptive Field**： for any element (tensors on the conv layer) x, all the elements that may effect x in the previous layers in the forward population.\n",
    "4. **Padding, Stride**: $\\lfloor (n_h - k_h + p_h + s_h) / s_h \\rfloor \\times \\lfloor (n_w - k_w + p_w + s_w) / s_w \\rfloor$, often `p_h = k_h - 1`, the same for `p_w`. `p_h = p_h_upper + p_h_lower`\n",
    "5. **Channel**:\n",
    "   - multi in $c_i$ -> kernel must also have the same channels ($c_i \\times k_h \\times k_w$), then add them up.\n",
    "   - multi out $c_o$ -> kernel with $c_o \\times c_i \\times k_h \\times k_w$, get $c_o$ output channels.\n",
    "6. use `torch.stack` to stack tensors\n",
    "7. **Pooling**: mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0070a-bf43-4f5b-a1b8-d264e02296f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 8 : Modern CNN\n",
    "1. **AlexNet**: first deep conv successful, using dropout, Relu, polling\n",
    "2. **VGG**: multiple 3 * 3 conv layers (two 3 * 3 conv touch 5 * 5 input as a 5 * 5 conv, but 2 * 3 * 3  = 18 < 25 = 5 * 5)\n",
    "3. **NiN**: to handle 2 problem (1. much ram for the MLP at the end; 2. can not add MLP between the conv to increase the degree of nonlinearity as it will destroy the spatial information)\n",
    "   - use 1 * 1 conv layer to add local nonlinearities across the channel activations\n",
    "   - use global average pooling to integrate across all locations in the last representation layer. (must combine with added nonlinearities)\n",
    "4. **GoogleNet**: Inception layer, parallel conv multi scales, and then concate them\n",
    "5. **Batch Normalization**:\n",
    "   - $BN(\\mathbf x) = \\mathbf{\\gamma} \\bigodot \\frac{\\mathbf x - \\mathbf{\\mu_B}}{\\sigma^2_B} + \\mathbf \\beta$, $\\mathbf{\\mu_B} = \\frac{1}{|B|}\\sum_{x \\in B} \\mathbf x$,\n",
    "     $\\sigma^2_B = \\frac{1}{|B|} \\sum_{x \\in B} (x - \\mathbf{\\mu_B})^2 + \\epsilon$\n",
    "   - On linear layer [N, D] it will get across D (different features in D will not do calculations), on conv layer [N, C, H, W] it will across C (save the difference between channels)\n",
    "     - For example, [N, C, H, W] shape input x, for x[N, 0, H, W], get it's mean mu and std and do (x[N, 0, H, W] - mu) / std, here mu and std are scalar.\n",
    "   - At the testing stage, we will use the global (whole) data mean and varience, instead of minibatch mean and varience. Just like dropout.\n",
    "   - So BN also serves as a noise introducer! (minibatch information != true mean and var) Teye et al. (2018) and Luo et al. (2018).\n",
    "   - So it best works for batch size of 50 ~ 100, higher the noise is small, lower it is too high.\n",
    "   - Moving global mean and var: when testing, no minibatch, so we use a global one that is stored during training.\n",
    "     - It is a kind of exp weighted mean, closest batch has higer weight\n",
    "     - $\\mu_m = \\mu_m * (1 - \\tau) + \\mu * \\tau, \\Sigma_m = \\Sigma_m * (1 - \\tau) + \\Sigma * \\tau$, $\\tau$ is called momentum term.\n",
    "6. **Layer Normalization**: often used in NLP\n",
    "   - For features like [N, A, B] it will save difference between N, A and B are typically seq_len, hidden_size.\n",
    "7. **ResNet**: residual block, pass x as one of the branch before a activation function (for the original paper, and later it is changed to BN -> AC -> Conv)\n",
    "   - To get the passed x has the correct shape to add up, we can use 1 * 1 conv if it is needed\n",
    "   - **Idea**: nested-function class, shallower net (like ResNet-20) is subclass of depper net (like ResNet-50). Because in ResNet-50 if the layers after 20th layer are f(x) = x, then it is the same as RestNet-20! So we can make sure f' (the best we can get in ResNet-50 for certain data) will be better than f (ResNet-20 on the same data) or at least the same.\n",
    "   - <p align=\"center\">\n",
    "       <img alt=\"Residul Block\" src=\"https://d2l.ai/_images/resnet-block.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       Rusidul Block\n",
    "   </p>\n",
    "   - **ResNeXt**: use g groups of 3 * 3 conv layers between two 1 * 1 conv of channel $b$ and $c_o$, so $\\mathcal O(c_i c_o) \\rightarrow \\mathcal O(g ~ c_i / g ~ c_o / g) = \\mathcal O(c_ic_o/g)$\n",
    "     - This is a **Bottleneck** arch if $b < c_i$\n",
    "       </br>\n",
    "   - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/resnext-block.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       ResNeXt Block\n",
    "8. **DenseNet**: instead of plus x, we concatenate x repeatedly.\n",
    "   - For example (\\<channel\\> indicates the channel): x\\<c_1\\> -> f_1(x)\\<c_2\\> end up with [x, f_1(x)]\\<c_1 + c_2\\> -> f_2([x, f_1(x)])\\<c_3\\> end up with [x, f_1(x), f_2([x, f_1(x)])]\\<c_1 + c_2 + c_3\\>\n",
    "   - Too many of this layer will cause the dimeansion too big, so we need some layer to reduce it. **Translation** layer use 1 * 1 conv to reduce channel and avgpool to half the H and W.\n",
    "9. **RegNet**:\n",
    "   - AnyNet: network with **stem** -> **body** -> **head**.\n",
    "   - Distrubution of net: $F(e,Z)=∑_{i=1}^{n}1(e_i<e)$, use this empirical CDF to approximate $F(e, p)$, $p$ is the net arch distrubution. $Z$ is a sample of net sample from $p$, if $F(e, Z_1) < F(e, Z_2)$ then we say $Z_1$ is better, it's parameters are better.\n",
    "   - So for RegNet, they find that we should use same k (k = 1, no bottlenet, is best, says in paper) and g for the ResNeXt blocks with no harm, and increase the network depth d and weight c along the stage. And keep the c change linearly with $c_j = c_o + c_aj$ with slope $c_a$\n",
    "   - neural architecture search (NAS) : with certain search space, use RL (NASNet), evolution alg (AmoebaNet), gradient based (DARTS) or shared weight (ENAS) to get the model. But it takes to much computation resource.\n",
    "   - <img src=\"https://d2l.ai/_images/anynet.svg\" style=\"background-color: white; display: inline-block;\"> AnyNet Structure (Search Space)\n",
    "   </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0398cca-b9c1-4e42-a8fb-e7d90b8f5754",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 9 : RNN\n",
    "- Two form of sequence to sequence task:\n",
    "  - **aligned**: input at certain time step aligns with corrsponding output, like tagging (fight -> verb)\n",
    "  - **unaligned**: no step-to-step correspondence, like maching translation\n",
    "- **Autoregressive** model: regress value based on previous value\n",
    "  - latent autoregressive models (since $h_t$ is never observed): estimate $P(x_t | x_{t-1} \\dots x_1)$ with $\\hat x_t = P(x_t | h_t)$ and $h_t = g(h_{t-1}, x_{t-1})$\n",
    "- **Sequence Model**: to get joint probablity of a sequence $p(x_1, \\dots, x_T)$, we change it to a form like autoregressive one: $p(x_1) \\prod_{t=2}^T p(x_t|x_{t-1}, \\dots, x_1)$\n",
    "  - **Markov Condition**: if we can make the condition above into $x_{t-1}, \\dots, x_{t-\\tau}$ without any loss, aka the future is conditionally independent of the past, given the recent history, then the sequence satisfies a Markov condition. And it is $\\tau^{th}$-order Markov model.\n",
    "- Zipf’s law: the frequency of words will decrease exponentially, n-grams too (with smaller slope).\n",
    "  - So use word frequency to construct the probility is not good, for example. $\\hat p(learning|deep) = n(deep, learning) / n(deep)$, $n(deep, learning)$ will be very small compared to denominator. We can use so called **Laplace Smooth** but that will not help too much.\n",
    "- **Perplexity**: (how confusion it is), given a true test data, the cross-entropy is $J = \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t | x_{t-1}, \\dots, x_1)$, and the perplexity is $\\exp(J)$.\n",
    "- Partioning the sequence: for a $T$ token indices sequence, we add some randomness, discard first $d \\in U(0, n]$ tokens and partion the rest into $m = \\lfloor (T-d) / n \\rfloor$ group. For a sequence $x_t$ the target sequence is shifted by one token $x_{t+1}$.\n",
    "---\n",
    "- **RNN**: for a vocab with size $|\\mathcal V|$, the model parameters should go up to $|\\mathcal V|^n$, $n$ is the sequence length.So we $P(x_t | x_{t-1} \\dots x_1) \\approx P(x_t | h_{t-1})$，$h$ is a **hidden state**, it varies at different time step and contains information of previous time steps. Hidden layer, on the other hand, is a structure, it dose not change in forward calculation.\n",
    "  - recurrent: $H_t = \\phi (X_tW_{th} + H_{t-1}W_{hh} + b_h)$, output is $O_t = H_tW_{tq} + b_q$.\n",
    "  - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       RNN Block\n",
    "  - clip the gradient: $g = \\min(1, \\frac{\\theta}{|| g ||}) g$, it is a hack but useful.\n",
    "  - **Warm-up**: When predicting, we can first feed a prefix (now called prompt I think), just iter the prefix into the network without generating output until we need to predict.\n",
    "- For RNN: the input shape is (sequence_length, batch_size, feature_size), first is time_step, third is one-hot dim or word2vec dim.\n",
    "- **Backpropagation through time**\n",
    "  - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> Computation graph of RNN\n",
    "  - How to reduce gradient explosion or vanishing: truncate the gradient propagete at certain time step.\n",
    "  - In the img above: $\\frac{\\partial L}{\\partial h_T} = W_{qh}^{\\intercal} \\frac{\\partial L}{\\partial o_T}$, $\\frac{\\partial L}{\\partial h_t} = \\sum_{i=t}^T (W_{hh}^{\\intercal})^{T-i} W_{qh}^{\\intercal} \\frac{\\partial L}{\\partial o_{T+t-i}}$, $\\frac{\\partial L}{\\partial W_{hx}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} x_t^{\\intercal}$, $\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} h_{t-1}^{\\intercal}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5f197-9d57-42e8-8db4-2196d7d04264",
   "metadata": {},
   "source": [
    "## Chapter 10 : Modern RNN\n",
    "- **LSTM**\n",
    "  - The structure is :\n",
    "  - <img alt=\"LSTM Arch\" src=\"https://zh.d2l.ai/_images/lstm-3.svg\" style=\"background-color: white; display: inline-block;\"> LSTM Arch\n",
    "- **GRU**\n",
    "  - <img alt=\"GRU Arch\" src=\"https://d2l.ai/_images/gru-3.svg\" style=\"background-color: white; display: inline-block;\"> GRU Arch\n",
    "  - Reset gates help capture short-term dependencies in sequences.\n",
    "  - Update gates help capture long-term dependencies in sequences.\n",
    "- **Deep RNN**\n",
    "  - <img alt=\"Deep RNN\" src=\"https://d2l.ai/_images/deep-rnn.svg\" style=\"background-color: white; display: inline-block;\"> Deep RNN\n",
    "  - In deep rnn, the output is the last layer of the hidden state with every timestep, and state is the last time step hidden state with all layer of rnn.\n",
    "- **Bidirection RNN**, it is slow and gradient chain is long\n",
    "  - $P(x_1,\\ldots,x_T,h_1,\\ldots,h_T)=\\prod_{t=1}^TP(h_t\\mid h_{t-1})P(x_t\\mid h_t),\\mathrm{~where~}P(h_1\\mid h_0)=P(h_1)$, it is a hidden markov model. We can use dynamic programming method compute is from start to end, also from end to start. Just how B-RNN is capable of.\n",
    "  - <img alt=\"Bidirection RNN\" src=\"https://zh.d2l.ai/_images/birnn.svg\" style=\"background-color: white; display: inline-block;\"> B-RNN\n",
    "  - And we just need to concatenate these two H.\n",
    "- **Machine translation**\n",
    "  - non-breaking space, some space should not split to new line, like Mr. Smith.\n",
    "  - Teacher Forcing : all the input will be pad with \\<pad\\>, source token no special treat, decoder input (target seq use as input) will start with \\<bos\\>, and label is shift by 1 (no \\<bos\\> at the begining).\n",
    "  - **Important**: when use teacher forcing, the truth target is feed to the decoder. This will make the traning faster and stable, but it will make training and predicting different (because when predicting we do not have truth target label, we have to repeatedly predict). We can make them the same, but the tranning will be harder.\n",
    "- **Sequence to Sequence**\n",
    "  - We use this Encoder - Decoder Arch to get varied length input and varied length output.\n",
    "  - We do not use one-hot, instead we use nn.Embed layer, which will take token i, and return ith row of the matrix of this embeding layer.\n",
    "  - From the encoder, we get the hidden states, and use a funcion $c = q(h_1, \\cdots, h_T)$, for example, just use the $h_T$. And in the decoder, we concatenate this with the target embed output, and feed to rnn.\n",
    "  - When calculating the loss, we should not take \\<pad\\> into acount. So we need to musk the loss with the tokens.\n",
    "  - <img alt=\"Encoder Decoder\" src=\"https://d2l.ai/_images/seq2seq-details.svg\" style=\"background-color: white; display: inline-block;\"> Encoder Decoder\n",
    "  - Bilingual Evaluation Understudy, BLEU evaluates whether this n-gram in the predicted sequence appears in the target sequence. For example, target sequence ABCDEF, predict sequence ABBCD, $p_1 = 4/5$, we have ABCD in the target sequence, $p_2 = 3 / 4$, we have AB, BC, CD. So we get BLEU as $\\exp\\left(\\min\\left(0,1-\\frac{\\mathrm{len}_{\\mathrm{label}}}{\\mathrm{len}_{\\mathrm{pred}}}\\right)\\right)\\prod_{n=1}^kp_n^{1/2^n}$, higher n will have higher weight, small length of predict length takes lower.\n",
    "- **Beam Search**\n",
    "  - Before this section, we use greedy search to get prediction, use argmax on the prediction vector : $y_{t^{\\prime}}=\\underset{y\\in\\mathcal{Y}}{\\operatorname*{\\operatorname*{argmax}}}P(y\\mid y_1,\\ldots,y_{t^{\\prime}-1},\\mathbf{c})$, where $\\mathcal Y$ is the vacab. Once our model outputs “<eos>” (or we reach the maximum length $T'$) the output sequence is completed.\n",
    "  - However, use the most likely tokens is not the same with the most likely sequence : $\\prod_{t^{\\prime}=1}^{T^{\\prime}}P(y_{t^{\\prime}}\\mid y_1,\\ldots,y_{t^{\\prime}-1},\\mathbf{c})$. For example, in this figure below, ACB will have this probability of 0.5 * 0.3 * 0.6 = 0.09. On the other hand, greedy search choose ABC which is 0.5 * 0.4 * 0.4 = 0.08, it is lower, not optimal!\n",
    "  - <img alt=\"Max sequence\" src=\"https://d2l.ai/_images/s2s-prob2.svg\" style=\"background-color: white; display: inline-block;\">Max sequence <img alt=\"Max token\" src=\"https://d2l.ai/_images/s2s-prob1.svg\" style=\"background-color: white; display: inline-block;\"> Max token.\n",
    "  - If we want the optimal one, we need to do exhaustive search, search all possible sequence, it is not possible!\n",
    "  - The most straightforward type of beam search is keep k candidates. In time step 2, we get $P ( A, y_{2} \\mid\\mathbf{c} )=P ( A \\mid\\mathbf{c} ) P ( y_{2} \\mid A, \\mathbf{c} )$ for the top, and $P ( C, y_{2} \\mid\\mathbf{c} )=P ( C \\mid\\mathbf{c} ) P ( y_{2} \\mid C, \\mathbf{c} ) $ for the bottom, then choose most 2 from them. And then choose sequence that maximize $\\frac{1} {L^{\\alpha}} \\mathrm{l o g} \\, P ( y_{1}, \\ldots, y_{L} \\mid\\mathbf{c} )=\\frac{1} {L^{\\alpha}} \\sum_{t^{\\prime}=1}^{L} \\mathrm{l o g} \\, P ( y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, \\mathbf{c} ) ; $. Note tha we have **6** candidates (A, C ..).\n",
    "  - <img alt=\"Max sequence\" src=\"https://d2l.ai/_images/beam-search.svg\" style=\"background-color: white; display: inline-block;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52b3fd-45fa-4fa6-8a6c-4ba56c83b8b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 11 : Attention Mechanisms and Transformers\n",
    "- **Idea** : attention first come up in encoder - decoder design, rather than tranform the input to a fixed size feature and the feed it to all the decoder step, we want to create a representation that has the same length of input and decoder at each time step can pay attention to different input sequence (with it's weight). And transfromer give up residual connection, instead use attention at all.\n",
    "- **Q, K, V**\n",
    "  - Define database $\\mathcal{D} \\stackrel{\\mathrm{d e f}} {=} \\{( \\mathbf{k}_{1}, \\mathbf{v}_{1} ), \\ldots( \\mathbf{k}_{m}, \\mathbf{v}_{m} ) \\} $, give some query, the attention is $\\text{Attention}(q, D) \\stackrel{\\mathrm{d e f}} {=} \\sum_{i=1}^m \\alpha(q, k_i)v_i $, where $\\alpha(q, k_i)$ are scalar attention weight, this operation is also called attention pooling. We want this attention weight to be larger than 0 and sum up to 1, so we can use a softmax to transfrom it.\n",
    "- **Attention pooling with similarity**\n",
    "  - In a regression task, we can use kernel output as the attention weight (after normalization).\n",
    "  - When directly compute loss $(f(x_i) - y_i)^2$， because we have $y_i$, the $\\sigma$ will go to zero, causing overfitting. Even if we remove $y_i$ from the loss, if the dataset is large enough, we may still overfit.\n",
    "  - <img alt=\"Attention Pooling\" src=\"https://d2l.ai/_images/attention-output.svg\" style=\"background-color: white; display: inline-block;\"> Attention Pooling\n",
    "- **Attention Scoring Function**\n",
    "  - **Dot Product Attention**: note that kernel of gaussian is $a(q, k_i) = q^{\\intercal}k_i - 0.5 * ||q||^2 - 0.5 * ||k_i||^2$, and after the softmax normalization, second one is cancled out. Then if we get $k_i$ with batch or layer normalization, it's length will be bounded and often constant, so we can get rid of last term without penalty. This leads to $a(q, k_i) = q^{\\intercal}k_i$, ann assume both $q$ and $k_i$ have zero mean and unit variance, this attention weight will have a variance of $d$ (which is query feature size), so we can further normalize it : $a(q, k_i) = q^{\\intercal}k_i / \\sqrt{d}$, this is the common one used in transformer. At last, we do a softmax on it.\n",
    "  - Because we do not want consider \\<pad\\>, so we can do musk softmax. And for Batch Matrix Multiplication, use torch.bmm. bmm(Q, K) take Q(n, a, b) and K(n, b, c), which return [Q1 @ K1, ..., Q_n @ K_n].\n",
    "  - **Additive Attention**: when q and k has different feature size, we can do a transform ($q^{\\intercal}Mk$), or use this additive attention $a(\\mathbf{q},\\mathbf{k})=\\mathbf{w}_v^\\top\\tanh(\\mathbf{W}_q\\mathbf{q}+\\mathbf{W}_k\\mathbf{k})\\in\\mathbb{R}$, inside the () we add by broadcasting.\n",
    "- **Bahdanau Attention**\n",
    "  - Attention function will work between encoder hidden state and decoder hidden state, $c_{t'} = \\sum_t^T a(s_{t'-1}, h_t)h_t$, and this will used to generate $s_{t'}$.\n",
    "  - <img alt=\"Bahdanau Attention\" src=\"https://d2l.ai/_images/seq2seq-details-attention.svg\" style=\"background-color: white; display: inline-block;\"> Bahdanau Attention\n",
    "  - Seq2SeqAttentionDecoder first use encoder last layer hidden state as the query, and later use it's own hidden state from it's rnn model. Keys and values are all encoder outputs (last layer hidden state at all time step), then concatenate the embed X with this context (attention result) then feed to it's rnn.\n",
    "- **Multi-head Attention**\n",
    "  - <img alt=\"Multi-head Attention\" src=\"https://d2l.ai/_images/multi-head-attention.svg\" style=\"background-color: white; display: inline-block;\"> Multi-head Attention\n",
    "  - We want the same Q, K, V to have different behaviour with the same attention mechanism, so we have to copy them $h$ times and first pass them into FC layer which has learnable param that can change the QKV, then feed to attention, get $h$ results, concatenate them. $h_i = f(W_i^{q}q_i, W_i^kk_i, W_i^vv_i)$, $f$ is attention pooling, each $W$ have shape of $(p_q, d_q), (p_k, d_k)$ and $(p_v, d_v)$, $h_i$ is of shape $(p_v,)$. We concatenate these h to a $(h \\times p_v,)$ shape. And we use a big learnable matrix of shape $(p_o, h \\times p_v)$ times the concatenated result, which finnally return output of shape $(p_o, )$. For the purpose of parallel computation, we set $hp_q = hp_k = hp_v = p_o$.\n",
    "  - Impl of d2l is the same idea, but for parrallel, it use some trick, hidden_size is h * p_q, put num_head into batch_size, make the batch_size = batch_size * num_head, and in the output reverse it.\n",
    "- **Self Attention**\n",
    "  - Do the attention(X, X, X) to get encoder. Compare CNN, RNN and self-attention, given n sequence with d dimension. CNN : choose kernel of 3, computation is O(knd^2), longest connect path is O(n/k). RNN : compute O(nd^2), path O(n). Self attention : compute O(n^2d), path O(1).\n",
    "  - Positional Encoding: self attention does not contain position (order) information, so a token at time step 1 and 5 are the same (but it should not!). So we need to add something to keep the position information. First we use fixed position encoding with sine and cosine. $X^{n \\times d}$ is the input representation of n tokens, and the position encoding is $X + P$, where $p_{i,2j} = \\sin \\left( \\frac{i}{10000^{2j/d}} \\right)$ and $p_{i,2j+1} = \\cos \\left( \\frac{i}{10000^{2j/d}} \\right)$. This works because the function abouve contain different frequency information.\n",
    "  - Relative position encoding : $\\begin{bmatrix}\n",
    "\\cos(\\delta\\omega_j) & \\sin(\\delta\\omega_j) \\\\\n",
    "-\\sin(\\delta\\omega_j) & \\cos(\\delta\\omega_j)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "p_{i,2j} \\\\\n",
    "p_{i,2j+1}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "p_{i+\\delta,2j} \\\\\n",
    "p_{i+\\delta,2j+1}\n",
    "\\end{bmatrix}$, we can just add a (1, step, hidden) param, and add it to the embed(X) to learn the position.\n",
    "- **Transformer**\n",
    "  - <img alt=\"Transformer\" src=\"https://d2l.ai/_images/transformer.svg\" style=\"background-color: white; display: inline-block;\"> Transformer Arch\n",
    "  - The encoder-decoder attention layer take decoder self-attention layer output as query, and encoder output as key and value.\n",
    "  - Note that in decoder self-attention, we will carefully mask output to reserve the autoregreesive nature, we do not take position in the outpt (later as the input of decoder self-attention layer) after the position we are calculating.\n",
    "  - Before pos encoding we first multiply sqrt(d) with embed(X) to rescale it, maybe because embed(X) has small variace.\n",
    "  - For prediction, we need to cache the input X for the decoder, in training we can just compute all time step all together. In impl, it is cached in state[2].\n",
    "  - **!!** only the last output of the encoder will do attention on all block of decoder.\n",
    "- **Vision Transformer**\n",
    "  - <img alt=\"Vision Transformer\" src=\"https://d2l.ai/_images/vit.svg\" style=\"background-color: white; display: inline-block;\"> Vision Transformer\n",
    "  - patch embeding will feed to a conv then flatten it, return shape of (batch, patch, hidden)\n",
    "  - Do the normalization before the attention is better for the efficient learning of transformer. The vit mlp layer use GELU and dropout is applied to the output of each fully connected layer in the MLP for regularization.\n",
    "\n",
    "- Large Scale Pre-training\n",
    "  - Encoder only, ViT, BERT. BERT use masked language modeling, and for a token, tokens at left and right can all attend to this masked token. So it is a bidirection encoder --- in the figure below, each token along the vertical axis attends to all input tokens along the horizontal axis.\n",
    "  - <img alt=\"BERT\" src=\"https://d2l.ai/_images/bert-encoder-only.svg\" style=\"background-color: white; display: inline-block;\"> BERT\n",
    "  - Encoder-Decoder, BART & T5, both attempt to reconstruct original text in their pretraining objectives, while the former emphasizes noising input (e.g., masking, deletion, permutation, and rotation) and the latter highlights multitask unification with comprehensive ablation studies.\n",
    "  - <img alt=\"T5\" src=\"https://d2l.ai/_images/t5-encoder-decoder.svg\" style=\"background-color: white; display: inline-block;\"> T5\n",
    "  - Decoder only, GPT. **In-context learning** : conditional on an input sequence with the task description, task-specific input–output examples, and a prompt (task input). \n",
    "  - <img alt=\"GPT\" src=\"https://d2l.ai/_images/gpt-decoder-only.svg\" style=\"background-color: white; display: inline-block;\"> GPT\n",
    "  - <img alt=\"x - shot\" src=\"https://d2l.ai/_images/gpt-3-xshot.svg\" style=\"background-color: white; display: inline-block;\"> x-shot\n",
    "- Efficient Transformer design (see that survey)\n",
    "  - Sparse attention：Longformer：使用滑动窗口和全局注意力，降低复杂度到O(n)。BigBird：结合随机注意力、窗口注意力和全局注意力，适合长序列。\n",
    "  - Low rank approximation：Linformer：通过低秩分解将注意力矩阵投影到较低维度，复杂度从O(n²)降到O(n)。\n",
    "  - Memory：Transformer-XL：引入循环记忆，处理长序列时重用之前的隐藏状态，避免重复计算。\n",
    "  - Efficient attention：Performer：使用核方法（Favor+）近似点积注意力，复杂度降为O(n)。\n",
    "  - Model compress：Distillation：将大Transformer蒸馏为小模型（如DistilBERT）。量化：减少参数精度，降低内存占用。\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\"> -->\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee3e187-0c67-4bc3-a076-bdfd591cfadf",
   "metadata": {},
   "source": [
    "## Chapter 12 : Optimization\n",
    "- **Convexity**\n",
    "  - Convex set : for any $a, b \\in \\mathcal X$, given $\\lambda \\in [0, 1]$, $\\lambda a + (1 - \\lambda) b \\in \\mathcal X$.\n",
    "  - Convex function : for any function $f : \\mathcal X \\rightarrow \\mathbb R$, we have $\\lambda f(x) + (1 - \\lambda) f(x') >= f(\\lambda x + (1-\\lambda)x')$.\n",
    "  - jensen's inequality : $\\sum_i\\alpha_if(x_i)\\geq f\\left(\\sum_i\\alpha_ix_i\\right)$ and $E_X[f(X)]\\geq f\\left(E_X[X]\\right)$\n",
    "  - Properties : Local Minima Are Global Minima, below set $\\mathcal{S}_b\\overset{\\mathrm{def}}{\\operatorname*{=}}\\{x|x\\in\\mathcal{X}\\mathrm{~and~}f(x)\\leq b\\}$ is also a convex set, f is convex if hessian of f is positive semidefinite ($\\nabla^2 f = H, x^THx >= 0$).\n",
    "  - Convex with constraint $c_i(x) <= 0$, can be dealed with lagrangian, and the KKT condition. KKT are:\n",
    "    - Stationarity : $\\nabla_x L(f(x), \\lambda_1, \\ldots, \\lambda_n) = 0$.\n",
    "    - Primal Feasibility : $c_i(x) <= 0$\n",
    "    - Dual Feasibility : $\\lambda_i >= 0$\n",
    "    - Complementary Slackness : $\\lambda_i c_i(x) = 0$\n",
    "  - Penality is robust than constraint. We can also use projection to satisfy constraints.\n",
    "- **Gradient Descent**\n",
    "  - $x \\leftarrow x - \\eta \\nabla_x f(x)$, with newton's method $\\eta = \\nabla_x^{-2} f(x) = H^{-1}$\n",
    "  - H is expensive, so we can use precondition $x \\leftarrow x - \\eta \\text{diag}(H)^{-1}\\nabla_x f(x)$, this means for different $x_i$ we use different learning rates.\n",
    "  - Line search : use binary search to find $\\eta$ that minimize $f(x - \\eta \\nabla_x f(x))$.\n",
    "- **SGD**： converge with rate $\\mathcal O (1/\\sqrt T)$, $T$ is the sample number. More details of the math please see the book.\n",
    "- **Momentum**\n",
    "  - Use leaky average $v_k = \\beta v_{k-1} + g_{k, k-1}$ as the gradient, this is the momentum!\n",
    "  - Gradient descent with and without momentum for a convex quadratic function decomposes into coordinate-wise optimization in the direction of the eigenvectors of the quadratic matrix.\n",
    "  - The velocity converge condition is loose than gradient converge condition, so add momentum (with big $\\beta$ ) is theoritaly better.\n",
    "- **Adagrad** : it is a SGD alg\n",
    "  - Some features are rare, so we want to update it faster ( we do not update their gradient much ).\n",
    "  - Some problem has large condition number $k = \\lambda_{max} / \\lambda_{min}$, which is not good. We can rescale them by some matrix (if Hessian of the problem L is possitive semidefinite), or just rescalse the diag of the Q. $\\tilde Q = \\text{diag}(Q)^{-1/2}Q\\text{diag}(Q)^{-1/2}$. However this is not realistic in DL, because we don't have second derivitive of Q, so Adagrad use the norm of the gradient as the scalse item. And this makes it adjust element wise (like only diag will change).\n",
    "  - $s_t = s_{t-1} + g_t^2, w_t = w_t - \\eta / \\sqrt{s_t+\\epsilon} \\odot g_t$, one problem of Adagrad is that it's learning rate decrease $\\mathcal O(t^{-1/2})$.\n",
    "- **RMSProp**\n",
    "  - $s_t = \\gamma s_{t-1} + (1-\\gamma) g_t^2$, only difference with Adagrad\n",
    "- **Adadelta**\n",
    "  - $\\mathbf{s}_{t}=\\rho \\mathbf{s}_{t-1}+(1-\\rho) \\mathbf{g}_{t}^{2}$, $\\mathbf{g}_{t}^{\\prime}=\\frac{\\sqrt{\\Delta \\mathbf{x}_{t-1}+\\epsilon}}{\\sqrt{\\mathbf{s}_{t}+\\epsilon}} \\odot \\mathbf{g}_{t}$, $x_t = x_t - \\mathbf{g}_{t}^{\\prime}$, $\\Delta\\mathbf{x}_{t}=\\rho\\Delta\\mathbf{x}_{t-1}+( 1-\\rho) \\mathbf{g}_{t}^{\\prime\\, 2}, $.\n",
    "- **Adam**\n",
    "  - $v_t = \\beta_1 v_{t-1} + (1-\\beta_1)g_{t}$, $s_t = \\beta_2 s_{t-1} + (1-\\beta_2)g^2_{t}$, and the rescale it (otherwise the initial numbers are too diverge from gradient), $\\hat v_t = v_t / (1 + \\beta_1^t)$, $\\hat s_t = s_t / (1 + \\beta_2^t)$. Then finnally $x_t = x_t - \\eta \\hat v_t / (\\sqrt{\\hat s_t} + \\epsilon)$\n",
    "  - One of the problems of Adam is that it can fail to converge even in convex settings when the second moment estimate in $s_t$ blows up as $g^2_t$ being too large and forget the history. Yogi update is $s_t = s_{t-1} + (1-\\beta_2)g^2_{t} \\odot (g^2_{t} - s_{t-1})$, the update is not the deviation of $g^2_{t} - s_{t-1}$, it is $g^2_{t}$ with regard to the sign.\n",
    "- Scheduler\n",
    "  - Warmup: In particular they find that a warmup phase limits the amount of divergence of parameters in very deep networks. A closer look at deep learning heuristics: learning rate restarts, warmup and distillation. ArXiv:1810.13243.\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\"> -->\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bd627-0c39-4250-beab-8bd912b57950",
   "metadata": {},
   "source": [
    "## Chapter 13 : Computation\n",
    "- compiler\n",
    "  - net = torch.jit.script(net)\n",
    "- Automatic Parallesim\n",
    "  - y.to('cpu', non_blocking=non_blocking) for y in x, will return x[i-1] when calculate x[i]\n",
    "- Tranning on multiple GPU\n",
    "  - <img alt=\"Partion Methods\" src=\"https://d2l.ai/_images/splitting.svg\" style=\"background-color: white; display: inline-block;\"> Partion Methods\n",
    "  - nn.parallel.scatter to split data to different devices\n",
    "  - 显式同步（torch.cuda.synchronize()）仅在需要精确测量执行时间或调试异步错误时必要，其他情况会自己根据cpu或者后续数据需求隐式调用\n",
    "- Concise impl :\n",
    "  - What we need to do\n",
    "    - Network parameters need to be initialized across all devices.\n",
    "    - While iterating over the dataset minibatches are to be divided across all devices.\n",
    "    - We compute the loss and its gradient in parallel across devices.\n",
    "    - Gradients are aggregated and parameters are updated accordingly.\n",
    "  - Use torch.nn.parallel.DistributedDataParallel\n",
    "- Parameter Server\n",
    "  - <img alt=\"Parameter Exchange\" src=\"https://d2l.ai/_images/ps-distributed.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "  - last graph above assume gradient can be divided into four parts, and exchange each one of them each GPU.\n",
    "  - Ring Synchronization\n",
    "  - Key–Value Stores\n",
    "\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\"> -->\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd5258-54da-404e-948e-0a1c5ce85984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd36908-a9f8-413d-8c45-011c7e2cd889",
   "metadata": {},
   "source": [
    "在**深度学习（DL）工程**和**硬件优化**方面，需要掌握一系列工具、技术和最佳实践，以确保模型能够高效训练、优化和部署。  \n",
    "\n",
    "---\n",
    "\n",
    "# **1. 深度学习工程**\n",
    "**目标**：不仅要训练模型，还要能够在实际应用中高效地**数据处理、训练、调优、部署和维护**。\n",
    "\n",
    "## **1.1 数据工程**\n",
    "深度学习的性能很大程度上依赖于数据质量和预处理效率。\n",
    "\n",
    "### **(1) 数据收集与存储**\n",
    "- **结构化数据**（SQL, Pandas, BigQuery）\n",
    "- **图像数据**（OpenCV, PIL, TensorFlow Datasets）\n",
    "- **文本数据**（NLTK, Hugging Face Datasets）\n",
    "- **流数据**（Kafka, Apache Spark）\n",
    "\n",
    "### **(2) 数据预处理**\n",
    "- **标准化 / 归一化**（Min-Max Scaling, Z-score）\n",
    "- **数据增强**（图像：旋转、裁剪；文本：同义词替换）\n",
    "- **降维**（PCA, t-SNE, UMAP）\n",
    "- **缺失值处理**（均值填充、插值）\n",
    "\n",
    "### **(3) 数据加载优化**\n",
    "- **批量加载（Batch Loading）**\n",
    "- **多线程 / 多进程数据预处理（Dataloader, TensorFlow tf.data）**\n",
    "- **TFRecord / HDF5**（二进制格式加速数据读取）\n",
    "\n",
    "---\n",
    "\n",
    "## **1.2 训练与超参数调优**\n",
    "深度学习模型训练是一个计算密集型过程，需要高效的**优化策略**和**超参数调整**。\n",
    "\n",
    "### **(1) 训练优化**\n",
    "- **优化器选择**\n",
    "  - SGD（标准梯度下降）\n",
    "  - Adam / RMSprop（自适应优化）\n",
    "  - LARS / LAMB（用于大规模分布式训练）\n",
    "  \n",
    "- **正则化**\n",
    "  - Dropout（随机丢弃神经元）\n",
    "  - Batch Normalization（批量归一化）\n",
    "  - Weight Decay（L2 正则化）\n",
    "\n",
    "- **梯度裁剪（Gradient Clipping）**\n",
    "  - 解决梯度爆炸问题\n",
    "\n",
    "### **(2) 超参数优化**\n",
    "自动搜索最优超参数（例如学习率、batch size、权重初始化）。\n",
    "- **Grid Search（网格搜索）**\n",
    "- **Random Search（随机搜索）**\n",
    "- **Bayesian Optimization（贝叶斯优化）**\n",
    "- **Hyperband（高效采样）**\n",
    "- **Optuna / Ray Tune（自动化超参数调优工具）**\n",
    "\n",
    "---\n",
    "\n",
    "## **1.3 训练加速**\n",
    "大规模训练时需要高效的训练加速技术：\n",
    "\n",
    "### **(1) GPU 加速**\n",
    "- 训练时尽可能利用 **CUDA** / **cuDNN**\n",
    "- **混合精度训练（Mixed Precision）**：使用 FP16（Half Precision）加速计算\n",
    "- **数据并行（DataParallel）** vs. **模型并行（ModelParallel）**\n",
    "  \n",
    "### **(2) 分布式训练**\n",
    "- **单机多卡（Multi-GPU Training）**\n",
    "  - PyTorch `DataParallel`\n",
    "  - PyTorch `DistributedDataParallel (DDP)`\n",
    "  \n",
    "- **多机多卡（Multi-Node Training）**\n",
    "  - TensorFlow `MirroredStrategy`\n",
    "  - Horovod（Uber 提出的高效分布式训练框架）\n",
    "\n",
    "---\n",
    "\n",
    "## **1.4 部署与推理优化**\n",
    "深度学习不仅要训练，还要在**边缘设备**或**服务器端**高效推理。\n",
    "\n",
    "### **(1) 模型压缩**\n",
    "- **剪枝（Pruning）**：去掉不重要的权重\n",
    "- **量化（Quantization）**：\n",
    "  - **8-bit INT 量化**（TensorRT, TFLite）\n",
    "  - **混合精度推理（FP16, INT8）**\n",
    "\n",
    "- **知识蒸馏（Knowledge Distillation）**：\n",
    "  - 用大模型训练小模型，提高推理效率\n",
    "\n",
    "### **(2) 推理框架**\n",
    "- **ONNX（Open Neural Network Exchange）**：模型通用格式，可用于 PyTorch / TensorFlow 互转\n",
    "- **TensorRT（NVIDIA）**：高效的 GPU 加速推理\n",
    "- **TVM（Apache）**：自动优化模型推理\n",
    "\n",
    "### **(3) 部署方式**\n",
    "- **服务器部署**\n",
    "  - Flask / FastAPI（REST API 部署）\n",
    "  - TensorFlow Serving / TorchServe（高效模型服务）\n",
    "\n",
    "- **移动端 / 边缘部署**\n",
    "  - TensorFlow Lite（TFLite）\n",
    "  - CoreML（iOS 设备）\n",
    "  - NVIDIA Jetson（嵌入式 AI）\n",
    "\n",
    "---\n",
    "\n",
    "# **2. 硬件优化**\n",
    "深度学习的计算量极大，硬件的优化能**显著提高训练和推理速度**。\n",
    "\n",
    "## **2.1 GPU 计算**\n",
    "GPU 是深度学习的核心计算设备，NVIDIA CUDA 生态至关重要。\n",
    "\n",
    "### **(1) GPU 编程基础**\n",
    "- CUDA 编程（掌握 Kernel 编写）\n",
    "- cuDNN（深度学习优化库）\n",
    "- Tensor Core（用于混合精度计算）\n",
    "  \n",
    "### **(2) GPU 训练优化**\n",
    "- **减少 CPU-GPU 传输**（优化 `pin_memory=True`）\n",
    "- **梯度累积（Gradient Accumulation）**，减少显存占用\n",
    "- **使用 FP16 训练**（提高吞吐量）\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2 分布式计算**\n",
    "适用于**超大规模数据训练**（如 GPT、Llama 等模型）。\n",
    "\n",
    "### **(1) 并行策略**\n",
    "- **数据并行（Data Parallelism）**\n",
    "  - 复制模型到多个 GPU，每个 GPU 训练不同数据\n",
    "  - PyTorch `DistributedDataParallel (DDP)`\n",
    "\n",
    "- **模型并行（Model Parallelism）**\n",
    "  - 适用于超大模型（如 GPT-4）\n",
    "  - DeepSpeed / Megatron-LM 优化\n",
    "\n",
    "- **流水线并行（Pipeline Parallelism）**\n",
    "  - 将不同层分配到不同 GPU，提高计算效率\n",
    "  - **适用于 Transformer 训练**\n",
    "\n",
    "### **(2) 高效通信**\n",
    "- **NCCL（NVIDIA Collective Communication Library）**：优化 GPU 之间的通信\n",
    "- **RDMA（远程直接内存访问）**：用于 GPU 服务器间高速通信\n",
    "\n",
    "---\n",
    "\n",
    "## **2.3 专用 AI 硬件**\n",
    "除了 GPU，AI 训练还可以用专用芯片加速：\n",
    "- **TPU（Google）**：专门优化深度学习计算\n",
    "- **Graphcore IPU**（稀疏计算优化）\n",
    "- **Cerebras Wafer-Scale Engine**（超大规模 AI 计算）\n",
    "\n",
    "---\n",
    "\n",
    "# **3. 总结**\n",
    "| **类别** | **关键内容** |\n",
    "|----------|--------------|\n",
    "| **数据工程** | 数据清洗、数据增强、数据加载优化 |\n",
    "| **训练优化** | 超参数调优、正则化、优化器选择 |\n",
    "| **训练加速** | GPU 加速、混合精度、分布式训练 |\n",
    "| **部署优化** | 模型量化、剪枝、TensorRT 加速 |\n",
    "| **硬件优化** | CUDA、NCCL、TPU/FPGA |\n",
    "\n",
    "你已经有**矩阵分解和 Rust 经验**，如果想深入工程优化，可以：\n",
    "1. **研究 PyTorch DDP / DeepSpeed**（分布式训练优化）\n",
    "2. **学习 CUDA / cuDNN 编程**（低级 GPU 加速）\n",
    "3. **尝试 TensorRT / ONNX**（推理加速）\n",
    "\n",
    "这将让你在 **深度学习工程 & 硬件优化** 方面具备更强的竞争力 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c15dc5c-18d4-4f93-b9a1-938224653390",
   "metadata": {},
   "source": [
    "## 梯度检查点\n",
    "\n",
    "梯度检查点（Gradient Checkpointing，也叫 Checkpointing 或 Recomputation）是一种在训练深度神经网络时用来**节省内存**的技术，尤其在网络非常深或批量大小（batch size）较大时非常有用。它的核心思想是：在内存有限的情况下，通过牺牲一些计算时间来减少存储激活值（中间输出）的内存需求。下面我将详细解释它的原理、实现方式以及优缺点。\n",
    "\n",
    "---\n",
    "\n",
    "### 背景：为什么需要梯度检查点？\n",
    "在深度神经网络的训练中，反向传播需要用到前向传播时每一层的激活值。这些激活值通常会被存储在内存中，以便在计算梯度时直接使用。然而：\n",
    "- 对于深层网络（比如 ResNet-101、Transformer 等），层数非常多，激活值的内存占用会变得非常大。\n",
    "- 当批量大小增加时，激活值的存储需求进一步线性增长。\n",
    "- 如果显存（如 GPU 内存）不足，可能无法训练模型，或者只能使用很小的批量大小，影响训练效果。\n",
    "\n",
    "梯度检查点的目标是通过**不存储所有激活值**，而是在需要时重新计算它们，从而大幅减少内存占用。\n",
    "\n",
    "---\n",
    "\n",
    "### 基本原理\n",
    "正常情况下，训练一个神经网络的过程是：\n",
    "1. **前向传播**：从输入到输出，计算每一层的激活值并存储。\n",
    "2. **反向传播**：利用存储的激活值和损失函数的梯度，计算每一层参数的梯度。\n",
    "\n",
    "梯度检查点的工作方式是：\n",
    "- 在前向传播中，**只保存部分关键层的激活值**（这些点称为“检查点”），而不是每一层的激活值。\n",
    "- 在反向传播时，对于未保存激活值的层，通过从最近的检查点重新运行前向传播来**重新计算激活值**，然后继续计算梯度。\n",
    "\n",
    "这种方法用额外的计算（重新计算激活值）换取了内存的节省。\n",
    "\n",
    "---\n",
    "\n",
    "### 详细步骤\n",
    "假设一个简单的网络有 5 层：\\( L_1 \\rightarrow L_2 \\rightarrow L_3 \\rightarrow L_4 \\rightarrow L_5 \\)。\n",
    "\n",
    "#### 普通训练\n",
    "1. 前向传播：计算并存储所有层的激活值 \\( A_1, A_2, A_3, A_4, A_5 \\)。\n",
    "2. 反向传播：从 \\( L_5 \\) 开始，利用 \\( A_5, A_4, \\ldots, A_1 \\) 计算每一层的梯度。\n",
    "3. 内存需求：存储所有 \\( A_1 \\) 到 \\( A_5 \\)，假设每层激活值占 \\( M \\) 个字节，总共 \\( 5M \\)。\n",
    "\n",
    "#### 使用梯度检查点\n",
    "假设我们选择 \\( L_2 \\) 和 \\( L_4 \\) 作为检查点：\n",
    "1. **前向传播**：\n",
    "   - 计算 \\( L_1 \\rightarrow L_5 \\)，但只保存检查点的激活值 \\( A_2 \\) 和 \\( A_4 \\)。\n",
    "   - 其他层的激活值 \\( A_1, A_3, A_5 \\) 在前向传播后被丢弃。\n",
    "   - 内存需求：仅存储 \\( A_2 \\) 和 \\( A_4 \\)，即 \\( 2M \\)。\n",
    "\n",
    "2. **反向传播**：\n",
    "   - 从 \\( L_5 \\) 开始计算梯度，需要 \\( A_4 \\)（已保存）和 \\( A_5 \\)（未保存）。\n",
    "   - 从 \\( A_4 \\) 重新运行前向传播 \\( L_4 \\rightarrow L_5 \\)，重新计算 \\( A_5 \\)，然后计算 \\( L_5 \\) 和 \\( L_4 \\) 的梯度。\n",
    "   - 继续到 \\( L_3 \\)，需要 \\( A_2 \\)（已保存）和 \\( A_3 \\)（未保存）。\n",
    "   - 从 \\( A_2 \\) 重新运行 \\( L_2 \\rightarrow L_3 \\)，计算 \\( A_3 \\)，然后计算 \\( L_3 \\) 的梯度。\n",
    "   - 最后从 \\( A_2 \\) 和输入重新计算 \\( A_1 \\)，完成 \\( L_2 \\) 和 \\( L_1 \\) 的梯度。\n",
    "\n",
    "3. **内存节省**：只存 2 个检查点的激活值（\\( 2M \\)），而不是 5 个（\\( 5M \\)）。\n",
    "4. **额外计算**：需要重新运行部分前向传播（例如 \\( L_2 \\rightarrow L_3 \\) 和 \\( L_4 \\rightarrow L_5 \\)）。\n",
    "\n",
    "---\n",
    "\n",
    "### 检查点的选择\n",
    "如何选择检查点是一个关键问题：\n",
    "- **均匀分布**：例如每隔 \\( k \\) 层设置一个检查点（如每 10 层）。\n",
    "- **动态选择**：根据内存需求和计算复杂度，优先选择内存占用大的层作为检查点。\n",
    "- **理论最优**：研究表明，最优检查点策略可以将内存需求从 \\( O(N) \\)（N 为层数）降低到 \\( O(\\sqrt{N}) \\)，但实现起来较复杂。\n",
    "\n",
    "现代深度学习框架（如 PyTorch、TensorFlow）通常内置了检查点功能，用户只需指定哪些层或子模块作为检查点。\n",
    "\n",
    "---\n",
    "\n",
    "### 数学分析\n",
    "假设网络有 \\( N \\) 层，每层激活值占 \\( M \\) 字节：\n",
    "- **普通训练**：内存需求 \\( N \\times M \\)。\n",
    "- **检查点训练**：\n",
    "  - 设检查点数为 \\( K \\)（\\( K < N \\)），内存需求为 \\( K \\times M \\)。\n",
    "  - 额外计算量与 \\( N/K \\) 成正比（每个检查点负责的层数）。\n",
    "  - 最优情况下，\\( K \\approx \\sqrt{N} \\)，内存降为 \\( O(\\sqrt{N} \\times M) \\)。\n",
    "\n",
    "例如，\\( N = 100 \\)：\n",
    "- 普通训练：\\( 100M \\)。\n",
    "- 检查点训练（\\( K = 10 \\)）：\\( 10M \\) + 少量额外计算。\n",
    "\n",
    "---\n",
    "\n",
    "### 优点\n",
    "1. **内存效率**：显著减少激活值的存储需求，允许训练更深的网络或使用更大的批量大小。\n",
    "2. **灵活性**：在显存受限的设备上也能运行复杂模型。\n",
    "3. **广泛适用**：适用于 CNN、RNN、Transformer 等各种架构。\n",
    "\n",
    "---\n",
    "\n",
    "### 缺点\n",
    "1. **计算开销**：重新计算激活值增加了训练时间，通常比普通训练慢 20%-50%，具体取决于检查点数量和网络结构。\n",
    "2. **实现复杂性**：需要手动指定检查点或依赖框架支持，调试可能更困难。\n",
    "3. **不适合所有场景**：如果内存不是瓶颈，检查点反而会降低效率。\n",
    "\n",
    "---\n",
    "\n",
    "### 实际应用\n",
    "- **PyTorch 示例**：\n",
    "  ```python\n",
    "  import torch\n",
    "  import torch.nn as nn\n",
    "  from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "  class MyModel(nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.layer1 = nn.Linear(1024, 1024)\n",
    "          self.layer2 = nn.Linear(1024, 1024)\n",
    "          self.layer3 = nn.Linear(1024, 1024)\n",
    "\n",
    "      def forward(self, x):\n",
    "          x = checkpoint(self.layer1, x)  # 检查点包裹层1\n",
    "          x = checkpoint(self.layerThe layer2(x)      # 普通层\n",
    "          x = checkpoint(self.layer3, x)  # 检查点包裹层3\n",
    "          return x\n",
    "\n",
    "  model = MyModel()\n",
    "  input = torch.randn(32, 1024)  # batch_size = 32\n",
    "  output = model(input)\n",
    "  ```\n",
    "\n",
    "- **大模型**：如 GPT、LLaMA 或大型 Transformer，层数可能高达数百层，使用梯度检查点可以显著降低显存需求。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "梯度检查点是一种以时间换空间的优化技术，通过减少激活值的存储来降低内存需求，非常适合显存受限或深层网络的场景。它的核心是选择部分检查点并在反向传播时重新计算其他层的激活值。虽然增加了计算开销，但在内存是瓶颈时，它能让“不可能的任务”变得可行。\n",
    "\n",
    "如果你对实现细节或具体场景有疑问，随时告诉我，我可以进一步展开！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4bad4-c60c-48fe-ac77-818728b22984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
