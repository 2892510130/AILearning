{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4c420e-be32-4965-a531-ff998f6cc05b",
   "metadata": {},
   "source": [
    "### Chapter 2 : Preliminary Knowledge\n",
    "- æ•°æ®æ“ä½œ\n",
    "  - å¹¿æ’­æœºåˆ¶ï¼ˆä¸¤ä¸ªæ•°æ®åˆ†åˆ«å¤åˆ¶æ‰©å……åˆ°åŒæ ·çš„å°ºå¯¸ï¼‰\n",
    "  - èŠ‚çœå†…å­˜ï¼ˆä½¿ç”¨X[:] = \\<expression\\>æˆ–X+=\\<expression\\>æ¥é¿å…é‡æ–°åˆ†é…ï¼‰\n",
    "- æ•°æ®é¢„å¤„ç†\n",
    "- çº¿æ€§ä»£æ•° \n",
    "  - è½¬ç½®.T èŒƒæ•°norm\n",
    "  - éé™ç»´æ±‚å’Œ (keepdims=True)ï¼Œç´¯ç§¯å’Œcumsum\n",
    "  - torch.dotåªæ”¯æŒå‘é‡ï¼ŒçŸ©é˜µå’Œå‘é‡é—´ç”¨mvï¼ŒçŸ©é˜µä¹‹é—´ç”¨mm\n",
    "- å¾®ç§¯åˆ†\n",
    "  - è®¾Tæ˜¯æ¢¯åº¦ç®—ç¬¦ï¼ŒT(Ax) = A.T, T(x.TÂ·A) = A, T(x.T A x) = (A + A.T)x\n",
    "- è‡ªåŠ¨å¾®åˆ†\n",
    "  - åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¼šç´¯ç§¯æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦æ¸…é™¤ä¹‹å‰çš„å€¼\n",
    "  - è‡ªåŠ¨å¾®åˆ†å¿…é¡»æ˜¯æ ‡é‡ï¼Œéæ ‡é‡çš„è¯è¦ä¹ˆè½¬æˆæ ‡é‡ï¼Œè¦ä¹ˆæŒ‡å®šè¾“å‡ºå½¢çŠ¶\n",
    "  - åˆ†ç¦»æ“ä½œ\n",
    "- æ¦‚ç‡è®º\n",
    "- æŸ¥é˜…æ–‡æ¡£ã€APIçš„æŒ‡å¯¼\n",
    "  - diræŸ¥çœ‹å¯ä»¥è°ƒç”¨çš„å‡½æ•°å’Œç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a80380-84c4-4ad0-84e1-4a76d4de7076",
   "metadata": {},
   "source": [
    "### Chapter 3 : Linear Neural Network\n",
    "- Minibatch stochastic gradient descent (å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™)\n",
    "- ä¸€èˆ¬çš„è®­ç»ƒè¿‡ç¨‹\n",
    "  - model.forward() ä¸ y_hat åšå·®ï¼Œç„¶ååå‘ä¼ æ’­ï¼Œä¼˜åŒ–å™¨æ ¹æ®å¯¼æ•°å»æ›´æ–°å‚æ•°\n",
    "- Machine Learning Concept\n",
    "  - lasso regression: l1 norm; ridge regression: l2 norm;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff340e0-b5fd-437e-8f8b-0698e9faf41f",
   "metadata": {},
   "source": [
    "## Chapter 4 : Classification\n",
    "- softmax:\n",
    "  $y_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}$, often minus max(oj) to get numerical stable\n",
    "- Information theory\n",
    "  - cross-entropy lossï¼š$l(y, \\hat y) = - \\sum y_i * \\log(\\hat y_i)$\n",
    "  - amount of information $\\log{\\frac{1}{P(j)}} = - \\log{P(j)}$ \n",
    "  - entorpy $H[P] = \\sum -P(j) \\log{P(j)}$\n",
    "  - cross-entorpy $H(P, Q) = \\sum -P(j) \\log{Q(j)}, ~ P=Q \\rightarrow H(P, Q) = H(P, P) = H(P)$\n",
    "- Image Classification Rules:\n",
    "  - image stored in (channel, height, weight) manner.\n",
    "- Distrubution shift:\n",
    "  - Covariate Shift (feature shift): $p(x) \\neq q(x), p(y|x) = q(y|x)$\n",
    "    - For example: p(x) and q(x) are features of oral and urban house, y is the price, we assume the feature and label relation is the same\n",
    "    - Method: weighted by $\\beta(x) = p(x) / q(x) \\rightarrow \\int\\int l(f(x), y)p(y|x)p(x)dxdy = \\int\\int l(f(x), y)q(y|x)q(x) \\frac{p(x)}{q(x)}dxdy \\rightarrow \\sum_i \\beta_i l(f(x_i), y_i)$, $\\beta$ can be obtained with logistic regression.\n",
    "  - Label Shift, $p(y) \\neq q(y), p(x|y) = q(x|y)$, the same method $\\beta(y) = p(y) / q(y)$, but now $q(y)$ is hard to get, we need compute a confusion matrix on the val data then use the model to pridcit the distrubution of the $q(y)$\n",
    "  - Concept Shift (the concept of the label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab587f-119f-4a04-bbde-9e7d49953d9d",
   "metadata": {},
   "source": [
    "## Chapter 5 : Multilayer Perceptrons\n",
    "- Activation Function: relu, sigmoid, tanh ($\\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}$)\n",
    "- Numerical stability: vanish and explode are common\n",
    "  - Symmetry: linear layer and conv (with no share weight) layer are symmetric so we can tell apart from different weight and try to explain it, so we need to **Bread the Symmetry** (like using a dropout)\n",
    "  - Xavier initilization: get from distrubution of zero mean and variance $\\sigma = \\sqrt{2 / (n_{in} + n_{out})}$\n",
    "  - Dropout, shared param...\n",
    "- (Rolnick et al., 2017) has revealed that in the setting of label noise, neural networks tend to fit cleanly labeled data **first** and only subsequently to interpolate the mislabeled data.\n",
    "  - so we can use early stop once error on val is minimal or the patience hit. usually combined with regularization.\n",
    "- Dropout:\n",
    "  - $h^{'} = \\left \\{ \n",
    "  \\begin{array}{lll}\n",
    "  & 0, p \\\\\n",
    "  & \\frac{h}{1-p}, 1-p\n",
    "  \\end{array} \n",
    "  \\right .$, now $E[h^{'}] = E[h]$\n",
    "  - We do not use dropout in test, except we want to know the uncertainty of the model output (by comparing different dropout)\n",
    "  - Use lower p in lower layer (to get lower feature), higher p in higher layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8510492-eac8-4c69-bc9f-49cbe5acb237",
   "metadata": {},
   "source": [
    "## Chapter 6: Beginner Guide\n",
    "- Tied layer: gradient will add up along different chain\n",
    "- Custom initialization: `apply` method\n",
    "- I/O\n",
    "  - save tensor: `torch.save(x:Uinon[List[tensor], Dict], name:str)` and load\n",
    "  - save model: the same, just input dict of the net (`net.state_dict()`) then `net.load_state_dict(torch.load(name))`\n",
    "- GPU\n",
    "  - operation between tensors must in the same GPU\n",
    "  - print or transform to numpy will copy to memory, and even worse wait the python **GIL** (`Global Interpreter Lock`, make sure at the same time only one thread can execute the python bytecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72453bd2-625c-49c8-9715-37123159d39e",
   "metadata": {},
   "source": [
    "## Chapter 7 : CNN\n",
    "1. **Invariance**: translation equivariance, locality -> The earliest layers should respond similarly to the same patch and focus on local regions.\n",
    "2. **Convolution**: math is $(f * g)(i, j) = \\sum_a \\sum_b f(a, b)  g(i - a, j - b)$, remind that **cross-correlation** is $(f * g)(i, j) = \\sum_a \\sum_b f(a, b)  g(i + a, j + b)$\n",
    "   - The difference is not important as we will learn the kernel, `k_conv_learned = k_corr_learned.T`, or `conv(X, k_conv_learned) = corr(X, k_corr_learned)`\n",
    "3. **Receptive Field**ï¼š for any element (tensors on the conv layer) x, all the elements that may effect x in the previous layers in the forward population.\n",
    "4. **Padding, Stride**: $\\lfloor (n_h - k_h + p_h + s_h) / s_h \\rfloor \\times \\lfloor (n_w - k_w + p_w + s_w) / s_w \\rfloor$, often `p_h = k_h - 1`, the same for `p_w`. `p_h = p_h_upper + p_h_lower`\n",
    "5. **Channel**:\n",
    "   - multi in $c_i$ -> kernel must also have the same channels ($c_i \\times k_h \\times k_w$), then add them up.\n",
    "   - multi out $c_o$ -> kernel with $c_o \\times c_i \\times k_h \\times k_w$, get $c_o$ output channels.\n",
    "6. use `torch.stack` to stack tensors\n",
    "7. **Pooling**: mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0070a-bf43-4f5b-a1b8-d264e02296f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 8 : Modern CNN\n",
    "1. **AlexNet**: first deep conv successful, using dropout, Relu, polling\n",
    "2. **VGG**: multiple 3 * 3 conv layers (two 3 * 3 conv touch 5 * 5 input as a 5 * 5 conv, but 2 * 3 * 3  = 18 < 25 = 5 * 5)\n",
    "3. **NiN**: to handle 2 problem (1. much ram for the MLP at the end; 2. can not add MLP between the conv to increase the degree of nonlinearity as it will destroy the spatial information)\n",
    "   - use 1 * 1 conv layer to add local nonlinearities across the channel activations\n",
    "   - use global average pooling to integrate across all locations in the last representation layer. (must combine with added nonlinearities)\n",
    "4. **GoogleNet**: Inception layer, parallel conv multi scales, and then concate them\n",
    "5. **Batch Normalization**:\n",
    "   - $BN(\\mathbf x) = \\mathbf{\\gamma} \\bigodot \\frac{\\mathbf x - \\mathbf{\\mu_B}}{\\sigma^2_B} + \\mathbf \\beta$, $\\mathbf{\\mu_B} = \\frac{1}{|B|}\\sum_{x \\in B} \\mathbf x$,\n",
    "     $\\sigma^2_B = \\frac{1}{|B|} \\sum_{x \\in B} (x - \\mathbf{\\mu_B})^2 + \\epsilon$\n",
    "   - On linear layer [N, D] it will get across D (different features in D will not do calculations), on conv layer [N, C, H, W] it will across C (save the difference between channels)\n",
    "     - For example, [N, C, H, W] shape input x, for x[N, 0, H, W], get it's mean mu and std and do (x[N, 0, H, W] - mu) / std, here mu and std are scalar.\n",
    "   - At the testing stage, we will use the global (whole) data mean and varience, instead of minibatch mean and varience. Just like dropout.\n",
    "   - So BN also serves as a noise introducer! (minibatch information != true mean and var) Teye et al. (2018) and Luo et al. (2018).\n",
    "   - So it best works for batch size of 50 ~ 100, higher the noise is small, lower it is too high.\n",
    "   - Moving global mean and var: when testing, no minibatch, so we use a global one that is stored during training.\n",
    "     - It is a kind of exp weighted mean, closest batch has higer weight\n",
    "     - $\\mu_m = \\mu_m * (1 - \\tau) + \\mu * \\tau, \\Sigma_m = \\Sigma_m * (1 - \\tau) + \\Sigma * \\tau$, $\\tau$ is called momentum term.\n",
    "6. **Layer Normalization**: often used in NLP\n",
    "   - For features like [N, A, B] it will save difference between N, A and B are typically seq_len, hidden_size.\n",
    "7. **ResNet**: residual block, pass x as one of the branch before a activation function (for the original paper, and later it is changed to BN -> AC -> Conv)\n",
    "   - To get the passed x has the correct shape to add up, we can use 1 * 1 conv if it is needed\n",
    "   - **Idea**: nested-function class, shallower net (like ResNet-20) is subclass of depper net (like ResNet-50). Because in ResNet-50 if the layers after 20th layer are f(x) = x, then it is the same as RestNet-20! So we can make sure f' (the best we can get in ResNet-50 for certain data) will be better than f (ResNet-20 on the same data) or at least the same.\n",
    "   - <p align=\"center\">\n",
    "       <img alt=\"Residul Block\" src=\"https://d2l.ai/_images/resnet-block.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       Rusidul Block\n",
    "   </p>\n",
    "   - **ResNeXt**: use g groups of 3 * 3 conv layers between two 1 * 1 conv of channel $b$ and $c_o$, so $\\mathcal O(c_i c_o) \\rightarrow \\mathcal O(g ~ c_i / g ~ c_o / g) = \\mathcal O(c_ic_o/g)$\n",
    "     - This is a **Bottleneck** arch if $b < c_i$\n",
    "       </br>\n",
    "   - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/resnext-block.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       ResNeXt Block\n",
    "8. **DenseNet**: instead of plus x, we concatenate x repeatedly.\n",
    "   - For example (\\<channel\\> indicates the channel): x\\<c_1\\> -> f_1(x)\\<c_2\\> end up with [x, f_1(x)]\\<c_1 + c_2\\> -> f_2([x, f_1(x)])\\<c_3\\> end up with [x, f_1(x), f_2([x, f_1(x)])]\\<c_1 + c_2 + c_3\\>\n",
    "   - Too many of this layer will cause the dimeansion too big, so we need some layer to reduce it. **Translation** layer use 1 * 1 conv to reduce channel and avgpool to half the H and W.\n",
    "9. **RegNet**:\n",
    "   - AnyNet: network with **stem** -> **body** -> **head**.\n",
    "   - Distrubution of net: $F(e,Z)=âˆ‘_{i=1}^{n}1(e_i<e)$, use this empirical CDF to approximate $F(e, p)$, $p$ is the net arch distrubution. $Z$ is a sample of net sample from $p$, if $F(e, Z_1) < F(e, Z_2)$ then we say $Z_1$ is better, it's parameters are better.\n",
    "   - So for RegNet, they find that we should use same k (k = 1, no bottlenet, is best, says in paper) and g for the ResNeXt blocks with no harm, and increase the network depth d and weight c along the stage. And keep the c change linearly with $c_j = c_o + c_aj$ with slope $c_a$\n",
    "   - neural architecture search (NAS) : with certain search space, use RL (NASNet), evolution alg (AmoebaNet), gradient based (DARTS) or shared weight (ENAS) to get the model. But it takes to much computation resource.\n",
    "   - <img src=\"https://d2l.ai/_images/anynet.svg\" style=\"background-color: white; display: inline-block;\"> AnyNet Structure\n",
    "   </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0398cca-b9c1-4e42-a8fb-e7d90b8f5754",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 9 : RNN\n",
    "- Two form of sequence to sequence task:\n",
    "  - **aligned**: input at certain time step aligns with corrsponding output, like tagging (fight -> verb)\n",
    "  - **unaligned**: no step-to-step correspondence, like maching translation\n",
    "- **Autoregressive** model: regress value based on previous value\n",
    "  - latent autoregressive models (since $h_t$ is never observed): estimate $P(x_t | x_{t-1} \\dots x_1)$ with $\\hat x_t = P(x_t | h_t)$ and $h_t = g(h_{t-1}, x_{t-1})$\n",
    "- **Sequence Model**: to get joint probablity of a sequence $p(x_1, \\dots, x_T)$, we change it to a form like autoregressive one: $p(x_1) \\prod_{t=2}^T p(x_t|x_{t-1}, \\dots, x_1)$\n",
    "  - **Markov Condition**: if we can make the condition above into $x_{t-1}, \\dots, x_{t-\\tau}$ without any loss, aka the future is conditionally independent of the past, given the recent history, then the sequence satisfies a Markov condition. And it is $\\tau^{th}$-order Markov model.\n",
    "- Zipfâ€™s law: the frequency of words will decrease exponentially, n-grams too (with smaller slope).\n",
    "  - So use word frequency to construct the probility is not good, for example. $\\hat p(learning|deep) = n(deep, learning) / n(deep)$, $n(deep, learning)$ will be very small compared to denominator. We can use so called **Laplace Smooth** but that will not help too much.\n",
    "- **Perplexity**: (how confusion it is), given a true test data, the cross-entropy is $J = \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t | x_{t-1}, \\dots, x_1)$, and the perplexity is $\\exp(J)$.\n",
    "- Partioning the sequence: for a $T$ token indices sequence, we add some randomness, discard first $d \\in U(0, n]$ tokens and partion the rest into $m = \\lfloor (T-d) / n \\rfloor$ group. For a sequence $x_t$ the target sequence is shifted by one token $x_{t+1}$.\n",
    "---\n",
    "- **RNN**: for a vocab with size $|\\mathcal V|$, the model parameters should go up to $|\\mathcal V|^n$, $n$ is the sequence length.So we $P(x_t | x_{t-1} \\dots x_1) \\approx P(x_t | h_{t-1})$ï¼Œ$h$ is a **hidden state**, it varies at different time step and contains information of previous time steps. Hidden layer, on the other hand, is a structure, it dose not change in forward calculation.\n",
    "  - recurrent: $H_t = \\phi (X_tW_{th} + H_{t-1}W_{hh} + b_h)$, output is $O_t = H_tW_{tq} + b_q$.\n",
    "  - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       RNN Block\n",
    "  - clip the gradient: $g = \\min(1, \\frac{\\theta}{|| g ||}) g$, it is a hack but useful.\n",
    "  - **Warm-up**: When predicting, we can first feed a prefix (now called prompt I think), just iter the prefix into the network without generating output until we need to predict.\n",
    "- For RNN: the input shape is (sequence_length, batch_size, feature_size), first is time_step, third is one-hot dim or word2vec dim.\n",
    "- **Backpropagation through time**\n",
    "  - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> Computation graph of RNN\n",
    "  - How to reduce gradient explosion or vanishing: truncate the gradient propagete at certain time step.\n",
    "  - In the img above: $\\frac{\\partial L}{\\partial h_T} = W_{qh}^T \\frac{\\partial L}{\\partial o_T}$, $\\frac{\\partial L}{\\partial h_t} = \\sum_{i=t}^T (W_{hh}^T)^{T-i} W_{qh}^T \\frac{\\partial L}{\\partial o_{T+t-i}}$, $\\frac{\\partial L}{\\partial W_{hx}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} x_t^T$, $\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} h_{t-1}^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a5165-4c67-4a65-aae3-75be9291fe8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd36908-a9f8-413d-8c45-011c7e2cd889",
   "metadata": {},
   "source": [
    "åœ¨**æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å·¥ç¨‹**å’Œ**ç¡¬ä»¶ä¼˜åŒ–**æ–¹é¢ï¼Œéœ€è¦æŒæ¡ä¸€ç³»åˆ—å·¥å…·ã€æŠ€æœ¯å’Œæœ€ä½³å®è·µï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆè®­ç»ƒã€ä¼˜åŒ–å’Œéƒ¨ç½²ã€‚  \n",
    "\n",
    "---\n",
    "\n",
    "# **1. æ·±åº¦å­¦ä¹ å·¥ç¨‹**\n",
    "**ç›®æ ‡**ï¼šä¸ä»…è¦è®­ç»ƒæ¨¡å‹ï¼Œè¿˜è¦èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­é«˜æ•ˆåœ°**æ•°æ®å¤„ç†ã€è®­ç»ƒã€è°ƒä¼˜ã€éƒ¨ç½²å’Œç»´æŠ¤**ã€‚\n",
    "\n",
    "## **1.1 æ•°æ®å·¥ç¨‹**\n",
    "æ·±åº¦å­¦ä¹ çš„æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ•°æ®è´¨é‡å’Œé¢„å¤„ç†æ•ˆç‡ã€‚\n",
    "\n",
    "### **(1) æ•°æ®æ”¶é›†ä¸å­˜å‚¨**\n",
    "- **ç»“æ„åŒ–æ•°æ®**ï¼ˆSQL, Pandas, BigQueryï¼‰\n",
    "- **å›¾åƒæ•°æ®**ï¼ˆOpenCV, PIL, TensorFlow Datasetsï¼‰\n",
    "- **æ–‡æœ¬æ•°æ®**ï¼ˆNLTK, Hugging Face Datasetsï¼‰\n",
    "- **æµæ•°æ®**ï¼ˆKafka, Apache Sparkï¼‰\n",
    "\n",
    "### **(2) æ•°æ®é¢„å¤„ç†**\n",
    "- **æ ‡å‡†åŒ– / å½’ä¸€åŒ–**ï¼ˆMin-Max Scaling, Z-scoreï¼‰\n",
    "- **æ•°æ®å¢å¼º**ï¼ˆå›¾åƒï¼šæ—‹è½¬ã€è£å‰ªï¼›æ–‡æœ¬ï¼šåŒä¹‰è¯æ›¿æ¢ï¼‰\n",
    "- **é™ç»´**ï¼ˆPCA, t-SNE, UMAPï¼‰\n",
    "- **ç¼ºå¤±å€¼å¤„ç†**ï¼ˆå‡å€¼å¡«å……ã€æ’å€¼ï¼‰\n",
    "\n",
    "### **(3) æ•°æ®åŠ è½½ä¼˜åŒ–**\n",
    "- **æ‰¹é‡åŠ è½½ï¼ˆBatch Loadingï¼‰**\n",
    "- **å¤šçº¿ç¨‹ / å¤šè¿›ç¨‹æ•°æ®é¢„å¤„ç†ï¼ˆDataloader, TensorFlow tf.dataï¼‰**\n",
    "- **TFRecord / HDF5**ï¼ˆäºŒè¿›åˆ¶æ ¼å¼åŠ é€Ÿæ•°æ®è¯»å–ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## **1.2 è®­ç»ƒä¸è¶…å‚æ•°è°ƒä¼˜**\n",
    "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†å‹è¿‡ç¨‹ï¼Œéœ€è¦é«˜æ•ˆçš„**ä¼˜åŒ–ç­–ç•¥**å’Œ**è¶…å‚æ•°è°ƒæ•´**ã€‚\n",
    "\n",
    "### **(1) è®­ç»ƒä¼˜åŒ–**\n",
    "- **ä¼˜åŒ–å™¨é€‰æ‹©**\n",
    "  - SGDï¼ˆæ ‡å‡†æ¢¯åº¦ä¸‹é™ï¼‰\n",
    "  - Adam / RMSpropï¼ˆè‡ªé€‚åº”ä¼˜åŒ–ï¼‰\n",
    "  - LARS / LAMBï¼ˆç”¨äºå¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒï¼‰\n",
    "  \n",
    "- **æ­£åˆ™åŒ–**\n",
    "  - Dropoutï¼ˆéšæœºä¸¢å¼ƒç¥ç»å…ƒï¼‰\n",
    "  - Batch Normalizationï¼ˆæ‰¹é‡å½’ä¸€åŒ–ï¼‰\n",
    "  - Weight Decayï¼ˆL2 æ­£åˆ™åŒ–ï¼‰\n",
    "\n",
    "- **æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰**\n",
    "  - è§£å†³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜\n",
    "\n",
    "### **(2) è¶…å‚æ•°ä¼˜åŒ–**\n",
    "è‡ªåŠ¨æœç´¢æœ€ä¼˜è¶…å‚æ•°ï¼ˆä¾‹å¦‚å­¦ä¹ ç‡ã€batch sizeã€æƒé‡åˆå§‹åŒ–ï¼‰ã€‚\n",
    "- **Grid Searchï¼ˆç½‘æ ¼æœç´¢ï¼‰**\n",
    "- **Random Searchï¼ˆéšæœºæœç´¢ï¼‰**\n",
    "- **Bayesian Optimizationï¼ˆè´å¶æ–¯ä¼˜åŒ–ï¼‰**\n",
    "- **Hyperbandï¼ˆé«˜æ•ˆé‡‡æ ·ï¼‰**\n",
    "- **Optuna / Ray Tuneï¼ˆè‡ªåŠ¨åŒ–è¶…å‚æ•°è°ƒä¼˜å·¥å…·ï¼‰**\n",
    "\n",
    "---\n",
    "\n",
    "## **1.3 è®­ç»ƒåŠ é€Ÿ**\n",
    "å¤§è§„æ¨¡è®­ç»ƒæ—¶éœ€è¦é«˜æ•ˆçš„è®­ç»ƒåŠ é€ŸæŠ€æœ¯ï¼š\n",
    "\n",
    "### **(1) GPU åŠ é€Ÿ**\n",
    "- è®­ç»ƒæ—¶å°½å¯èƒ½åˆ©ç”¨ **CUDA** / **cuDNN**\n",
    "- **æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precisionï¼‰**ï¼šä½¿ç”¨ FP16ï¼ˆHalf Precisionï¼‰åŠ é€Ÿè®¡ç®—\n",
    "- **æ•°æ®å¹¶è¡Œï¼ˆDataParallelï¼‰** vs. **æ¨¡å‹å¹¶è¡Œï¼ˆModelParallelï¼‰**\n",
    "  \n",
    "### **(2) åˆ†å¸ƒå¼è®­ç»ƒ**\n",
    "- **å•æœºå¤šå¡ï¼ˆMulti-GPU Trainingï¼‰**\n",
    "  - PyTorch `DataParallel`\n",
    "  - PyTorch `DistributedDataParallel (DDP)`\n",
    "  \n",
    "- **å¤šæœºå¤šå¡ï¼ˆMulti-Node Trainingï¼‰**\n",
    "  - TensorFlow `MirroredStrategy`\n",
    "  - Horovodï¼ˆUber æå‡ºçš„é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## **1.4 éƒ¨ç½²ä¸æ¨ç†ä¼˜åŒ–**\n",
    "æ·±åº¦å­¦ä¹ ä¸ä»…è¦è®­ç»ƒï¼Œè¿˜è¦åœ¨**è¾¹ç¼˜è®¾å¤‡**æˆ–**æœåŠ¡å™¨ç«¯**é«˜æ•ˆæ¨ç†ã€‚\n",
    "\n",
    "### **(1) æ¨¡å‹å‹ç¼©**\n",
    "- **å‰ªæï¼ˆPruningï¼‰**ï¼šå»æ‰ä¸é‡è¦çš„æƒé‡\n",
    "- **é‡åŒ–ï¼ˆQuantizationï¼‰**ï¼š\n",
    "  - **8-bit INT é‡åŒ–**ï¼ˆTensorRT, TFLiteï¼‰\n",
    "  - **æ··åˆç²¾åº¦æ¨ç†ï¼ˆFP16, INT8ï¼‰**\n",
    "\n",
    "- **çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰**ï¼š\n",
    "  - ç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹ï¼Œæé«˜æ¨ç†æ•ˆç‡\n",
    "\n",
    "### **(2) æ¨ç†æ¡†æ¶**\n",
    "- **ONNXï¼ˆOpen Neural Network Exchangeï¼‰**ï¼šæ¨¡å‹é€šç”¨æ ¼å¼ï¼Œå¯ç”¨äº PyTorch / TensorFlow äº’è½¬\n",
    "- **TensorRTï¼ˆNVIDIAï¼‰**ï¼šé«˜æ•ˆçš„ GPU åŠ é€Ÿæ¨ç†\n",
    "- **TVMï¼ˆApacheï¼‰**ï¼šè‡ªåŠ¨ä¼˜åŒ–æ¨¡å‹æ¨ç†\n",
    "\n",
    "### **(3) éƒ¨ç½²æ–¹å¼**\n",
    "- **æœåŠ¡å™¨éƒ¨ç½²**\n",
    "  - Flask / FastAPIï¼ˆREST API éƒ¨ç½²ï¼‰\n",
    "  - TensorFlow Serving / TorchServeï¼ˆé«˜æ•ˆæ¨¡å‹æœåŠ¡ï¼‰\n",
    "\n",
    "- **ç§»åŠ¨ç«¯ / è¾¹ç¼˜éƒ¨ç½²**\n",
    "  - TensorFlow Liteï¼ˆTFLiteï¼‰\n",
    "  - CoreMLï¼ˆiOS è®¾å¤‡ï¼‰\n",
    "  - NVIDIA Jetsonï¼ˆåµŒå…¥å¼ AIï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "# **2. ç¡¬ä»¶ä¼˜åŒ–**\n",
    "æ·±åº¦å­¦ä¹ çš„è®¡ç®—é‡æå¤§ï¼Œç¡¬ä»¶çš„ä¼˜åŒ–èƒ½**æ˜¾è‘—æé«˜è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦**ã€‚\n",
    "\n",
    "## **2.1 GPU è®¡ç®—**\n",
    "GPU æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒè®¡ç®—è®¾å¤‡ï¼ŒNVIDIA CUDA ç”Ÿæ€è‡³å…³é‡è¦ã€‚\n",
    "\n",
    "### **(1) GPU ç¼–ç¨‹åŸºç¡€**\n",
    "- CUDA ç¼–ç¨‹ï¼ˆæŒæ¡ Kernel ç¼–å†™ï¼‰\n",
    "- cuDNNï¼ˆæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼‰\n",
    "- Tensor Coreï¼ˆç”¨äºæ··åˆç²¾åº¦è®¡ç®—ï¼‰\n",
    "  \n",
    "### **(2) GPU è®­ç»ƒä¼˜åŒ–**\n",
    "- **å‡å°‘ CPU-GPU ä¼ è¾“**ï¼ˆä¼˜åŒ– `pin_memory=True`ï¼‰\n",
    "- **æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰**ï¼Œå‡å°‘æ˜¾å­˜å ç”¨\n",
    "- **ä½¿ç”¨ FP16 è®­ç»ƒ**ï¼ˆæé«˜ååé‡ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2 åˆ†å¸ƒå¼è®¡ç®—**\n",
    "é€‚ç”¨äº**è¶…å¤§è§„æ¨¡æ•°æ®è®­ç»ƒ**ï¼ˆå¦‚ GPTã€Llama ç­‰æ¨¡å‹ï¼‰ã€‚\n",
    "\n",
    "### **(1) å¹¶è¡Œç­–ç•¥**\n",
    "- **æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰**\n",
    "  - å¤åˆ¶æ¨¡å‹åˆ°å¤šä¸ª GPUï¼Œæ¯ä¸ª GPU è®­ç»ƒä¸åŒæ•°æ®\n",
    "  - PyTorch `DistributedDataParallel (DDP)`\n",
    "\n",
    "- **æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰**\n",
    "  - é€‚ç”¨äºè¶…å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰\n",
    "  - DeepSpeed / Megatron-LM ä¼˜åŒ–\n",
    "\n",
    "- **æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰**\n",
    "  - å°†ä¸åŒå±‚åˆ†é…åˆ°ä¸åŒ GPUï¼Œæé«˜è®¡ç®—æ•ˆç‡\n",
    "  - **é€‚ç”¨äº Transformer è®­ç»ƒ**\n",
    "\n",
    "### **(2) é«˜æ•ˆé€šä¿¡**\n",
    "- **NCCLï¼ˆNVIDIA Collective Communication Libraryï¼‰**ï¼šä¼˜åŒ– GPU ä¹‹é—´çš„é€šä¿¡\n",
    "- **RDMAï¼ˆè¿œç¨‹ç›´æ¥å†…å­˜è®¿é—®ï¼‰**ï¼šç”¨äº GPU æœåŠ¡å™¨é—´é«˜é€Ÿé€šä¿¡\n",
    "\n",
    "---\n",
    "\n",
    "## **2.3 ä¸“ç”¨ AI ç¡¬ä»¶**\n",
    "é™¤äº† GPUï¼ŒAI è®­ç»ƒè¿˜å¯ä»¥ç”¨ä¸“ç”¨èŠ¯ç‰‡åŠ é€Ÿï¼š\n",
    "- **TPUï¼ˆGoogleï¼‰**ï¼šä¸“é—¨ä¼˜åŒ–æ·±åº¦å­¦ä¹ è®¡ç®—\n",
    "- **Graphcore IPU**ï¼ˆç¨€ç–è®¡ç®—ä¼˜åŒ–ï¼‰\n",
    "- **Cerebras Wafer-Scale Engine**ï¼ˆè¶…å¤§è§„æ¨¡ AI è®¡ç®—ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "# **3. æ€»ç»“**\n",
    "| **ç±»åˆ«** | **å…³é”®å†…å®¹** |\n",
    "|----------|--------------|\n",
    "| **æ•°æ®å·¥ç¨‹** | æ•°æ®æ¸…æ´—ã€æ•°æ®å¢å¼ºã€æ•°æ®åŠ è½½ä¼˜åŒ– |\n",
    "| **è®­ç»ƒä¼˜åŒ–** | è¶…å‚æ•°è°ƒä¼˜ã€æ­£åˆ™åŒ–ã€ä¼˜åŒ–å™¨é€‰æ‹© |\n",
    "| **è®­ç»ƒåŠ é€Ÿ** | GPU åŠ é€Ÿã€æ··åˆç²¾åº¦ã€åˆ†å¸ƒå¼è®­ç»ƒ |\n",
    "| **éƒ¨ç½²ä¼˜åŒ–** | æ¨¡å‹é‡åŒ–ã€å‰ªæã€TensorRT åŠ é€Ÿ |\n",
    "| **ç¡¬ä»¶ä¼˜åŒ–** | CUDAã€NCCLã€TPU/FPGA |\n",
    "\n",
    "ä½ å·²ç»æœ‰**çŸ©é˜µåˆ†è§£å’Œ Rust ç»éªŒ**ï¼Œå¦‚æœæƒ³æ·±å…¥å·¥ç¨‹ä¼˜åŒ–ï¼Œå¯ä»¥ï¼š\n",
    "1. **ç ”ç©¶ PyTorch DDP / DeepSpeed**ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–ï¼‰\n",
    "2. **å­¦ä¹  CUDA / cuDNN ç¼–ç¨‹**ï¼ˆä½çº§ GPU åŠ é€Ÿï¼‰\n",
    "3. **å°è¯• TensorRT / ONNX**ï¼ˆæ¨ç†åŠ é€Ÿï¼‰\n",
    "\n",
    "è¿™å°†è®©ä½ åœ¨ **æ·±åº¦å­¦ä¹ å·¥ç¨‹ & ç¡¬ä»¶ä¼˜åŒ–** æ–¹é¢å…·å¤‡æ›´å¼ºçš„ç«äº‰åŠ› ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc4c3e-6b4c-42c0-acfa-aa8b86612137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
