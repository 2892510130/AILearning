{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4c420e-be32-4965-a531-ff998f6cc05b",
   "metadata": {},
   "source": [
    "### Chapter 2 : Preliminary Knowledge\n",
    "- 数据操作\n",
    "  - 广播机制（两个数据分别复制扩充到同样的尺寸）\n",
    "  - 节省内存（使用X[:] = \\<expression\\>或X+=\\<expression\\>来避免重新分配）\n",
    "- 数据预处理\n",
    "- 线性代数 \n",
    "  - 转置.T 范数norm\n",
    "  - 非降维求和 (keepdims=True)，累积和cumsum\n",
    "  - torch.dot只支持向量，矩阵和向量间用mv，矩阵之间用mm\n",
    "- 微积分\n",
    "  - 设T是梯度算符，T(Ax) = A.T, T(x.T·A) = A, T(x.T A x) = (A + A.T)x\n",
    "- 自动微分\n",
    "  - 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "  - 自动微分必须是标量，非标量的话要么转成标量，要么指定输出形状\n",
    "  - 分离操作\n",
    "- 概率论\n",
    "- 查阅文档、API的指导\n",
    "  - dir查看可以调用的函数和类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a80380-84c4-4ad0-84e1-4a76d4de7076",
   "metadata": {},
   "source": [
    "### Chapter 3 : Linear Neural Network\n",
    "- Minibatch stochastic gradient descent (小批量随机梯度下降)\n",
    "- 一般的训练过程\n",
    "  - model.forward() 与 y_hat 做差，然后反向传播，优化器根据导数去更新参数\n",
    "- Machine Learning Concept\n",
    "  - lasso regression: l1 norm; ridge regression: l2 norm;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff340e0-b5fd-437e-8f8b-0698e9faf41f",
   "metadata": {},
   "source": [
    "## Chapter 4 : Classification\n",
    "- softmax:\n",
    "  $y_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}$, often minus max(oj) to get numerical stable\n",
    "- Information theory\n",
    "  - cross-entropy loss：$l(y, \\hat y) = - \\sum y_i * \\log(\\hat y_i)$\n",
    "  - amount of information $\\log{\\frac{1}{P(j)}} = - \\log{P(j)}$ \n",
    "  - entorpy $H[P] = \\sum -P(j) \\log{P(j)}$\n",
    "  - cross-entorpy $H(P, Q) = \\sum -P(j) \\log{Q(j)}, ~ P=Q \\rightarrow H(P, Q) = H(P, P) = H(P)$\n",
    "- Image Classification Rules:\n",
    "  - image stored in (channel, height, weight) manner.\n",
    "- Distrubution shift:\n",
    "  - Covariate Shift (feature shift): $p(x) \\neq q(x), p(y|x) = q(y|x)$\n",
    "    - For example: p(x) and q(x) are features of oral and urban house, y is the price, we assume the feature and label relation is the same\n",
    "    - Method: weighted by $\\beta(x) = p(x) / q(x) \\rightarrow \\int\\int l(f(x), y)p(y|x)p(x)dxdy = \\int\\int l(f(x), y)q(y|x)q(x) \\frac{p(x)}{q(x)}dxdy \\rightarrow \\sum_i \\beta_i l(f(x_i), y_i)$, $\\beta$ can be obtained with logistic regression.\n",
    "  - Label Shift, $p(y) \\neq q(y), p(x|y) = q(x|y)$, the same method $\\beta(y) = p(y) / q(y)$, but now $q(y)$ is hard to get, we need compute a confusion matrix on the val data then use the model to pridcit the distrubution of the $q(y)$\n",
    "  - Concept Shift (the concept of the label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab587f-119f-4a04-bbde-9e7d49953d9d",
   "metadata": {},
   "source": [
    "## Chapter 5 : Multilayer Perceptrons\n",
    "- Activation Function: relu, sigmoid, tanh ($\\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}$)\n",
    "- Numerical stability: vanish and explode are common\n",
    "  - Symmetry: linear layer and conv (with no share weight) layer are symmetric so we can tell apart from different weight and try to explain it, so we need to **Bread the Symmetry** (like using a dropout)\n",
    "  - Xavier initilization: get from distrubution of zero mean and variance $\\sigma = \\sqrt{2 / (n_{in} + n_{out})}$\n",
    "  - Dropout, shared param...\n",
    "- (Rolnick et al., 2017) has revealed that in the setting of label noise, neural networks tend to fit cleanly labeled data **first** and only subsequently to interpolate the mislabeled data.\n",
    "  - so we can use early stop once error on val is minimal or the patience hit. usually combined with regularization.\n",
    "- Dropout:\n",
    "  - $h^{'} = \\left \\{ \n",
    "  \\begin{array}{lll}\n",
    "  & 0, p \\\\\n",
    "  & \\frac{h}{1-p}, 1-p\n",
    "  \\end{array} \n",
    "  \\right .$, now $E[h^{'}] = E[h]$\n",
    "  - We do not use dropout in test, except we want to know the uncertainty of the model output (by comparing different dropout)\n",
    "  - Use lower p in lower layer (to get lower feature), higher p in higher layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8510492-eac8-4c69-bc9f-49cbe5acb237",
   "metadata": {},
   "source": [
    "## Chapter 6: Beginner Guide\n",
    "- Tied layer: gradient will add up along different chain\n",
    "- Custom initialization: `apply` method\n",
    "- I/O\n",
    "  - save tensor: `torch.save(x:Uinon[List[tensor], Dict], name:str)` and load\n",
    "  - save model: the same, just input dict of the net (`net.state_dict()`) then `net.load_state_dict(torch.load(name))`\n",
    "- GPU\n",
    "  - operation between tensors must in the same GPU\n",
    "  - print or transform to numpy will copy to memory, and even worse wait the python **GIL** (`Global Interpreter Lock`, make sure at the same time only one thread can execute the python bytecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72453bd2-625c-49c8-9715-37123159d39e",
   "metadata": {},
   "source": [
    "## Chapter 7 : CNN\n",
    "1. **Invariance**: translation equivariance, locality -> The earliest layers should respond similarly to the same patch and focus on local regions.\n",
    "2. **Convolution**: math is $(f * g)(i, j) = \\sum_a \\sum_b f(a, b)  g(i - a, j - b)$, remind that **cross-correlation** is $(f * g)(i, j) = \\sum_a \\sum_b f(a, b)  g(i + a, j + b)$\n",
    "   - The difference is not important as we will learn the kernel, `k_conv_learned = k_corr_learned.T`, or `conv(X, k_conv_learned) = corr(X, k_corr_learned)`\n",
    "3. **Receptive Field**： for any element (tensors on the conv layer) x, all the elements that may effect x in the previous layers in the forward population.\n",
    "4. **Padding, Stride**: $\\lfloor (n_h - k_h + p_h + s_h) / s_h \\rfloor \\times \\lfloor (n_w - k_w + p_w + s_w) / s_w \\rfloor$, often `p_h = k_h - 1`, the same for `p_w`. `p_h = p_h_upper + p_h_lower`\n",
    "5. **Channel**:\n",
    "   - multi in $c_i$ -> kernel must also have the same channels ($c_i \\times k_h \\times k_w$), then add them up.\n",
    "   - multi out $c_o$ -> kernel with $c_o \\times c_i \\times k_h \\times k_w$, get $c_o$ output channels.\n",
    "6. use `torch.stack` to stack tensors\n",
    "7. **Pooling**: mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0070a-bf43-4f5b-a1b8-d264e02296f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 8 : Modern CNN\n",
    "1. **AlexNet**: first deep conv successful, using dropout, Relu, polling\n",
    "2. **VGG**: multiple 3 * 3 conv layers (two 3 * 3 conv touch 5 * 5 input as a 5 * 5 conv, but 2 * 3 * 3  = 18 < 25 = 5 * 5)\n",
    "3. **NiN**: to handle 2 problem (1. much ram for the MLP at the end; 2. can not add MLP between the conv to increase the degree of nonlinearity as it will destroy the spatial information)\n",
    "   - use 1 * 1 conv layer to add local nonlinearities across the channel activations\n",
    "   - use global average pooling to integrate across all locations in the last representation layer. (must combine with added nonlinearities)\n",
    "4. **GoogleNet**: Inception layer, parallel conv multi scales, and then concate them\n",
    "5. **Batch Normalization**:\n",
    "   - $BN(\\mathbf x) = \\mathbf{\\gamma} \\bigodot \\frac{\\mathbf x - \\mathbf{\\mu_B}}{\\sigma^2_B} + \\mathbf \\beta$, $\\mathbf{\\mu_B} = \\frac{1}{|B|}\\sum_{x \\in B} \\mathbf x$,\n",
    "     $\\sigma^2_B = \\frac{1}{|B|} \\sum_{x \\in B} (x - \\mathbf{\\mu_B})^2 + \\epsilon$\n",
    "   - On linear layer [N, D] it will get across D (different features in D will not do calculations), on conv layer [N, C, H, W] it will across C (save the difference between channels)\n",
    "     - For example, [N, C, H, W] shape input x, for x[N, 0, H, W], get it's mean mu and std and do (x[N, 0, H, W] - mu) / std, here mu and std are scalar.\n",
    "   - At the testing stage, we will use the global (whole) data mean and varience, instead of minibatch mean and varience. Just like dropout.\n",
    "   - So BN also serves as a noise introducer! (minibatch information != true mean and var) Teye et al. (2018) and Luo et al. (2018).\n",
    "   - So it best works for batch size of 50 ~ 100, higher the noise is small, lower it is too high.\n",
    "   - Moving global mean and var: when testing, no minibatch, so we use a global one that is stored during training.\n",
    "     - It is a kind of exp weighted mean, closest batch has higer weight\n",
    "     - $\\mu_m = \\mu_m * (1 - \\tau) + \\mu * \\tau, \\Sigma_m = \\Sigma_m * (1 - \\tau) + \\Sigma * \\tau$, $\\tau$ is called momentum term.\n",
    "6. **Layer Normalization**: often used in NLP\n",
    "   - For features like [N, A, B] it will save difference between N, A and B are typically seq_len, hidden_size.\n",
    "7. **ResNet**: residual block, pass x as one of the branch before a activation function (for the original paper, and later it is changed to BN -> AC -> Conv)\n",
    "   - To get the passed x has the correct shape to add up, we can use 1 * 1 conv if it is needed\n",
    "   - **Idea**: nested-function class, shallower net (like ResNet-20) is subclass of depper net (like ResNet-50). Because in ResNet-50 if the layers after 20th layer are f(x) = x, then it is the same as RestNet-20! So we can make sure f' (the best we can get in ResNet-50 for certain data) will be better than f (ResNet-20 on the same data) or at least the same.\n",
    "   - <p align=\"center\">\n",
    "       <img alt=\"Residul Block\" src=\"https://d2l.ai/_images/resnet-block.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       Rusidul Block\n",
    "   </p>\n",
    "   - **ResNeXt**: use g groups of 3 * 3 conv layers between two 1 * 1 conv of channel $b$ and $c_o$, so $\\mathcal O(c_i c_o) \\rightarrow \\mathcal O(g ~ c_i / g ~ c_o / g) = \\mathcal O(c_ic_o/g)$\n",
    "     - This is a **Bottleneck** arch if $b < c_i$\n",
    "       </br>\n",
    "   - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/resnext-block.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       ResNeXt Block\n",
    "8. **DenseNet**: instead of plus x, we concatenate x repeatedly.\n",
    "   - For example (\\<channel\\> indicates the channel): x\\<c_1\\> -> f_1(x)\\<c_2\\> end up with [x, f_1(x)]\\<c_1 + c_2\\> -> f_2([x, f_1(x)])\\<c_3\\> end up with [x, f_1(x), f_2([x, f_1(x)])]\\<c_1 + c_2 + c_3\\>\n",
    "   - Too many of this layer will cause the dimeansion too big, so we need some layer to reduce it. **Translation** layer use 1 * 1 conv to reduce channel and avgpool to half the H and W.\n",
    "9. **RegNet**:\n",
    "   - AnyNet: network with **stem** -> **body** -> **head**.\n",
    "   - Distrubution of net: $F(e,Z)=∑_{i=1}^{n}1(e_i<e)$, use this empirical CDF to approximate $F(e, p)$, $p$ is the net arch distrubution. $Z$ is a sample of net sample from $p$, if $F(e, Z_1) < F(e, Z_2)$ then we say $Z_1$ is better, it's parameters are better.\n",
    "   - So for RegNet, they find that we should use same k (k = 1, no bottlenet, is best, says in paper) and g for the ResNeXt blocks with no harm, and increase the network depth d and weight c along the stage. And keep the c change linearly with $c_j = c_o + c_aj$ with slope $c_a$\n",
    "   - neural architecture search (NAS) : with certain search space, use RL (NASNet), evolution alg (AmoebaNet), gradient based (DARTS) or shared weight (ENAS) to get the model. But it takes to much computation resource.\n",
    "   - <img src=\"https://d2l.ai/_images/anynet.svg\" style=\"background-color: white; display: inline-block;\"> AnyNet Structure\n",
    "   </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0398cca-b9c1-4e42-a8fb-e7d90b8f5754",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 9 : RNN\n",
    "- Two form of sequence to sequence task:\n",
    "  - **aligned**: input at certain time step aligns with corrsponding output, like tagging (fight -> verb)\n",
    "  - **unaligned**: no step-to-step correspondence, like maching translation\n",
    "- **Autoregressive** model: regress value based on previous value\n",
    "  - latent autoregressive models (since $h_t$ is never observed): estimate $P(x_t | x_{t-1} \\dots x_1)$ with $\\hat x_t = P(x_t | h_t)$ and $h_t = g(h_{t-1}, x_{t-1})$\n",
    "- **Sequence Model**: to get joint probablity of a sequence $p(x_1, \\dots, x_T)$, we change it to a form like autoregressive one: $p(x_1) \\prod_{t=2}^T p(x_t|x_{t-1}, \\dots, x_1)$\n",
    "  - **Markov Condition**: if we can make the condition above into $x_{t-1}, \\dots, x_{t-\\tau}$ without any loss, aka the future is conditionally independent of the past, given the recent history, then the sequence satisfies a Markov condition. And it is $\\tau^{th}$-order Markov model.\n",
    "- Zipf’s law: the frequency of words will decrease exponentially, n-grams too (with smaller slope).\n",
    "  - So use word frequency to construct the probility is not good, for example. $\\hat p(learning|deep) = n(deep, learning) / n(deep)$, $n(deep, learning)$ will be very small compared to denominator. We can use so called **Laplace Smooth** but that will not help too much.\n",
    "- **Perplexity**: (how confusion it is), given a true test data, the cross-entropy is $J = \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t | x_{t-1}, \\dots, x_1)$, and the perplexity is $\\exp(J)$.\n",
    "- Partioning the sequence: for a $T$ token indices sequence, we add some randomness, discard first $d \\in U(0, n]$ tokens and partion the rest into $m = \\lfloor (T-d) / n \\rfloor$ group. For a sequence $x_t$ the target sequence is shifted by one token $x_{t+1}$.\n",
    "---\n",
    "- **RNN**: for a vocab with size $|\\mathcal V|$, the model parameters should go up to $|\\mathcal V|^n$, $n$ is the sequence length.So we $P(x_t | x_{t-1} \\dots x_1) \\approx P(x_t | h_{t-1})$，$h$ is a **hidden state**, it varies at different time step and contains information of previous time steps. Hidden layer, on the other hand, is a structure, it dose not change in forward calculation.\n",
    "  - recurrent: $H_t = \\phi (X_tW_{th} + H_{t-1}W_{hh} + b_h)$, output is $O_t = H_tW_{tq} + b_q$.\n",
    "  - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       RNN Block\n",
    "  - clip the gradient: $g = \\min(1, \\frac{\\theta}{|| g ||}) g$, it is a hack but useful.\n",
    "  - **Warm-up**: When predicting, we can first feed a prefix (now called prompt I think), just iter the prefix into the network without generating output until we need to predict.\n",
    "- For RNN: the input shape is (sequence_length, batch_size, feature_size), first is time_step, third is one-hot dim or word2vec dim.\n",
    "- **Backpropagation through time**\n",
    "  - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> Computation graph of RNN\n",
    "  - How to reduce gradient explosion or vanishing: truncate the gradient propagete at certain time step.\n",
    "  - In the img above: $\\frac{\\partial L}{\\partial h_T} = W_{qh}^T \\frac{\\partial L}{\\partial o_T}$, $\\frac{\\partial L}{\\partial h_t} = \\sum_{i=t}^T (W_{hh}^T)^{T-i} W_{qh}^T \\frac{\\partial L}{\\partial o_{T+t-i}}$, $\\frac{\\partial L}{\\partial W_{hx}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} x_t^T$, $\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} h_{t-1}^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a5165-4c67-4a65-aae3-75be9291fe8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd36908-a9f8-413d-8c45-011c7e2cd889",
   "metadata": {},
   "source": [
    "在**深度学习（DL）工程**和**硬件优化**方面，需要掌握一系列工具、技术和最佳实践，以确保模型能够高效训练、优化和部署。  \n",
    "\n",
    "---\n",
    "\n",
    "# **1. 深度学习工程**\n",
    "**目标**：不仅要训练模型，还要能够在实际应用中高效地**数据处理、训练、调优、部署和维护**。\n",
    "\n",
    "## **1.1 数据工程**\n",
    "深度学习的性能很大程度上依赖于数据质量和预处理效率。\n",
    "\n",
    "### **(1) 数据收集与存储**\n",
    "- **结构化数据**（SQL, Pandas, BigQuery）\n",
    "- **图像数据**（OpenCV, PIL, TensorFlow Datasets）\n",
    "- **文本数据**（NLTK, Hugging Face Datasets）\n",
    "- **流数据**（Kafka, Apache Spark）\n",
    "\n",
    "### **(2) 数据预处理**\n",
    "- **标准化 / 归一化**（Min-Max Scaling, Z-score）\n",
    "- **数据增强**（图像：旋转、裁剪；文本：同义词替换）\n",
    "- **降维**（PCA, t-SNE, UMAP）\n",
    "- **缺失值处理**（均值填充、插值）\n",
    "\n",
    "### **(3) 数据加载优化**\n",
    "- **批量加载（Batch Loading）**\n",
    "- **多线程 / 多进程数据预处理（Dataloader, TensorFlow tf.data）**\n",
    "- **TFRecord / HDF5**（二进制格式加速数据读取）\n",
    "\n",
    "---\n",
    "\n",
    "## **1.2 训练与超参数调优**\n",
    "深度学习模型训练是一个计算密集型过程，需要高效的**优化策略**和**超参数调整**。\n",
    "\n",
    "### **(1) 训练优化**\n",
    "- **优化器选择**\n",
    "  - SGD（标准梯度下降）\n",
    "  - Adam / RMSprop（自适应优化）\n",
    "  - LARS / LAMB（用于大规模分布式训练）\n",
    "  \n",
    "- **正则化**\n",
    "  - Dropout（随机丢弃神经元）\n",
    "  - Batch Normalization（批量归一化）\n",
    "  - Weight Decay（L2 正则化）\n",
    "\n",
    "- **梯度裁剪（Gradient Clipping）**\n",
    "  - 解决梯度爆炸问题\n",
    "\n",
    "### **(2) 超参数优化**\n",
    "自动搜索最优超参数（例如学习率、batch size、权重初始化）。\n",
    "- **Grid Search（网格搜索）**\n",
    "- **Random Search（随机搜索）**\n",
    "- **Bayesian Optimization（贝叶斯优化）**\n",
    "- **Hyperband（高效采样）**\n",
    "- **Optuna / Ray Tune（自动化超参数调优工具）**\n",
    "\n",
    "---\n",
    "\n",
    "## **1.3 训练加速**\n",
    "大规模训练时需要高效的训练加速技术：\n",
    "\n",
    "### **(1) GPU 加速**\n",
    "- 训练时尽可能利用 **CUDA** / **cuDNN**\n",
    "- **混合精度训练（Mixed Precision）**：使用 FP16（Half Precision）加速计算\n",
    "- **数据并行（DataParallel）** vs. **模型并行（ModelParallel）**\n",
    "  \n",
    "### **(2) 分布式训练**\n",
    "- **单机多卡（Multi-GPU Training）**\n",
    "  - PyTorch `DataParallel`\n",
    "  - PyTorch `DistributedDataParallel (DDP)`\n",
    "  \n",
    "- **多机多卡（Multi-Node Training）**\n",
    "  - TensorFlow `MirroredStrategy`\n",
    "  - Horovod（Uber 提出的高效分布式训练框架）\n",
    "\n",
    "---\n",
    "\n",
    "## **1.4 部署与推理优化**\n",
    "深度学习不仅要训练，还要在**边缘设备**或**服务器端**高效推理。\n",
    "\n",
    "### **(1) 模型压缩**\n",
    "- **剪枝（Pruning）**：去掉不重要的权重\n",
    "- **量化（Quantization）**：\n",
    "  - **8-bit INT 量化**（TensorRT, TFLite）\n",
    "  - **混合精度推理（FP16, INT8）**\n",
    "\n",
    "- **知识蒸馏（Knowledge Distillation）**：\n",
    "  - 用大模型训练小模型，提高推理效率\n",
    "\n",
    "### **(2) 推理框架**\n",
    "- **ONNX（Open Neural Network Exchange）**：模型通用格式，可用于 PyTorch / TensorFlow 互转\n",
    "- **TensorRT（NVIDIA）**：高效的 GPU 加速推理\n",
    "- **TVM（Apache）**：自动优化模型推理\n",
    "\n",
    "### **(3) 部署方式**\n",
    "- **服务器部署**\n",
    "  - Flask / FastAPI（REST API 部署）\n",
    "  - TensorFlow Serving / TorchServe（高效模型服务）\n",
    "\n",
    "- **移动端 / 边缘部署**\n",
    "  - TensorFlow Lite（TFLite）\n",
    "  - CoreML（iOS 设备）\n",
    "  - NVIDIA Jetson（嵌入式 AI）\n",
    "\n",
    "---\n",
    "\n",
    "# **2. 硬件优化**\n",
    "深度学习的计算量极大，硬件的优化能**显著提高训练和推理速度**。\n",
    "\n",
    "## **2.1 GPU 计算**\n",
    "GPU 是深度学习的核心计算设备，NVIDIA CUDA 生态至关重要。\n",
    "\n",
    "### **(1) GPU 编程基础**\n",
    "- CUDA 编程（掌握 Kernel 编写）\n",
    "- cuDNN（深度学习优化库）\n",
    "- Tensor Core（用于混合精度计算）\n",
    "  \n",
    "### **(2) GPU 训练优化**\n",
    "- **减少 CPU-GPU 传输**（优化 `pin_memory=True`）\n",
    "- **梯度累积（Gradient Accumulation）**，减少显存占用\n",
    "- **使用 FP16 训练**（提高吞吐量）\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2 分布式计算**\n",
    "适用于**超大规模数据训练**（如 GPT、Llama 等模型）。\n",
    "\n",
    "### **(1) 并行策略**\n",
    "- **数据并行（Data Parallelism）**\n",
    "  - 复制模型到多个 GPU，每个 GPU 训练不同数据\n",
    "  - PyTorch `DistributedDataParallel (DDP)`\n",
    "\n",
    "- **模型并行（Model Parallelism）**\n",
    "  - 适用于超大模型（如 GPT-4）\n",
    "  - DeepSpeed / Megatron-LM 优化\n",
    "\n",
    "- **流水线并行（Pipeline Parallelism）**\n",
    "  - 将不同层分配到不同 GPU，提高计算效率\n",
    "  - **适用于 Transformer 训练**\n",
    "\n",
    "### **(2) 高效通信**\n",
    "- **NCCL（NVIDIA Collective Communication Library）**：优化 GPU 之间的通信\n",
    "- **RDMA（远程直接内存访问）**：用于 GPU 服务器间高速通信\n",
    "\n",
    "---\n",
    "\n",
    "## **2.3 专用 AI 硬件**\n",
    "除了 GPU，AI 训练还可以用专用芯片加速：\n",
    "- **TPU（Google）**：专门优化深度学习计算\n",
    "- **Graphcore IPU**（稀疏计算优化）\n",
    "- **Cerebras Wafer-Scale Engine**（超大规模 AI 计算）\n",
    "\n",
    "---\n",
    "\n",
    "# **3. 总结**\n",
    "| **类别** | **关键内容** |\n",
    "|----------|--------------|\n",
    "| **数据工程** | 数据清洗、数据增强、数据加载优化 |\n",
    "| **训练优化** | 超参数调优、正则化、优化器选择 |\n",
    "| **训练加速** | GPU 加速、混合精度、分布式训练 |\n",
    "| **部署优化** | 模型量化、剪枝、TensorRT 加速 |\n",
    "| **硬件优化** | CUDA、NCCL、TPU/FPGA |\n",
    "\n",
    "你已经有**矩阵分解和 Rust 经验**，如果想深入工程优化，可以：\n",
    "1. **研究 PyTorch DDP / DeepSpeed**（分布式训练优化）\n",
    "2. **学习 CUDA / cuDNN 编程**（低级 GPU 加速）\n",
    "3. **尝试 TensorRT / ONNX**（推理加速）\n",
    "\n",
    "这将让你在 **深度学习工程 & 硬件优化** 方面具备更强的竞争力 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc4c3e-6b4c-42c0-acfa-aa8b86612137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
