{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851d558d-4c99-417f-9017-d482eb233a28",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "- Chapter 2 : Preliminary Knowledge\n",
    "- Chapter 3 : Linear Neural Network\n",
    "- Chapter 4 : Classification\n",
    "- Chapter 5 : Multilayer Perceptrons\n",
    "- Chapter 6 : Beginner Guide\n",
    "- Chapter 7 : CNN\n",
    "- Chapter 8 : Modern CNN\n",
    "- Chapter 9 : RNN\n",
    "\n",
    "  \n",
    "- å·¥ç¨‹å’Œç¡¬ä»¶ä¼˜åŒ– RoadMap\n",
    "- æ¢¯åº¦æ£€æŸ¥æŠ€æœ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c420e-be32-4965-a531-ff998f6cc05b",
   "metadata": {},
   "source": [
    "### Chapter 2 : Preliminary Knowledge\n",
    "- æ•°æ®æ“ä½œ\n",
    "  - å¹¿æ’­æœºåˆ¶ï¼ˆä¸¤ä¸ªæ•°æ®åˆ†åˆ«å¤åˆ¶æ‰©å……åˆ°åŒæ ·çš„å°ºå¯¸ï¼‰\n",
    "  - èŠ‚çœå†…å­˜ï¼ˆä½¿ç”¨X[:] = \\<expression\\>æˆ–X+=\\<expression\\>æ¥é¿å…é‡æ–°åˆ†é…ï¼‰\n",
    "- æ•°æ®é¢„å¤„ç†\n",
    "- çº¿æ€§ä»£æ•° \n",
    "  - è½¬ç½®.T èŒƒæ•°norm\n",
    "  - éé™ç»´æ±‚å’Œ (keepdims=True)ï¼Œç´¯ç§¯å’Œcumsum\n",
    "  - torch.dotåªæ”¯æŒå‘é‡ï¼ŒçŸ©é˜µå’Œå‘é‡é—´ç”¨mvï¼ŒçŸ©é˜µä¹‹é—´ç”¨mm\n",
    "- å¾®ç§¯åˆ†\n",
    "  - è®¾Tæ˜¯æ¢¯åº¦ç®—ç¬¦ï¼ŒT(Ax) = A.T, T(x.TÂ·A) = A, T(x.T A x) = (A + A.T)x\n",
    "- è‡ªåŠ¨å¾®åˆ†\n",
    "  - åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¼šç´¯ç§¯æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦æ¸…é™¤ä¹‹å‰çš„å€¼\n",
    "  - è‡ªåŠ¨å¾®åˆ†å¿…é¡»æ˜¯æ ‡é‡ï¼Œéæ ‡é‡çš„è¯è¦ä¹ˆè½¬æˆæ ‡é‡ï¼Œè¦ä¹ˆæŒ‡å®šè¾“å‡ºå½¢çŠ¶\n",
    "  - åˆ†ç¦»æ“ä½œ\n",
    "- æ¦‚ç‡è®º\n",
    "- æŸ¥é˜…æ–‡æ¡£ã€APIçš„æŒ‡å¯¼\n",
    "  - diræŸ¥çœ‹å¯ä»¥è°ƒç”¨çš„å‡½æ•°å’Œç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a80380-84c4-4ad0-84e1-4a76d4de7076",
   "metadata": {},
   "source": [
    "### Chapter 3 : Linear Neural Network\n",
    "- Minibatch stochastic gradient descent (å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™)\n",
    "- ä¸€èˆ¬çš„è®­ç»ƒè¿‡ç¨‹\n",
    "  - model.forward() ä¸ y_hat åšå·®ï¼Œç„¶ååå‘ä¼ æ’­ï¼Œä¼˜åŒ–å™¨æ ¹æ®å¯¼æ•°å»æ›´æ–°å‚æ•°\n",
    "- Machine Learning Concept\n",
    "  - lasso regression: l1 norm; ridge regression: l2 norm;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff340e0-b5fd-437e-8f8b-0698e9faf41f",
   "metadata": {},
   "source": [
    "## Chapter 4 : Classification\n",
    "- softmax:\n",
    "  $y_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}$, often minus max(oj) to get numerical stable\n",
    "- Information theory\n",
    "  - cross-entropy lossï¼š$l(y, \\hat y) = - \\sum y_i * \\log(\\hat y_i)$\n",
    "  - amount of information $\\log{\\frac{1}{P(j)}} = - \\log{P(j)}$ \n",
    "  - entorpy $H[P] = \\sum -P(j) \\log{P(j)}$\n",
    "  - cross-entorpy $H(P, Q) = \\sum -P(j) \\log{Q(j)}, ~ P=Q \\rightarrow H(P, Q) = H(P, P) = H(P)$. In pytorch, F.cross_entropy will do the softmax for you.\n",
    "- Image Classification Rules:\n",
    "  - image stored in (channel, height, weight) manner.\n",
    "- Distrubution shift:\n",
    "  - Covariate Shift (feature shift): $p(x) \\neq q(x), p(y|x) = q(y|x)$\n",
    "    - For example: p(x) and q(x) are features of oral and urban house, y is the price, we assume the feature and label relation is the same\n",
    "    - Method: weighted by $\\beta(x) = p(x) / q(x) \\rightarrow \\int\\int l(f(x), y)p(y|x)p(x)dxdy = \\int\\int l(f(x), y)q(y|x)q(x) \\frac{p(x)}{q(x)}dxdy \\rightarrow \\sum_i \\beta_i l(f(x_i), y_i)$, $\\beta$ can be obtained with logistic regression.\n",
    "  - Label Shift, $p(y) \\neq q(y), p(x|y) = q(x|y)$, the same method $\\beta(y) = p(y) / q(y)$, but now $q(y)$ is hard to get, we need compute a confusion matrix on the val data then use the model to pridcit the distrubution of the $q(y)$\n",
    "  - Concept Shift (the concept of the label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab587f-119f-4a04-bbde-9e7d49953d9d",
   "metadata": {},
   "source": [
    "## Chapter 5 : Multilayer Perceptrons\n",
    "- Activation Function: relu, sigmoid, tanh ($\\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}$)\n",
    "- Numerical stability: vanish and explode are common\n",
    "  - Symmetry: linear layer and conv (with no share weight) layer are symmetric so we can not tell apart from different weight and try to explain it (for example 2 hidden unit with same initial value, they will update the same way), so we need to **Bread the Symmetry** (like using a dropout)\n",
    "  - Xavier initilization: get from distrubution of zero mean and variance $\\sigma = \\sqrt{2 / (n_{in} + n_{out})}$\n",
    "  - Dropout, shared param...\n",
    "- (Rolnick et al., 2017) has revealed that in the setting of label noise, neural networks tend to fit cleanly labeled data **first** and only subsequently to interpolate the mislabeled data.\n",
    "  - so we can use early stop once error on val is minimal or the patience hit. usually combined with regularization.\n",
    "- Dropout:\n",
    "  - $h^{'} = \\left \\{ \n",
    "  \\begin{array}{lll}\n",
    "  & 0, p \\\\\n",
    "  & \\frac{h}{1-p}, 1-p\n",
    "  \\end{array} \n",
    "  \\right .$, now $E[h^{'}] = E[h]$\n",
    "  - We do not use dropout in test, except we want to know the uncertainty of the model output (by comparing different dropout)\n",
    "  - Use lower p in lower layer (to get lower feature), higher p in higher layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8510492-eac8-4c69-bc9f-49cbe5acb237",
   "metadata": {},
   "source": [
    "## Chapter 6: Beginner Guide\n",
    "- Tied layer: gradient will add up along different chain\n",
    "- Custom initialization: `apply` method\n",
    "- I/O\n",
    "  - save tensor: `torch.save(x:Uinon[List[tensor], Dict], name:str)` and load\n",
    "  - save model: the same, just input dict of the net (`net.state_dict()`) then `net.load_state_dict(torch.load(name))`\n",
    "- GPU\n",
    "  - operation between tensors must in the same GPU\n",
    "  - print or transform to numpy will copy to memory, and even worse wait the python **GIL** (`Global Interpreter Lock`, make sure at the same time only one thread can execute the python bytecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72453bd2-625c-49c8-9715-37123159d39e",
   "metadata": {},
   "source": [
    "## Chapter 7 : CNN\n",
    "1. **Invariance**: translation equivariance, locality -> The earliest layers should respond similarly to the same patch and focus on local regions.\n",
    "2. **Convolution**: math is $(f * g)(i, j) = \\sum_a \\sum_b f(a, b)  g(i - a, j - b)$, remind that **cross-correlation** is $(f * g)(i, j) = \\sum_a \\sum_b f(a, b)  g(i + a, j + b)$\n",
    "   - The difference is not important as we will learn the kernel, `k_conv_learned = k_corr_learned.T`, or `conv(X, k_conv_learned) = corr(X, k_corr_learned)`\n",
    "3. **Receptive Field**ï¼š for any element (tensors on the conv layer) x, all the elements that may effect x in the previous layers in the forward population.\n",
    "4. **Padding, Stride**: $\\lfloor (n_h - k_h + p_h + s_h) / s_h \\rfloor \\times \\lfloor (n_w - k_w + p_w + s_w) / s_w \\rfloor$, often `p_h = k_h - 1`, the same for `p_w`. `p_h = p_h_upper + p_h_lower`\n",
    "5. **Channel**:\n",
    "   - multi in $c_i$ -> kernel must also have the same channels ($c_i \\times k_h \\times k_w$), then add them up.\n",
    "   - multi out $c_o$ -> kernel with $c_o \\times c_i \\times k_h \\times k_w$, get $c_o$ output channels.\n",
    "6. use `torch.stack` to stack tensors\n",
    "7. **Pooling**: mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0070a-bf43-4f5b-a1b8-d264e02296f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 8 : Modern CNN\n",
    "1. **AlexNet**: first deep conv successful, using dropout, Relu, polling\n",
    "2. **VGG**: multiple 3 * 3 conv layers (two 3 * 3 conv touch 5 * 5 input as a 5 * 5 conv, but 2 * 3 * 3  = 18 < 25 = 5 * 5)\n",
    "3. **NiN**: to handle 2 problem (1. much ram for the MLP at the end; 2. can not add MLP between the conv to increase the degree of nonlinearity as it will destroy the spatial information)\n",
    "   - use 1 * 1 conv layer to add local nonlinearities across the channel activations\n",
    "   - use global average pooling to integrate across all locations in the last representation layer. (must combine with added nonlinearities)\n",
    "4. **GoogleNet**: Inception layer, parallel conv multi scales, and then concate them\n",
    "5. **Batch Normalization**:\n",
    "   - $BN(\\mathbf x) = \\mathbf{\\gamma} \\bigodot \\frac{\\mathbf x - \\mathbf{\\mu_B}}{\\sigma^2_B} + \\mathbf \\beta$, $\\mathbf{\\mu_B} = \\frac{1}{|B|}\\sum_{x \\in B} \\mathbf x$,\n",
    "     $\\sigma^2_B = \\frac{1}{|B|} \\sum_{x \\in B} (x - \\mathbf{\\mu_B})^2 + \\epsilon$\n",
    "   - On linear layer [N, D] it will get across D (different features in D will not do calculations), on conv layer [N, C, H, W] it will across C (save the difference between channels)\n",
    "     - For example, [N, C, H, W] shape input x, for x[N, 0, H, W], get it's mean mu and std and do (x[N, 0, H, W] - mu) / std, here mu and std are scalar.\n",
    "   - At the testing stage, we will use the global (whole) data mean and varience, instead of minibatch mean and varience. Just like dropout.\n",
    "   - So BN also serves as a noise introducer! (minibatch information != true mean and var) Teye et al. (2018) and Luo et al. (2018).\n",
    "   - So it best works for batch size of 50 ~ 100, higher the noise is small, lower it is too high.\n",
    "   - Moving global mean and var: when testing, no minibatch, so we use a global one that is stored during training.\n",
    "     - It is a kind of exp weighted mean, closest batch has higer weight\n",
    "     - $\\mu_m = \\mu_m * (1 - \\tau) + \\mu * \\tau, \\Sigma_m = \\Sigma_m * (1 - \\tau) + \\Sigma * \\tau$, $\\tau$ is called momentum term.\n",
    "6. **Layer Normalization**: often used in NLP\n",
    "   - For features like [N, A, B] it will save difference between N, A and B are typically seq_len, hidden_size.\n",
    "7. **ResNet**: residual block, pass x as one of the branch before a activation function (for the original paper, and later it is changed to BN -> AC -> Conv)\n",
    "   - To get the passed x has the correct shape to add up, we can use 1 * 1 conv if it is needed\n",
    "   - **Idea**: nested-function class, shallower net (like ResNet-20) is subclass of depper net (like ResNet-50). Because in ResNet-50 if the layers after 20th layer are f(x) = x, then it is the same as RestNet-20! So we can make sure f' (the best we can get in ResNet-50 for certain data) will be better than f (ResNet-20 on the same data) or at least the same.\n",
    "   - <p align=\"center\">\n",
    "       <img alt=\"Residul Block\" src=\"https://d2l.ai/_images/resnet-block.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       Rusidul Block\n",
    "   </p>\n",
    "   - **ResNeXt**: use g groups of 3 * 3 conv layers between two 1 * 1 conv of channel $b$ and $c_o$, so $\\mathcal O(c_i c_o) \\rightarrow \\mathcal O(g ~ c_i / g ~ c_o / g) = \\mathcal O(c_ic_o/g)$\n",
    "     - This is a **Bottleneck** arch if $b < c_i$\n",
    "       </br>\n",
    "   - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/resnext-block.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       ResNeXt Block\n",
    "8. **DenseNet**: instead of plus x, we concatenate x repeatedly.\n",
    "   - For example (\\<channel\\> indicates the channel): x\\<c_1\\> -> f_1(x)\\<c_2\\> end up with [x, f_1(x)]\\<c_1 + c_2\\> -> f_2([x, f_1(x)])\\<c_3\\> end up with [x, f_1(x), f_2([x, f_1(x)])]\\<c_1 + c_2 + c_3\\>\n",
    "   - Too many of this layer will cause the dimeansion too big, so we need some layer to reduce it. **Translation** layer use 1 * 1 conv to reduce channel and avgpool to half the H and W.\n",
    "9. **RegNet**:\n",
    "   - AnyNet: network with **stem** -> **body** -> **head**.\n",
    "   - Distrubution of net: $F(e,Z)=âˆ‘_{i=1}^{n}1(e_i<e)$, use this empirical CDF to approximate $F(e, p)$, $p$ is the net arch distrubution. $Z$ is a sample of net sample from $p$, if $F(e, Z_1) < F(e, Z_2)$ then we say $Z_1$ is better, it's parameters are better.\n",
    "   - So for RegNet, they find that we should use same k (k = 1, no bottlenet, is best, says in paper) and g for the ResNeXt blocks with no harm, and increase the network depth d and weight c along the stage. And keep the c change linearly with $c_j = c_o + c_aj$ with slope $c_a$\n",
    "   - neural architecture search (NAS) : with certain search space, use RL (NASNet), evolution alg (AmoebaNet), gradient based (DARTS) or shared weight (ENAS) to get the model. But it takes to much computation resource.\n",
    "   - <img src=\"https://d2l.ai/_images/anynet.svg\" style=\"background-color: white; display: inline-block;\"> AnyNet Structure (Search Space)\n",
    "   </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0398cca-b9c1-4e42-a8fb-e7d90b8f5754",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 9 : RNN\n",
    "- Two form of sequence to sequence task:\n",
    "  - **aligned**: input at certain time step aligns with corrsponding output, like tagging (fight -> verb)\n",
    "  - **unaligned**: no step-to-step correspondence, like maching translation\n",
    "- **Autoregressive** model: regress value based on previous value\n",
    "  - latent autoregressive models (since $h_t$ is never observed): estimate $P(x_t | x_{t-1} \\dots x_1)$ with $\\hat x_t = P(x_t | h_t)$ and $h_t = g(h_{t-1}, x_{t-1})$\n",
    "- **Sequence Model**: to get joint probablity of a sequence $p(x_1, \\dots, x_T)$, we change it to a form like autoregressive one: $p(x_1) \\prod_{t=2}^T p(x_t|x_{t-1}, \\dots, x_1)$\n",
    "  - **Markov Condition**: if we can make the condition above into $x_{t-1}, \\dots, x_{t-\\tau}$ without any loss, aka the future is conditionally independent of the past, given the recent history, then the sequence satisfies a Markov condition. And it is $\\tau^{th}$-order Markov model.\n",
    "- Zipfâ€™s law: the frequency of words will decrease exponentially, n-grams too (with smaller slope).\n",
    "  - So use word frequency to construct the probility is not good, for example. $\\hat p(learning|deep) = n(deep, learning) / n(deep)$, $n(deep, learning)$ will be very small compared to denominator. We can use so called **Laplace Smooth** but that will not help too much.\n",
    "- **Perplexity**: (how confusion it is), given a true test data, the cross-entropy is $J = \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t | x_{t-1}, \\dots, x_1)$, and the perplexity is $\\exp(J)$.\n",
    "- Partioning the sequence: for a $T$ token indices sequence, we add some randomness, discard first $d \\in U(0, n]$ tokens and partion the rest into $m = \\lfloor (T-d) / n \\rfloor$ group. For a sequence $x_t$ the target sequence is shifted by one token $x_{t+1}$.\n",
    "---\n",
    "- **RNN**: for a vocab with size $|\\mathcal V|$, the model parameters should go up to $|\\mathcal V|^n$, $n$ is the sequence length.So we $P(x_t | x_{t-1} \\dots x_1) \\approx P(x_t | h_{t-1})$ï¼Œ$h$ is a **hidden state**, it varies at different time step and contains information of previous time steps. Hidden layer, on the other hand, is a structure, it dose not change in forward calculation.\n",
    "  - recurrent: $H_t = \\phi (X_tW_{th} + H_{t-1}W_{hh} + b_h)$, output is $O_t = H_tW_{tq} + b_q$.\n",
    "  - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "       RNN Block\n",
    "  - clip the gradient: $g = \\min(1, \\frac{\\theta}{|| g ||}) g$, it is a hack but useful.\n",
    "  - **Warm-up**: When predicting, we can first feed a prefix (now called prompt I think), just iter the prefix into the network without generating output until we need to predict.\n",
    "- For RNN: the input shape is (sequence_length, batch_size, feature_size), first is time_step, third is one-hot dim or word2vec dim.\n",
    "- **Backpropagation through time**\n",
    "  - <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> Computation graph of RNN\n",
    "  - How to reduce gradient explosion or vanishing: truncate the gradient propagete at certain time step.\n",
    "  - In the img above: $\\frac{\\partial L}{\\partial h_T} = W_{qh}^{\\intercal} \\frac{\\partial L}{\\partial o_T}$, $\\frac{\\partial L}{\\partial h_t} = \\sum_{i=t}^T (W_{hh}^{\\intercal})^{T-i} W_{qh}^{\\intercal} \\frac{\\partial L}{\\partial o_{T+t-i}}$, $\\frac{\\partial L}{\\partial W_{hx}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} x_t^{\\intercal}$, $\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} h_{t-1}^{\\intercal}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5f197-9d57-42e8-8db4-2196d7d04264",
   "metadata": {},
   "source": [
    "## Chapter 10 : Modern RNN\n",
    "- **LSTM**\n",
    "  - The structure is :\n",
    "  - <img alt=\"LSTM Arch\" src=\"https://zh.d2l.ai/_images/lstm-3.svg\" style=\"background-color: white; display: inline-block;\"> LSTM Arch\n",
    "- **GRU**\n",
    "  - <img alt=\"GRU Arch\" src=\"https://d2l.ai/_images/gru-3.svg\" style=\"background-color: white; display: inline-block;\"> GRU Arch\n",
    "  - Reset gates help capture short-term dependencies in sequences.\n",
    "  - Update gates help capture long-term dependencies in sequences.\n",
    "- **Deep RNN**\n",
    "  - <img alt=\"Deep RNN\" src=\"https://d2l.ai/_images/deep-rnn.svg\" style=\"background-color: white; display: inline-block;\"> Deep RNN\n",
    "  - In deep rnn, the output is the last layer of the hidden state with every timestep, and state is the last time step hidden state with all layer of rnn.\n",
    "- **Bidirection RNN**, it is slow and gradient chain is long\n",
    "  - $P(x_1,\\ldots,x_T,h_1,\\ldots,h_T)=\\prod_{t=1}^TP(h_t\\mid h_{t-1})P(x_t\\mid h_t),\\mathrm{~where~}P(h_1\\mid h_0)=P(h_1)$, it is a hidden markov model. We can use dynamic programming method compute is from start to end, also from end to start. Just how B-RNN is capable of.\n",
    "  - <img alt=\"Bidirection RNN\" src=\"https://zh.d2l.ai/_images/birnn.svg\" style=\"background-color: white; display: inline-block;\"> B-RNN\n",
    "  - And we just need to concatenate these two H.\n",
    "- **Machine translation**\n",
    "  - non-breaking space, some space should not split to new line, like Mr. Smith.\n",
    "  - Teacher Forcing : all the input will be pad with \\<pad\\>, source token no special treat, decoder input (target seq use as input) will start with \\<bos\\>, and label is shift by 1 (no \\<bos\\> at the begining).\n",
    "  - **Important**: when use teacher forcing, the truth target is feed to the decoder. This will make the traning faster and stable, but it will make training and predicting different (because when predicting we do not have truth target label, we have to repeatedly predict). We can make them the same, but the tranning will be harder.\n",
    "- **Sequence to Sequence**\n",
    "  - We use this Encoder - Decoder Arch to get varied length input and varied length output.\n",
    "  - We do not use one-hot, instead we use nn.Embed layer, which will take token i, and return ith row of the matrix of this embeding layer.\n",
    "  - From the encoder, we get the hidden states, and use a funcion $c = q(h_1, \\cdots, h_T)$, for example, just use the $h_T$. And in the decoder, we concatenate this with the target embed output, and feed to rnn.\n",
    "  - When calculating the loss, we should not take \\<pad\\> into acount. So we need to musk the loss with the tokens.\n",
    "  - <img alt=\"Encoder Decoder\" src=\"https://d2l.ai/_images/seq2seq-details.svg\" style=\"background-color: white; display: inline-block;\"> Encoder Decoder\n",
    "  - Bilingual Evaluation Understudy, BLEU evaluates whether this n-gram in the predicted sequence appears in the target sequence. For example, target sequence ABCDEF, predict sequence ABBCD, $p_1 = 4/5$, we have ABCD in the target sequence, $p_2 = 3 / 4$, we have AB, BC, CD. So we get BLEU as $\\exp\\left(\\min\\left(0,1-\\frac{\\mathrm{len}_{\\mathrm{label}}}{\\mathrm{len}_{\\mathrm{pred}}}\\right)\\right)\\prod_{n=1}^kp_n^{1/2^n}$, higher n will have higher weight, small length of predict length takes lower.\n",
    "- **Beam Search**\n",
    "  - Before this section, we use greedy search to get prediction, use argmax on the prediction vector : $y_{t^{\\prime}}=\\underset{y\\in\\mathcal{Y}}{\\operatorname*{\\operatorname*{argmax}}}P(y\\mid y_1,\\ldots,y_{t^{\\prime}-1},\\mathbf{c})$, where $\\mathcal Y$ is the vacab. Once our model outputs â€œ<eos>â€ (or we reach the maximum length $T'$) the output sequence is completed.\n",
    "  - However, use the most likely tokens is not the same with the most likely sequence : $\\prod_{t^{\\prime}=1}^{T^{\\prime}}P(y_{t^{\\prime}}\\mid y_1,\\ldots,y_{t^{\\prime}-1},\\mathbf{c})$. For example, in this figure below, ACB will have this probability of 0.5 * 0.3 * 0.6 = 0.09. On the other hand, greedy search choose ABC which is 0.5 * 0.4 * 0.4 = 0.08, it is lower, not optimal!\n",
    "  - <img alt=\"Max sequence\" src=\"https://d2l.ai/_images/s2s-prob2.svg\" style=\"background-color: white; display: inline-block;\">Max sequence <img alt=\"Max token\" src=\"https://d2l.ai/_images/s2s-prob1.svg\" style=\"background-color: white; display: inline-block;\"> Max token.\n",
    "  - If we want the optimal one, we need to do exhaustive search, search all possible sequence, it is not possible!\n",
    "  - The most straightforward type of beam search is keep k candidates. In time step 2, we get $P ( A, y_{2} \\mid\\mathbf{c} )=P ( A \\mid\\mathbf{c} ) P ( y_{2} \\mid A, \\mathbf{c} )$ for the top, and $P ( C, y_{2} \\mid\\mathbf{c} )=P ( C \\mid\\mathbf{c} ) P ( y_{2} \\mid C, \\mathbf{c} ) $ for the bottom, then choose most 2 from them. And then choose sequence that maximize $\\frac{1} {L^{\\alpha}} \\mathrm{l o g} \\, P ( y_{1}, \\ldots, y_{L} \\mid\\mathbf{c} )=\\frac{1} {L^{\\alpha}} \\sum_{t^{\\prime}=1}^{L} \\mathrm{l o g} \\, P ( y_{t^{\\prime}} \\mid y_{1}, \\ldots, y_{t^{\\prime}-1}, \\mathbf{c} ) ; $. Note tha we have **6** candidates (A, C ..).\n",
    "  - <img alt=\"Max sequence\" src=\"https://d2l.ai/_images/beam-search.svg\" style=\"background-color: white; display: inline-block;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52b3fd-45fa-4fa6-8a6c-4ba56c83b8b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter 11 : Attention Mechanisms and Transformers\n",
    "- **Idea** : attention first come up in encoder - decoder design, rather than tranform the input to a fixed size feature and the feed it to all the decoder step, we want to create a representation that has the same length of input and decoder at each time step can pay attention to different input sequence (with it's weight). And transfromer give up residual connection, instead use attention at all.\n",
    "- **Q, K, V**\n",
    "  - Define database $\\mathcal{D} \\stackrel{\\mathrm{d e f}} {=} \\{( \\mathbf{k}_{1}, \\mathbf{v}_{1} ), \\ldots( \\mathbf{k}_{m}, \\mathbf{v}_{m} ) \\} $, give some query, the attention is $\\text{Attention}(q, D) \\stackrel{\\mathrm{d e f}} {=} \\sum_{i=1}^m \\alpha(q, k_i)v_i $, where $\\alpha(q, k_i)$ are scalar attention weight, this operation is also called attention pooling. We want this attention weight to be larger than 0 and sum up to 1, so we can use a softmax to transfrom it.\n",
    "- **Attention pooling with similarity**\n",
    "  - In a regression task, we can use kernel output as the attention weight (after normalization).\n",
    "  - When directly compute loss $(f(x_i) - y_i)^2$ï¼Œ because we have $y_i$, the $\\sigma$ will go to zero, causing overfitting. Even if we remove $y_i$ from the loss, if the dataset is large enough, we may still overfit.\n",
    "  - <img alt=\"Attention Pooling\" src=\"https://d2l.ai/_images/attention-output.svg\" style=\"background-color: white; display: inline-block;\"> Attention Pooling\n",
    "- **Attention Scoring Function**\n",
    "  - **Dot Product Attention**: note that kernel of gaussian is $a(q, k_i) = q^{\\intercal}k_i - 0.5 * ||q||^2 - 0.5 * ||k_i||^2$, and after the softmax normalization, second one is cancled out. Then if we get $k_i$ with batch or layer normalization, it's length will be bounded and often constant, so we can get rid of last term without penalty. This leads to $a(q, k_i) = q^{\\intercal}k_i$, ann assume both $q$ and $k_i$ have zero mean and unit variance, this attention weight will have a variance of $d$ (which is query feature size), so we can further normalize it : $a(q, k_i) = q^{\\intercal}k_i / \\sqrt{d}$, this is the common one used in transformer. At last, we do a softmax on it.\n",
    "  - Because we do not want consider \\<pad\\>, so we can do musk softmax. And for Batch Matrix Multiplication, use torch.bmm. bmm(Q, K) take Q(n, a, b) and K(n, b, c), which return [Q1 @ K1, ..., Q_n @ K_n].\n",
    "  - **Additive Attention**: when q and k has different feature size, we can do a transform ($q^{\\intercal}Mk$), or use this additive attention $a(\\mathbf{q},\\mathbf{k})=\\mathbf{w}_v^\\top\\tanh(\\mathbf{W}_q\\mathbf{q}+\\mathbf{W}_k\\mathbf{k})\\in\\mathbb{R}$, inside the () we add by broadcasting.\n",
    "- **Bahdanau Attention**\n",
    "  - Attention function will work between encoder hidden state and decoder hidden state, $c_{t'} = \\sum_t^T a(s_{t'-1}, h_t)h_t$, and this will used to generate $s_{t'}$.\n",
    "  - <img alt=\"Bahdanau Attention\" src=\"https://d2l.ai/_images/seq2seq-details-attention.svg\" style=\"background-color: white; display: inline-block;\"> Bahdanau Attention\n",
    "  - Seq2SeqAttentionDecoder first use encoder last layer hidden state as the query, and later use it's own hidden state from it's rnn model. Keys and values are all encoder outputs (last layer hidden state at all time step), then concatenate the embed X with this context (attention result) then feed to it's rnn.\n",
    "- **Multi-head Attention**\n",
    "  - <img alt=\"Multi-head Attention\" src=\"https://d2l.ai/_images/multi-head-attention.svg\" style=\"background-color: white; display: inline-block;\"> Multi-head Attention\n",
    "  - We want the same Q, K, V to have different behaviour with the same attention mechanism, so we have to copy them $h$ times and first pass them into FC layer which has learnable param that can change the QKV, then feed to attention, get $h$ results, concatenate them. $h_i = f(W_i^{q}q_i, W_i^kk_i, W_i^vv_i)$, $f$ is attention pooling, each $W$ have shape of $(p_q, d_q), (p_k, d_k)$ and $(p_v, d_v)$, $h_i$ is of shape $(p_v,)$. We concatenate these h to a $(h \\times p_v,)$ shape. And we use a big learnable matrix of shape $(p_o, h \\times p_v)$ times the concatenated result, which finnally return output of shape $(p_o, )$. For the purpose of parallel computation, we set $hp_q = hp_k = hp_v = p_o$.\n",
    "  - Impl of d2l is the same idea, but for parrallel, it use some trick, hidden_size is h * p_q, put num_head into batch_size, make the batch_size = batch_size * num_head, and in the output reverse it.\n",
    "- **Self Attention**\n",
    "  - Do the attention(X, X, X) to get encoder. Compare CNN, RNN and self-attention, given n sequence with d dimension. CNN : choose kernel of 3, computation is O(knd^2), longest connect path is O(n/k). RNN : compute O(nd^2), path O(n). Self attention : compute O(n^2d), path O(1).\n",
    "  - Positional Encoding: self attention does not contain position (order) information, so a token at time step 1 and 5 are the same (but it should not!). So we need to add something to keep the position information. First we use fixed position encoding with sine and cosine. $X^{n \\times d}$ is the input representation of n tokens, and the position encoding is $X + P$, where $p_{i,2j} = \\sin \\left( \\frac{i}{10000^{2j/d}} \\right)$ and $p_{i,2j+1} = \\cos \\left( \\frac{i}{10000^{2j/d}} \\right)$. This works because the function abouve contain different frequency information.\n",
    "  - Relative position encoding : $\\begin{bmatrix}\n",
    "\\cos(\\delta\\omega_j) & \\sin(\\delta\\omega_j) \\\\\n",
    "-\\sin(\\delta\\omega_j) & \\cos(\\delta\\omega_j)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "p_{i,2j} \\\\\n",
    "p_{i,2j+1}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "p_{i+\\delta,2j} \\\\\n",
    "p_{i+\\delta,2j+1}\n",
    "\\end{bmatrix}$, we can just add a (1, step, hidden) param, and add it to the embed(X) to learn the position.\n",
    "- **Transformer**\n",
    "  - <img alt=\"Transformer\" src=\"https://d2l.ai/_images/transformer.svg\" style=\"background-color: white; display: inline-block;\"> Transformer Arch\n",
    "  - The encoder-decoder attention layer take decoder self-attention layer output as query, and encoder output as key and value.\n",
    "  - Note that in decoder self-attention, we will carefully mask output to reserve the autoregreesive nature, we do not take position in the outpt (later as the input of decoder self-attention layer) after the position we are calculating.\n",
    "  - Before pos encoding we first multiply sqrt(d) with embed(X) to rescale it, maybe because embed(X) has small variace.\n",
    "  - For prediction, we need to cache the input X for the decoder, in training we can just compute all time step all together. In impl, it is cached in state[2].\n",
    "  - **!!** only the last output of the encoder will do attention on all block of decoder.\n",
    "- **Vision Transformer**\n",
    "  - <img alt=\"Vision Transformer\" src=\"https://d2l.ai/_images/vit.svg\" style=\"background-color: white; display: inline-block;\"> Vision Transformer\n",
    "  - patch embeding will feed to a conv then flatten it, return shape of (batch, patch, hidden)\n",
    "  - Do the normalization before the attention is better for the efficient learning of transformer. The vit mlp layer use GELU and dropout is applied to the output of each fully connected layer in the MLP for regularization.\n",
    "\n",
    "- Large Scale Pre-training\n",
    "  - Encoder only, ViT, BERT. BERT use masked language modeling, and for a token, tokens at left and right can all attend to this masked token. So it is a bidirection encoder --- in the figure below, each token along the vertical axis attends to all input tokens along the horizontal axis.\n",
    "  - <img alt=\"BERT\" src=\"https://d2l.ai/_images/bert-encoder-only.svg\" style=\"background-color: white; display: inline-block;\"> BERT\n",
    "  - Encoder-Decoder, BART & T5, both attempt to reconstruct original text in their pretraining objectives, while the former emphasizes noising input (e.g., masking, deletion, permutation, and rotation) and the latter highlights multitask unification with comprehensive ablation studies.\n",
    "  - <img alt=\"T5\" src=\"https://d2l.ai/_images/t5-encoder-decoder.svg\" style=\"background-color: white; display: inline-block;\"> T5\n",
    "  - Decoder only, GPT. **In-context learning** : conditional on an input sequence with the task description, task-specific inputâ€“output examples, and a prompt (task input). \n",
    "  - <img alt=\"GPT\" src=\"https://d2l.ai/_images/gpt-decoder-only.svg\" style=\"background-color: white; display: inline-block;\"> GPT\n",
    "  - <img alt=\"x - shot\" src=\"https://d2l.ai/_images/gpt-3-xshot.svg\" style=\"background-color: white; display: inline-block;\"> x-shot\n",
    "- Efficient Transformer design (see that survey)\n",
    "  - Sparse attentionï¼šLongformerï¼šä½¿ç”¨æ»‘åŠ¨çª—å£å’Œå…¨å±€æ³¨æ„åŠ›ï¼Œé™ä½å¤æ‚åº¦åˆ°O(n)ã€‚BigBirdï¼šç»“åˆéšæœºæ³¨æ„åŠ›ã€çª—å£æ³¨æ„åŠ›å’Œå…¨å±€æ³¨æ„åŠ›ï¼Œé€‚åˆé•¿åºåˆ—ã€‚\n",
    "  - Low rank approximationï¼šLinformerï¼šé€šè¿‡ä½ç§©åˆ†è§£å°†æ³¨æ„åŠ›çŸ©é˜µæŠ•å½±åˆ°è¾ƒä½ç»´åº¦ï¼Œå¤æ‚åº¦ä»O(nÂ²)é™åˆ°O(n)ã€‚\n",
    "  - Memoryï¼šTransformer-XLï¼šå¼•å…¥å¾ªç¯è®°å¿†ï¼Œå¤„ç†é•¿åºåˆ—æ—¶é‡ç”¨ä¹‹å‰çš„éšè—çŠ¶æ€ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚\n",
    "  - Efficient attentionï¼šPerformerï¼šä½¿ç”¨æ ¸æ–¹æ³•ï¼ˆFavor+ï¼‰è¿‘ä¼¼ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œå¤æ‚åº¦é™ä¸ºO(n)ã€‚\n",
    "  - Model compressï¼šDistillationï¼šå°†å¤§Transformerè’¸é¦ä¸ºå°æ¨¡å‹ï¼ˆå¦‚DistilBERTï¼‰ã€‚é‡åŒ–ï¼šå‡å°‘å‚æ•°ç²¾åº¦ï¼Œé™ä½å†…å­˜å ç”¨ã€‚\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\"> -->\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee3e187-0c67-4bc3-a076-bdfd591cfadf",
   "metadata": {},
   "source": [
    "## Chapter 12 : Optimization\n",
    "- **Convexity**\n",
    "  - Convex set : for any $a, b \\in \\mathcal X$, given $\\lambda \\in [0, 1]$, $\\lambda a + (1 - \\lambda) b \\in \\mathcal X$.\n",
    "  - Convex function : for any function $f : \\mathcal X \\rightarrow \\mathbb R$, we have $\\lambda f(x) + (1 - \\lambda) f(x') >= f(\\lambda x + (1-\\lambda)x')$.\n",
    "  - jensen's inequality : $\\sum_i\\alpha_if(x_i)\\geq f\\left(\\sum_i\\alpha_ix_i\\right)$ and $E_X[f(X)]\\geq f\\left(E_X[X]\\right)$\n",
    "  - Properties : Local Minima Are Global Minima, below set $\\mathcal{S}_b\\overset{\\mathrm{def}}{\\operatorname*{=}}\\{x|x\\in\\mathcal{X}\\mathrm{~and~}f(x)\\leq b\\}$ is also a convex set, f is convex if hessian of f is positive semidefinite ($\\nabla^2 f = H, x^THx >= 0$).\n",
    "  - Convex with constraint $c_i(x) <= 0$, can be dealed with lagrangian, and the KKT condition. KKT are:\n",
    "    - Stationarity : $\\nabla_x L(f(x), \\lambda_1, \\ldots, \\lambda_n) = 0$.\n",
    "    - Primal Feasibility : $c_i(x) <= 0$\n",
    "    - Dual Feasibility : $\\lambda_i >= 0$\n",
    "    - Complementary Slackness : $\\lambda_i c_i(x) = 0$\n",
    "  - Penality is robust than constraint. We can also use projection to satisfy constraints.\n",
    "- **Gradient Descent**\n",
    "  - $x \\leftarrow x - \\eta \\nabla_x f(x)$, with newton's method $\\eta = \\nabla_x^{-2} f(x) = H^{-1}$\n",
    "  - H is expensive, so we can use precondition $x \\leftarrow x - \\eta \\text{diag}(H)^{-1}\\nabla_x f(x)$, this means for different $x_i$ we use different learning rates.\n",
    "  - Line search : use binary search to find $\\eta$ that minimize $f(x - \\eta \\nabla_x f(x))$.\n",
    "- **SGD**ï¼š converge with rate $\\mathcal O (1/\\sqrt T)$, $T$ is the sample number. More details of the math please see the book.\n",
    "- **Momentum**\n",
    "  - Use leaky average $v_k = \\beta v_{k-1} + g_{k, k-1}$ as the gradient, this is the momentum!\n",
    "  - Gradient descent with and without momentum for a convex quadratic function decomposes into coordinate-wise optimization in the direction of the eigenvectors of the quadratic matrix.\n",
    "  - The velocity converge condition is loose than gradient converge condition, so add momentum (with big $\\beta$ ) is theoritaly better.\n",
    "- **Adagrad** : it is a SGD alg\n",
    "  - Some features are rare, so we want to update it faster ( we do not update their gradient much ).\n",
    "  - Some problem has large condition number $k = \\lambda_{max} / \\lambda_{min}$, which is not good. We can rescale them by some matrix (if Hessian of the problem L is possitive semidefinite), or just rescalse the diag of the Q. $\\tilde Q = \\text{diag}(Q)^{-1/2}Q\\text{diag}(Q)^{-1/2}$. However this is not realistic in DL, because we don't have second derivitive of Q, so Adagrad use the norm of the gradient as the scalse item. And this makes it adjust element wise (like only diag will change).\n",
    "  - $s_t = s_{t-1} + g_t^2, w_t = w_t - \\eta / \\sqrt{s_t+\\epsilon} \\odot g_t$, one problem of Adagrad is that it's learning rate decrease $\\mathcal O(t^{-1/2})$.\n",
    "- **RMSProp**\n",
    "  - $s_t = \\gamma s_{t-1} + (1-\\gamma) g_t^2$, only difference with Adagrad\n",
    "- **Adadelta**\n",
    "  - $\\mathbf{s}_{t}=\\rho \\mathbf{s}_{t-1}+(1-\\rho) \\mathbf{g}_{t}^{2}$, $\\mathbf{g}_{t}^{\\prime}=\\frac{\\sqrt{\\Delta \\mathbf{x}_{t-1}+\\epsilon}}{\\sqrt{\\mathbf{s}_{t}+\\epsilon}} \\odot \\mathbf{g}_{t}$, $x_t = x_t - \\mathbf{g}_{t}^{\\prime}$, $\\Delta\\mathbf{x}_{t}=\\rho\\Delta\\mathbf{x}_{t-1}+( 1-\\rho) \\mathbf{g}_{t}^{\\prime\\, 2}, $.\n",
    "- **Adam**\n",
    "  - $v_t = \\beta_1 v_{t-1} + (1-\\beta_1)g_{t}$, $s_t = \\beta_2 s_{t-1} + (1-\\beta_2)g^2_{t}$, and the rescale it (otherwise the initial numbers are too diverge from gradient), $\\hat v_t = v_t / (1 + \\beta_1^t)$, $\\hat s_t = s_t / (1 + \\beta_2^t)$. Then finnally $x_t = x_t - \\eta \\hat v_t / (\\sqrt{\\hat s_t} + \\epsilon)$\n",
    "  - One of the problems of Adam is that it can fail to converge even in convex settings when the second moment estimate in $s_t$ blows up as $g^2_t$ being too large and forget the history. Yogi update is $s_t = s_{t-1} + (1-\\beta_2)g^2_{t} \\odot (g^2_{t} - s_{t-1})$, the update is not the deviation of $g^2_{t} - s_{t-1}$, it is $g^2_{t}$ with regard to the sign.\n",
    "- Scheduler\n",
    "  - Warmup: In particular they find that a warmup phase limits the amount of divergence of parameters in very deep networks. A closer look at deep learning heuristics: learning rate restarts, warmup and distillation. ArXiv:1810.13243.\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\"> -->\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bd627-0c39-4250-beab-8bd912b57950",
   "metadata": {},
   "source": [
    "## Chapter 13 : Computation\n",
    "- compiler\n",
    "  - net = torch.jit.script(net)\n",
    "- Automatic Parallesim\n",
    "  - y.to('cpu', non_blocking=non_blocking) for y in x, will return x[i-1] when calculate x[i]\n",
    "- Tranning on multiple GPU\n",
    "  - <img alt=\"Partion Methods\" src=\"https://d2l.ai/_images/splitting.svg\" style=\"background-color: white; display: inline-block;\"> Partion Methods\n",
    "  - nn.parallel.scatter to split data to different devices\n",
    "  - æ˜¾å¼åŒæ­¥ï¼ˆtorch.cuda.synchronize()ï¼‰ä»…åœ¨éœ€è¦ç²¾ç¡®æµ‹é‡æ‰§è¡Œæ—¶é—´æˆ–è°ƒè¯•å¼‚æ­¥é”™è¯¯æ—¶å¿…è¦ï¼Œå…¶ä»–æƒ…å†µä¼šè‡ªå·±æ ¹æ®cpuæˆ–è€…åç»­æ•°æ®éœ€æ±‚éšå¼è°ƒç”¨\n",
    "- Concise impl :\n",
    "  - What we need to do\n",
    "    - Network parameters need to be initialized across all devices.\n",
    "    - While iterating over the dataset minibatches are to be divided across all devices.\n",
    "    - We compute the loss and its gradient in parallel across devices.\n",
    "    - Gradients are aggregated and parameters are updated accordingly.\n",
    "  - Use torch.nn.parallel.DistributedDataParallel\n",
    "- Parameter Server\n",
    "  - <img alt=\"Parameter Exchange\" src=\"https://d2l.ai/_images/ps-distributed.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "  - last graph above assume gradient can be divided into four parts, and exchange each one of them each GPU.\n",
    "  - Ring Synchronization\n",
    "  - Keyâ€“Value Stores\n",
    "\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\"> -->\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd5258-54da-404e-948e-0a1c5ce85984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd36908-a9f8-413d-8c45-011c7e2cd889",
   "metadata": {},
   "source": [
    "åœ¨**æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å·¥ç¨‹**å’Œ**ç¡¬ä»¶ä¼˜åŒ–**æ–¹é¢ï¼Œéœ€è¦æŒæ¡ä¸€ç³»åˆ—å·¥å…·ã€æŠ€æœ¯å’Œæœ€ä½³å®è·µï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆè®­ç»ƒã€ä¼˜åŒ–å’Œéƒ¨ç½²ã€‚  \n",
    "\n",
    "---\n",
    "\n",
    "# **1. æ·±åº¦å­¦ä¹ å·¥ç¨‹**\n",
    "**ç›®æ ‡**ï¼šä¸ä»…è¦è®­ç»ƒæ¨¡å‹ï¼Œè¿˜è¦èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­é«˜æ•ˆåœ°**æ•°æ®å¤„ç†ã€è®­ç»ƒã€è°ƒä¼˜ã€éƒ¨ç½²å’Œç»´æŠ¤**ã€‚\n",
    "\n",
    "## **1.1 æ•°æ®å·¥ç¨‹**\n",
    "æ·±åº¦å­¦ä¹ çš„æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ•°æ®è´¨é‡å’Œé¢„å¤„ç†æ•ˆç‡ã€‚\n",
    "\n",
    "### **(1) æ•°æ®æ”¶é›†ä¸å­˜å‚¨**\n",
    "- **ç»“æ„åŒ–æ•°æ®**ï¼ˆSQL, Pandas, BigQueryï¼‰\n",
    "- **å›¾åƒæ•°æ®**ï¼ˆOpenCV, PIL, TensorFlow Datasetsï¼‰\n",
    "- **æ–‡æœ¬æ•°æ®**ï¼ˆNLTK, Hugging Face Datasetsï¼‰\n",
    "- **æµæ•°æ®**ï¼ˆKafka, Apache Sparkï¼‰\n",
    "\n",
    "### **(2) æ•°æ®é¢„å¤„ç†**\n",
    "- **æ ‡å‡†åŒ– / å½’ä¸€åŒ–**ï¼ˆMin-Max Scaling, Z-scoreï¼‰\n",
    "- **æ•°æ®å¢å¼º**ï¼ˆå›¾åƒï¼šæ—‹è½¬ã€è£å‰ªï¼›æ–‡æœ¬ï¼šåŒä¹‰è¯æ›¿æ¢ï¼‰\n",
    "- **é™ç»´**ï¼ˆPCA, t-SNE, UMAPï¼‰\n",
    "- **ç¼ºå¤±å€¼å¤„ç†**ï¼ˆå‡å€¼å¡«å……ã€æ’å€¼ï¼‰\n",
    "\n",
    "### **(3) æ•°æ®åŠ è½½ä¼˜åŒ–**\n",
    "- **æ‰¹é‡åŠ è½½ï¼ˆBatch Loadingï¼‰**\n",
    "- **å¤šçº¿ç¨‹ / å¤šè¿›ç¨‹æ•°æ®é¢„å¤„ç†ï¼ˆDataloader, TensorFlow tf.dataï¼‰**\n",
    "- **TFRecord / HDF5**ï¼ˆäºŒè¿›åˆ¶æ ¼å¼åŠ é€Ÿæ•°æ®è¯»å–ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## **1.2 è®­ç»ƒä¸è¶…å‚æ•°è°ƒä¼˜**\n",
    "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†å‹è¿‡ç¨‹ï¼Œéœ€è¦é«˜æ•ˆçš„**ä¼˜åŒ–ç­–ç•¥**å’Œ**è¶…å‚æ•°è°ƒæ•´**ã€‚\n",
    "\n",
    "### **(1) è®­ç»ƒä¼˜åŒ–**\n",
    "- **ä¼˜åŒ–å™¨é€‰æ‹©**\n",
    "  - SGDï¼ˆæ ‡å‡†æ¢¯åº¦ä¸‹é™ï¼‰\n",
    "  - Adam / RMSpropï¼ˆè‡ªé€‚åº”ä¼˜åŒ–ï¼‰\n",
    "  - LARS / LAMBï¼ˆç”¨äºå¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒï¼‰\n",
    "  \n",
    "- **æ­£åˆ™åŒ–**\n",
    "  - Dropoutï¼ˆéšæœºä¸¢å¼ƒç¥ç»å…ƒï¼‰\n",
    "  - Batch Normalizationï¼ˆæ‰¹é‡å½’ä¸€åŒ–ï¼‰\n",
    "  - Weight Decayï¼ˆL2 æ­£åˆ™åŒ–ï¼‰\n",
    "\n",
    "- **æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰**\n",
    "  - è§£å†³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜\n",
    "\n",
    "### **(2) è¶…å‚æ•°ä¼˜åŒ–**\n",
    "è‡ªåŠ¨æœç´¢æœ€ä¼˜è¶…å‚æ•°ï¼ˆä¾‹å¦‚å­¦ä¹ ç‡ã€batch sizeã€æƒé‡åˆå§‹åŒ–ï¼‰ã€‚\n",
    "- **Grid Searchï¼ˆç½‘æ ¼æœç´¢ï¼‰**\n",
    "- **Random Searchï¼ˆéšæœºæœç´¢ï¼‰**\n",
    "- **Bayesian Optimizationï¼ˆè´å¶æ–¯ä¼˜åŒ–ï¼‰**\n",
    "- **Hyperbandï¼ˆé«˜æ•ˆé‡‡æ ·ï¼‰**\n",
    "- **Optuna / Ray Tuneï¼ˆè‡ªåŠ¨åŒ–è¶…å‚æ•°è°ƒä¼˜å·¥å…·ï¼‰**\n",
    "\n",
    "---\n",
    "\n",
    "## **1.3 è®­ç»ƒåŠ é€Ÿ**\n",
    "å¤§è§„æ¨¡è®­ç»ƒæ—¶éœ€è¦é«˜æ•ˆçš„è®­ç»ƒåŠ é€ŸæŠ€æœ¯ï¼š\n",
    "\n",
    "### **(1) GPU åŠ é€Ÿ**\n",
    "- è®­ç»ƒæ—¶å°½å¯èƒ½åˆ©ç”¨ **CUDA** / **cuDNN**\n",
    "- **æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precisionï¼‰**ï¼šä½¿ç”¨ FP16ï¼ˆHalf Precisionï¼‰åŠ é€Ÿè®¡ç®—\n",
    "- **æ•°æ®å¹¶è¡Œï¼ˆDataParallelï¼‰** vs. **æ¨¡å‹å¹¶è¡Œï¼ˆModelParallelï¼‰**\n",
    "  \n",
    "### **(2) åˆ†å¸ƒå¼è®­ç»ƒ**\n",
    "- **å•æœºå¤šå¡ï¼ˆMulti-GPU Trainingï¼‰**\n",
    "  - PyTorch `DataParallel`\n",
    "  - PyTorch `DistributedDataParallel (DDP)`\n",
    "  \n",
    "- **å¤šæœºå¤šå¡ï¼ˆMulti-Node Trainingï¼‰**\n",
    "  - TensorFlow `MirroredStrategy`\n",
    "  - Horovodï¼ˆUber æå‡ºçš„é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## **1.4 éƒ¨ç½²ä¸æ¨ç†ä¼˜åŒ–**\n",
    "æ·±åº¦å­¦ä¹ ä¸ä»…è¦è®­ç»ƒï¼Œè¿˜è¦åœ¨**è¾¹ç¼˜è®¾å¤‡**æˆ–**æœåŠ¡å™¨ç«¯**é«˜æ•ˆæ¨ç†ã€‚\n",
    "\n",
    "### **(1) æ¨¡å‹å‹ç¼©**\n",
    "- **å‰ªæï¼ˆPruningï¼‰**ï¼šå»æ‰ä¸é‡è¦çš„æƒé‡\n",
    "- **é‡åŒ–ï¼ˆQuantizationï¼‰**ï¼š\n",
    "  - **8-bit INT é‡åŒ–**ï¼ˆTensorRT, TFLiteï¼‰\n",
    "  - **æ··åˆç²¾åº¦æ¨ç†ï¼ˆFP16, INT8ï¼‰**\n",
    "\n",
    "- **çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰**ï¼š\n",
    "  - ç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹ï¼Œæé«˜æ¨ç†æ•ˆç‡\n",
    "\n",
    "### **(2) æ¨ç†æ¡†æ¶**\n",
    "- **ONNXï¼ˆOpen Neural Network Exchangeï¼‰**ï¼šæ¨¡å‹é€šç”¨æ ¼å¼ï¼Œå¯ç”¨äº PyTorch / TensorFlow äº’è½¬\n",
    "- **TensorRTï¼ˆNVIDIAï¼‰**ï¼šé«˜æ•ˆçš„ GPU åŠ é€Ÿæ¨ç†\n",
    "- **TVMï¼ˆApacheï¼‰**ï¼šè‡ªåŠ¨ä¼˜åŒ–æ¨¡å‹æ¨ç†\n",
    "\n",
    "### **(3) éƒ¨ç½²æ–¹å¼**\n",
    "- **æœåŠ¡å™¨éƒ¨ç½²**\n",
    "  - Flask / FastAPIï¼ˆREST API éƒ¨ç½²ï¼‰\n",
    "  - TensorFlow Serving / TorchServeï¼ˆé«˜æ•ˆæ¨¡å‹æœåŠ¡ï¼‰\n",
    "\n",
    "- **ç§»åŠ¨ç«¯ / è¾¹ç¼˜éƒ¨ç½²**\n",
    "  - TensorFlow Liteï¼ˆTFLiteï¼‰\n",
    "  - CoreMLï¼ˆiOS è®¾å¤‡ï¼‰\n",
    "  - NVIDIA Jetsonï¼ˆåµŒå…¥å¼ AIï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "# **2. ç¡¬ä»¶ä¼˜åŒ–**\n",
    "æ·±åº¦å­¦ä¹ çš„è®¡ç®—é‡æå¤§ï¼Œç¡¬ä»¶çš„ä¼˜åŒ–èƒ½**æ˜¾è‘—æé«˜è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦**ã€‚\n",
    "\n",
    "## **2.1 GPU è®¡ç®—**\n",
    "GPU æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒè®¡ç®—è®¾å¤‡ï¼ŒNVIDIA CUDA ç”Ÿæ€è‡³å…³é‡è¦ã€‚\n",
    "\n",
    "### **(1) GPU ç¼–ç¨‹åŸºç¡€**\n",
    "- CUDA ç¼–ç¨‹ï¼ˆæŒæ¡ Kernel ç¼–å†™ï¼‰\n",
    "- cuDNNï¼ˆæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼‰\n",
    "- Tensor Coreï¼ˆç”¨äºæ··åˆç²¾åº¦è®¡ç®—ï¼‰\n",
    "  \n",
    "### **(2) GPU è®­ç»ƒä¼˜åŒ–**\n",
    "- **å‡å°‘ CPU-GPU ä¼ è¾“**ï¼ˆä¼˜åŒ– `pin_memory=True`ï¼‰\n",
    "- **æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰**ï¼Œå‡å°‘æ˜¾å­˜å ç”¨\n",
    "- **ä½¿ç”¨ FP16 è®­ç»ƒ**ï¼ˆæé«˜ååé‡ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2 åˆ†å¸ƒå¼è®¡ç®—**\n",
    "é€‚ç”¨äº**è¶…å¤§è§„æ¨¡æ•°æ®è®­ç»ƒ**ï¼ˆå¦‚ GPTã€Llama ç­‰æ¨¡å‹ï¼‰ã€‚\n",
    "\n",
    "### **(1) å¹¶è¡Œç­–ç•¥**\n",
    "- **æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰**\n",
    "  - å¤åˆ¶æ¨¡å‹åˆ°å¤šä¸ª GPUï¼Œæ¯ä¸ª GPU è®­ç»ƒä¸åŒæ•°æ®\n",
    "  - PyTorch `DistributedDataParallel (DDP)`\n",
    "\n",
    "- **æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰**\n",
    "  - é€‚ç”¨äºè¶…å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰\n",
    "  - DeepSpeed / Megatron-LM ä¼˜åŒ–\n",
    "\n",
    "- **æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰**\n",
    "  - å°†ä¸åŒå±‚åˆ†é…åˆ°ä¸åŒ GPUï¼Œæé«˜è®¡ç®—æ•ˆç‡\n",
    "  - **é€‚ç”¨äº Transformer è®­ç»ƒ**\n",
    "\n",
    "### **(2) é«˜æ•ˆé€šä¿¡**\n",
    "- **NCCLï¼ˆNVIDIA Collective Communication Libraryï¼‰**ï¼šä¼˜åŒ– GPU ä¹‹é—´çš„é€šä¿¡\n",
    "- **RDMAï¼ˆè¿œç¨‹ç›´æ¥å†…å­˜è®¿é—®ï¼‰**ï¼šç”¨äº GPU æœåŠ¡å™¨é—´é«˜é€Ÿé€šä¿¡\n",
    "\n",
    "---\n",
    "\n",
    "## **2.3 ä¸“ç”¨ AI ç¡¬ä»¶**\n",
    "é™¤äº† GPUï¼ŒAI è®­ç»ƒè¿˜å¯ä»¥ç”¨ä¸“ç”¨èŠ¯ç‰‡åŠ é€Ÿï¼š\n",
    "- **TPUï¼ˆGoogleï¼‰**ï¼šä¸“é—¨ä¼˜åŒ–æ·±åº¦å­¦ä¹ è®¡ç®—\n",
    "- **Graphcore IPU**ï¼ˆç¨€ç–è®¡ç®—ä¼˜åŒ–ï¼‰\n",
    "- **Cerebras Wafer-Scale Engine**ï¼ˆè¶…å¤§è§„æ¨¡ AI è®¡ç®—ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "# **3. æ€»ç»“**\n",
    "| **ç±»åˆ«** | **å…³é”®å†…å®¹** |\n",
    "|----------|--------------|\n",
    "| **æ•°æ®å·¥ç¨‹** | æ•°æ®æ¸…æ´—ã€æ•°æ®å¢å¼ºã€æ•°æ®åŠ è½½ä¼˜åŒ– |\n",
    "| **è®­ç»ƒä¼˜åŒ–** | è¶…å‚æ•°è°ƒä¼˜ã€æ­£åˆ™åŒ–ã€ä¼˜åŒ–å™¨é€‰æ‹© |\n",
    "| **è®­ç»ƒåŠ é€Ÿ** | GPU åŠ é€Ÿã€æ··åˆç²¾åº¦ã€åˆ†å¸ƒå¼è®­ç»ƒ |\n",
    "| **éƒ¨ç½²ä¼˜åŒ–** | æ¨¡å‹é‡åŒ–ã€å‰ªæã€TensorRT åŠ é€Ÿ |\n",
    "| **ç¡¬ä»¶ä¼˜åŒ–** | CUDAã€NCCLã€TPU/FPGA |\n",
    "\n",
    "ä½ å·²ç»æœ‰**çŸ©é˜µåˆ†è§£å’Œ Rust ç»éªŒ**ï¼Œå¦‚æœæƒ³æ·±å…¥å·¥ç¨‹ä¼˜åŒ–ï¼Œå¯ä»¥ï¼š\n",
    "1. **ç ”ç©¶ PyTorch DDP / DeepSpeed**ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–ï¼‰\n",
    "2. **å­¦ä¹  CUDA / cuDNN ç¼–ç¨‹**ï¼ˆä½çº§ GPU åŠ é€Ÿï¼‰\n",
    "3. **å°è¯• TensorRT / ONNX**ï¼ˆæ¨ç†åŠ é€Ÿï¼‰\n",
    "\n",
    "è¿™å°†è®©ä½ åœ¨ **æ·±åº¦å­¦ä¹ å·¥ç¨‹ & ç¡¬ä»¶ä¼˜åŒ–** æ–¹é¢å…·å¤‡æ›´å¼ºçš„ç«äº‰åŠ› ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c15dc5c-18d4-4f93-b9a1-938224653390",
   "metadata": {},
   "source": [
    "## æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "\n",
    "æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼Œä¹Ÿå« Checkpointing æˆ– Recomputationï¼‰æ˜¯ä¸€ç§åœ¨è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ—¶ç”¨æ¥**èŠ‚çœå†…å­˜**çš„æŠ€æœ¯ï¼Œå°¤å…¶åœ¨ç½‘ç»œéå¸¸æ·±æˆ–æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰è¾ƒå¤§æ—¶éå¸¸æœ‰ç”¨ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨å†…å­˜æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç‰ºç‰²ä¸€äº›è®¡ç®—æ—¶é—´æ¥å‡å°‘å­˜å‚¨æ¿€æ´»å€¼ï¼ˆä¸­é—´è¾“å‡ºï¼‰çš„å†…å­˜éœ€æ±‚ã€‚ä¸‹é¢æˆ‘å°†è¯¦ç»†è§£é‡Šå®ƒçš„åŸç†ã€å®ç°æ–¹å¼ä»¥åŠä¼˜ç¼ºç‚¹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆéœ€è¦æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Ÿ\n",
    "åœ¨æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒä¸­ï¼Œåå‘ä¼ æ’­éœ€è¦ç”¨åˆ°å‰å‘ä¼ æ’­æ—¶æ¯ä¸€å±‚çš„æ¿€æ´»å€¼ã€‚è¿™äº›æ¿€æ´»å€¼é€šå¸¸ä¼šè¢«å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œä»¥ä¾¿åœ¨è®¡ç®—æ¢¯åº¦æ—¶ç›´æ¥ä½¿ç”¨ã€‚ç„¶è€Œï¼š\n",
    "- å¯¹äºæ·±å±‚ç½‘ç»œï¼ˆæ¯”å¦‚ ResNet-101ã€Transformer ç­‰ï¼‰ï¼Œå±‚æ•°éå¸¸å¤šï¼Œæ¿€æ´»å€¼çš„å†…å­˜å ç”¨ä¼šå˜å¾—éå¸¸å¤§ã€‚\n",
    "- å½“æ‰¹é‡å¤§å°å¢åŠ æ—¶ï¼Œæ¿€æ´»å€¼çš„å­˜å‚¨éœ€æ±‚è¿›ä¸€æ­¥çº¿æ€§å¢é•¿ã€‚\n",
    "- å¦‚æœæ˜¾å­˜ï¼ˆå¦‚ GPU å†…å­˜ï¼‰ä¸è¶³ï¼Œå¯èƒ½æ— æ³•è®­ç»ƒæ¨¡å‹ï¼Œæˆ–è€…åªèƒ½ä½¿ç”¨å¾ˆå°çš„æ‰¹é‡å¤§å°ï¼Œå½±å“è®­ç»ƒæ•ˆæœã€‚\n",
    "\n",
    "æ¢¯åº¦æ£€æŸ¥ç‚¹çš„ç›®æ ‡æ˜¯é€šè¿‡**ä¸å­˜å‚¨æ‰€æœ‰æ¿€æ´»å€¼**ï¼Œè€Œæ˜¯åœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—å®ƒä»¬ï¼Œä»è€Œå¤§å¹…å‡å°‘å†…å­˜å ç”¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### åŸºæœ¬åŸç†\n",
    "æ­£å¸¸æƒ…å†µä¸‹ï¼Œè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œçš„è¿‡ç¨‹æ˜¯ï¼š\n",
    "1. **å‰å‘ä¼ æ’­**ï¼šä»è¾“å…¥åˆ°è¾“å‡ºï¼Œè®¡ç®—æ¯ä¸€å±‚çš„æ¿€æ´»å€¼å¹¶å­˜å‚¨ã€‚\n",
    "2. **åå‘ä¼ æ’­**ï¼šåˆ©ç”¨å­˜å‚¨çš„æ¿€æ´»å€¼å’ŒæŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œè®¡ç®—æ¯ä¸€å±‚å‚æ•°çš„æ¢¯åº¦ã€‚\n",
    "\n",
    "æ¢¯åº¦æ£€æŸ¥ç‚¹çš„å·¥ä½œæ–¹å¼æ˜¯ï¼š\n",
    "- åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œ**åªä¿å­˜éƒ¨åˆ†å…³é”®å±‚çš„æ¿€æ´»å€¼**ï¼ˆè¿™äº›ç‚¹ç§°ä¸ºâ€œæ£€æŸ¥ç‚¹â€ï¼‰ï¼Œè€Œä¸æ˜¯æ¯ä¸€å±‚çš„æ¿€æ´»å€¼ã€‚\n",
    "- åœ¨åå‘ä¼ æ’­æ—¶ï¼Œå¯¹äºæœªä¿å­˜æ¿€æ´»å€¼çš„å±‚ï¼Œé€šè¿‡ä»æœ€è¿‘çš„æ£€æŸ¥ç‚¹é‡æ–°è¿è¡Œå‰å‘ä¼ æ’­æ¥**é‡æ–°è®¡ç®—æ¿€æ´»å€¼**ï¼Œç„¶åç»§ç»­è®¡ç®—æ¢¯åº¦ã€‚\n",
    "\n",
    "è¿™ç§æ–¹æ³•ç”¨é¢å¤–çš„è®¡ç®—ï¼ˆé‡æ–°è®¡ç®—æ¿€æ´»å€¼ï¼‰æ¢å–äº†å†…å­˜çš„èŠ‚çœã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### è¯¦ç»†æ­¥éª¤\n",
    "å‡è®¾ä¸€ä¸ªç®€å•çš„ç½‘ç»œæœ‰ 5 å±‚ï¼š\\( L_1 \\rightarrow L_2 \\rightarrow L_3 \\rightarrow L_4 \\rightarrow L_5 \\)ã€‚\n",
    "\n",
    "#### æ™®é€šè®­ç»ƒ\n",
    "1. å‰å‘ä¼ æ’­ï¼šè®¡ç®—å¹¶å­˜å‚¨æ‰€æœ‰å±‚çš„æ¿€æ´»å€¼ \\( A_1, A_2, A_3, A_4, A_5 \\)ã€‚\n",
    "2. åå‘ä¼ æ’­ï¼šä» \\( L_5 \\) å¼€å§‹ï¼Œåˆ©ç”¨ \\( A_5, A_4, \\ldots, A_1 \\) è®¡ç®—æ¯ä¸€å±‚çš„æ¢¯åº¦ã€‚\n",
    "3. å†…å­˜éœ€æ±‚ï¼šå­˜å‚¨æ‰€æœ‰ \\( A_1 \\) åˆ° \\( A_5 \\)ï¼Œå‡è®¾æ¯å±‚æ¿€æ´»å€¼å  \\( M \\) ä¸ªå­—èŠ‚ï¼Œæ€»å…± \\( 5M \\)ã€‚\n",
    "\n",
    "#### ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "å‡è®¾æˆ‘ä»¬é€‰æ‹© \\( L_2 \\) å’Œ \\( L_4 \\) ä½œä¸ºæ£€æŸ¥ç‚¹ï¼š\n",
    "1. **å‰å‘ä¼ æ’­**ï¼š\n",
    "   - è®¡ç®— \\( L_1 \\rightarrow L_5 \\)ï¼Œä½†åªä¿å­˜æ£€æŸ¥ç‚¹çš„æ¿€æ´»å€¼ \\( A_2 \\) å’Œ \\( A_4 \\)ã€‚\n",
    "   - å…¶ä»–å±‚çš„æ¿€æ´»å€¼ \\( A_1, A_3, A_5 \\) åœ¨å‰å‘ä¼ æ’­åè¢«ä¸¢å¼ƒã€‚\n",
    "   - å†…å­˜éœ€æ±‚ï¼šä»…å­˜å‚¨ \\( A_2 \\) å’Œ \\( A_4 \\)ï¼Œå³ \\( 2M \\)ã€‚\n",
    "\n",
    "2. **åå‘ä¼ æ’­**ï¼š\n",
    "   - ä» \\( L_5 \\) å¼€å§‹è®¡ç®—æ¢¯åº¦ï¼Œéœ€è¦ \\( A_4 \\)ï¼ˆå·²ä¿å­˜ï¼‰å’Œ \\( A_5 \\)ï¼ˆæœªä¿å­˜ï¼‰ã€‚\n",
    "   - ä» \\( A_4 \\) é‡æ–°è¿è¡Œå‰å‘ä¼ æ’­ \\( L_4 \\rightarrow L_5 \\)ï¼Œé‡æ–°è®¡ç®— \\( A_5 \\)ï¼Œç„¶åè®¡ç®— \\( L_5 \\) å’Œ \\( L_4 \\) çš„æ¢¯åº¦ã€‚\n",
    "   - ç»§ç»­åˆ° \\( L_3 \\)ï¼Œéœ€è¦ \\( A_2 \\)ï¼ˆå·²ä¿å­˜ï¼‰å’Œ \\( A_3 \\)ï¼ˆæœªä¿å­˜ï¼‰ã€‚\n",
    "   - ä» \\( A_2 \\) é‡æ–°è¿è¡Œ \\( L_2 \\rightarrow L_3 \\)ï¼Œè®¡ç®— \\( A_3 \\)ï¼Œç„¶åè®¡ç®— \\( L_3 \\) çš„æ¢¯åº¦ã€‚\n",
    "   - æœ€åä» \\( A_2 \\) å’Œè¾“å…¥é‡æ–°è®¡ç®— \\( A_1 \\)ï¼Œå®Œæˆ \\( L_2 \\) å’Œ \\( L_1 \\) çš„æ¢¯åº¦ã€‚\n",
    "\n",
    "3. **å†…å­˜èŠ‚çœ**ï¼šåªå­˜ 2 ä¸ªæ£€æŸ¥ç‚¹çš„æ¿€æ´»å€¼ï¼ˆ\\( 2M \\)ï¼‰ï¼Œè€Œä¸æ˜¯ 5 ä¸ªï¼ˆ\\( 5M \\)ï¼‰ã€‚\n",
    "4. **é¢å¤–è®¡ç®—**ï¼šéœ€è¦é‡æ–°è¿è¡Œéƒ¨åˆ†å‰å‘ä¼ æ’­ï¼ˆä¾‹å¦‚ \\( L_2 \\rightarrow L_3 \\) å’Œ \\( L_4 \\rightarrow L_5 \\)ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### æ£€æŸ¥ç‚¹çš„é€‰æ‹©\n",
    "å¦‚ä½•é€‰æ‹©æ£€æŸ¥ç‚¹æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼š\n",
    "- **å‡åŒ€åˆ†å¸ƒ**ï¼šä¾‹å¦‚æ¯éš” \\( k \\) å±‚è®¾ç½®ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼ˆå¦‚æ¯ 10 å±‚ï¼‰ã€‚\n",
    "- **åŠ¨æ€é€‰æ‹©**ï¼šæ ¹æ®å†…å­˜éœ€æ±‚å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä¼˜å…ˆé€‰æ‹©å†…å­˜å ç”¨å¤§çš„å±‚ä½œä¸ºæ£€æŸ¥ç‚¹ã€‚\n",
    "- **ç†è®ºæœ€ä¼˜**ï¼šç ”ç©¶è¡¨æ˜ï¼Œæœ€ä¼˜æ£€æŸ¥ç‚¹ç­–ç•¥å¯ä»¥å°†å†…å­˜éœ€æ±‚ä» \\( O(N) \\)ï¼ˆN ä¸ºå±‚æ•°ï¼‰é™ä½åˆ° \\( O(\\sqrt{N}) \\)ï¼Œä½†å®ç°èµ·æ¥è¾ƒå¤æ‚ã€‚\n",
    "\n",
    "ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ PyTorchã€TensorFlowï¼‰é€šå¸¸å†…ç½®äº†æ£€æŸ¥ç‚¹åŠŸèƒ½ï¼Œç”¨æˆ·åªéœ€æŒ‡å®šå“ªäº›å±‚æˆ–å­æ¨¡å—ä½œä¸ºæ£€æŸ¥ç‚¹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### æ•°å­¦åˆ†æ\n",
    "å‡è®¾ç½‘ç»œæœ‰ \\( N \\) å±‚ï¼Œæ¯å±‚æ¿€æ´»å€¼å  \\( M \\) å­—èŠ‚ï¼š\n",
    "- **æ™®é€šè®­ç»ƒ**ï¼šå†…å­˜éœ€æ±‚ \\( N \\times M \\)ã€‚\n",
    "- **æ£€æŸ¥ç‚¹è®­ç»ƒ**ï¼š\n",
    "  - è®¾æ£€æŸ¥ç‚¹æ•°ä¸º \\( K \\)ï¼ˆ\\( K < N \\)ï¼‰ï¼Œå†…å­˜éœ€æ±‚ä¸º \\( K \\times M \\)ã€‚\n",
    "  - é¢å¤–è®¡ç®—é‡ä¸ \\( N/K \\) æˆæ­£æ¯”ï¼ˆæ¯ä¸ªæ£€æŸ¥ç‚¹è´Ÿè´£çš„å±‚æ•°ï¼‰ã€‚\n",
    "  - æœ€ä¼˜æƒ…å†µä¸‹ï¼Œ\\( K \\approx \\sqrt{N} \\)ï¼Œå†…å­˜é™ä¸º \\( O(\\sqrt{N} \\times M) \\)ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œ\\( N = 100 \\)ï¼š\n",
    "- æ™®é€šè®­ç»ƒï¼š\\( 100M \\)ã€‚\n",
    "- æ£€æŸ¥ç‚¹è®­ç»ƒï¼ˆ\\( K = 10 \\)ï¼‰ï¼š\\( 10M \\) + å°‘é‡é¢å¤–è®¡ç®—ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ä¼˜ç‚¹\n",
    "1. **å†…å­˜æ•ˆç‡**ï¼šæ˜¾è‘—å‡å°‘æ¿€æ´»å€¼çš„å­˜å‚¨éœ€æ±‚ï¼Œå…è®¸è®­ç»ƒæ›´æ·±çš„ç½‘ç»œæˆ–ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°ã€‚\n",
    "2. **çµæ´»æ€§**ï¼šåœ¨æ˜¾å­˜å—é™çš„è®¾å¤‡ä¸Šä¹Ÿèƒ½è¿è¡Œå¤æ‚æ¨¡å‹ã€‚\n",
    "3. **å¹¿æ³›é€‚ç”¨**ï¼šé€‚ç”¨äº CNNã€RNNã€Transformer ç­‰å„ç§æ¶æ„ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ç¼ºç‚¹\n",
    "1. **è®¡ç®—å¼€é”€**ï¼šé‡æ–°è®¡ç®—æ¿€æ´»å€¼å¢åŠ äº†è®­ç»ƒæ—¶é—´ï¼Œé€šå¸¸æ¯”æ™®é€šè®­ç»ƒæ…¢ 20%-50%ï¼Œå…·ä½“å–å†³äºæ£€æŸ¥ç‚¹æ•°é‡å’Œç½‘ç»œç»“æ„ã€‚\n",
    "2. **å®ç°å¤æ‚æ€§**ï¼šéœ€è¦æ‰‹åŠ¨æŒ‡å®šæ£€æŸ¥ç‚¹æˆ–ä¾èµ–æ¡†æ¶æ”¯æŒï¼Œè°ƒè¯•å¯èƒ½æ›´å›°éš¾ã€‚\n",
    "3. **ä¸é€‚åˆæ‰€æœ‰åœºæ™¯**ï¼šå¦‚æœå†…å­˜ä¸æ˜¯ç“¶é¢ˆï¼Œæ£€æŸ¥ç‚¹åè€Œä¼šé™ä½æ•ˆç‡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### å®é™…åº”ç”¨\n",
    "- **PyTorch ç¤ºä¾‹**ï¼š\n",
    "  ```python\n",
    "  import torch\n",
    "  import torch.nn as nn\n",
    "  from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "  class MyModel(nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.layer1 = nn.Linear(1024, 1024)\n",
    "          self.layer2 = nn.Linear(1024, 1024)\n",
    "          self.layer3 = nn.Linear(1024, 1024)\n",
    "\n",
    "      def forward(self, x):\n",
    "          x = checkpoint(self.layer1, x)  # æ£€æŸ¥ç‚¹åŒ…è£¹å±‚1\n",
    "          x = checkpoint(self.layerThe layer2(x)      # æ™®é€šå±‚\n",
    "          x = checkpoint(self.layer3, x)  # æ£€æŸ¥ç‚¹åŒ…è£¹å±‚3\n",
    "          return x\n",
    "\n",
    "  model = MyModel()\n",
    "  input = torch.randn(32, 1024)  # batch_size = 32\n",
    "  output = model(input)\n",
    "  ```\n",
    "\n",
    "- **å¤§æ¨¡å‹**ï¼šå¦‚ GPTã€LLaMA æˆ–å¤§å‹ Transformerï¼Œå±‚æ•°å¯èƒ½é«˜è¾¾æ•°ç™¾å±‚ï¼Œä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å¯ä»¥æ˜¾è‘—é™ä½æ˜¾å­˜éœ€æ±‚ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### æ€»ç»“\n",
    "æ¢¯åº¦æ£€æŸ¥ç‚¹æ˜¯ä¸€ç§ä»¥æ—¶é—´æ¢ç©ºé—´çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å‡å°‘æ¿€æ´»å€¼çš„å­˜å‚¨æ¥é™ä½å†…å­˜éœ€æ±‚ï¼Œéå¸¸é€‚åˆæ˜¾å­˜å—é™æˆ–æ·±å±‚ç½‘ç»œçš„åœºæ™¯ã€‚å®ƒçš„æ ¸å¿ƒæ˜¯é€‰æ‹©éƒ¨åˆ†æ£€æŸ¥ç‚¹å¹¶åœ¨åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—å…¶ä»–å±‚çš„æ¿€æ´»å€¼ã€‚è™½ç„¶å¢åŠ äº†è®¡ç®—å¼€é”€ï¼Œä½†åœ¨å†…å­˜æ˜¯ç“¶é¢ˆæ—¶ï¼Œå®ƒèƒ½è®©â€œä¸å¯èƒ½çš„ä»»åŠ¡â€å˜å¾—å¯è¡Œã€‚\n",
    "\n",
    "å¦‚æœä½ å¯¹å®ç°ç»†èŠ‚æˆ–å…·ä½“åœºæ™¯æœ‰ç–‘é—®ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥è¿›ä¸€æ­¥å±•å¼€ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4bad4-c60c-48fe-ac77-818728b22984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
