{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b9ac51-ea6e-4701-a0ea-47c3d8e8f48c",
   "metadata": {},
   "source": [
    "## Chapter 16 : NLP Applications\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img alt=\"nlp-map-app\" src=\"https://d2l.ai/_images/nlp-map-app.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "  <figcaption> NLP Applications </figcaption>\n",
    "</figure>\n",
    "\n",
    "- Sentiment Analysis using RNN\n",
    "  - Glove (embedding, frozen) -> Bidirection RNN (encoder) -> linear (decoder)\n",
    "- Using CNN:\n",
    "  - *One dimensional correlation of mutiple channel is same as two dimensional corelation*.\n",
    "  - *Max-over-time Pool*: pool the sequence into certain width (different channel no operation).\n",
    "  - <figure style=\"text-align: center;\">\n",
    "      <img alt=\"textcnn\" src=\"https://d2l.ai/_images/textcnn.svg\" style=\"background-color: white; display: inline-block;\">\n",
    "      <figcaption> Text CNN </figcaption>\n",
    "    </figure>\n",
    "- Natural Language Processing:\n",
    "  - Three type: entailment (positive next), contradiction (negitive next), neutral (not next).\n",
    "  - Given premise and hypohesis tokens $A = (a_1,  \\ldots, a_m), B = (b_1,  \\ldots, b_m)$, where $a_i, b_j \\in \\mathbb R^d$ is word vector. The attenton is $e_{ij} = f(a_i)^{\\top}f(b_j)$, $f$ is a mlp layer. We define the soft aligned representation:\n",
    "    $$\\boldsymbol{\\beta}_i = \\sum_{j=1}^{n}\\frac{\\exp(e_{ij})}{ \\sum_{k=1}^{n} \\exp(e_{ik})} \\mathbf{b}_j.$$\n",
    "    $$\\boldsymbol{\\alpha}_j = \\sum_{i=1}^{m}\\frac{\\exp(e_{ij})}{ \\sum_{k=1}^{m} \\exp(e_{kj})} \\mathbf{a}_i.$$\n",
    "    Then we compare this representations with word vectors (again $g$ is a mlp layer):\n",
    "    $$\\begin{split}\\mathbf{v}_{A,i} = g([\\mathbf{a}_i, \\boldsymbol{\\beta}_i]), i = 1, \\ldots, m\\\\ \\mathbf{v}_{B,j} = g([\\mathbf{b}_j, \\boldsymbol{\\alpha}_j]), j = 1, \\ldots, n.\\end{split}$$\n",
    "    Then we aggregating the information:\n",
    "    $$\\mathbf{v}_A = \\sum_{i=1}^{m} \\mathbf{v}_{A,i}, \\quad \\mathbf{v}_B = \\sum_{j=1}^{n}\\mathbf{v}_{B,j}.$$\n",
    "    and then feed them into a mlp layer then get the classification result: $\\hat{\\mathbf{y}} = h([\\mathbf{v}_A, \\mathbf{v}_B]).$\n",
    "- Fine-tune BERT: single text, text pair, text tag (on every token representation add a shared dense layer), Q&A (first sequence as question, second as passage, from passage find out answer for question, we also need additional two output layer for predicting start pos and end pos, and the prediction is[start, end] pair).\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn.svg\" style=\"background-color: white; display: inline-block;\"> -->\n",
    "<!-- <img alt=\"ResNeXt Block\" src=\"https://d2l.ai/_images/rnn-bptt.svg\" style=\"background-color: white; display: inline-block;\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65e7bd-468d-4657-ada7-01510d714412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
